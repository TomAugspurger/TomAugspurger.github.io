<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="/theme/css/style.less">
  <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="icon" type="image/vnd.microsoft.icon" href="/">
  <link rel="stylesheet" type="text/css" href="/theme/css/normalize.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto+Mono">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/hatena-bookmark-icon.css">


  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Tom Augspurger">
  <meta name="description" content="Posts and writings by Tom Augspurger">

  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="datas-frame Atom" />

<meta name="keywords" content="">

  <title>
    datas-frame
&ndash; dask-ml 0.4.1 Released  </title>

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48304175-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>

<body>
  <main>
    <header>
      <div class="site-name">
        <a href="">datas-frame</a>
      </div>
      <p>
        <a href="/archives.html"><i class="fa fa-archive"></i> Archive</a>
      </p>
    </header>

<article>
  <div class="article__title">
    <h1><a href="/dask-ml-041.html">dask-ml 0.4.1 Released</a></h1>
  </div>
  <div class="article__meta">
    <p class="article__meta__post-date">Posted on: Tue 13 February 2018</p>
    </p>
  </div>
  <div class="article__text">
    <p><em>This work is supported by <a href="http://anaconda.com/">Anaconda Inc</a> and the Data
Driven Discovery Initiative from the <a href="https://www.moore.org/">Moore
Foundation</a>.</em></p>
<p><a href="http://dask-ml.readthedocs.io/en/latest/">dask-ml</a> 0.4.1 was released today with a few enhancements. See the
<a href="http://dask-ml.readthedocs.io/en/latest/changelog.html">changelog</a> for all the changes from 0.4.0.</p>
<p>Conda packages are available on conda-forge</p>
<div class="highlight"><pre><span></span>$ conda install -c conda-forge dask-ml
</pre></div>


<p>and wheels and the source are available on PyPI</p>
<div class="highlight"><pre><span></span>$ pip install dask-ml
</pre></div>


<p>I wanted to highlight one change, that touches on a topic I mentioned in my
first post on <a href="scalable-ml-01">scalable Machine Learning</a>. I discussed how, in
my limited experience, a common workflow was to train on a small batch of data
and predict for a much larger set of data. The training data easily fits in
memory on a single machine, but the full dataset does not.</p>
<p>A new meta-estimator, <a href="http://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.wrappers.ParallelPostFit.html#dask_ml.wrappers.ParallelPostFit"><code>ParallelPostFit</code></a> helps with this
common case. It's a meta-estimator that wraps a regular scikit-learn estimator,
similar to how <code>GridSearchCV</code> wraps an estimator. The <code>.fit</code> method is very
simple; it just calls the underlying estimator's <code>.fit</code> method and copies over
the learned attributes. This means <code>ParalellPostFit</code> is not suitable for
<em>training</em> on large datasets. It is, however, perfect for post-fit tasks like
<code>.predict</code>, or <code>.transform</code>.</p>
<p>As an example, we'll fit a scikit-learn <code>GradientBoostingClassifier</code> on a small
in-memory dataset.</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">dask_ml.datasets</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="o">...</span>                                             <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">ParallelPostFit</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">GradientBoostingClassifier</span><span class="p">())</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ParallelPostFit</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="o">...</span><span class="p">))</span>
</pre></div>


<p>Nothing special so far. But now, let's suppose we had a "large" dataset for
prediction. We'll use <code>dask_ml.datasets.make_classifciation</code>, but in practice
you would read this from a file system or database.</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">X_big</span><span class="p">,</span> <span class="n">y_big</span> <span class="o">=</span> <span class="n">dask_ml</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
                                                        <span class="n">chunks</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                                        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<p>In this case we have a dataset with 100,000 samples split into blocks of 1,000.
We can now predict for this large dataset.</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">dask</span><span class="o">.</span><span class="n">array</span><span class="o">&lt;</span><span class="n">predict</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int64</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,)</span><span class="o">&gt;</span>
</pre></div>


<p>Now things are different. <code>ParallelPostFit.predict</code>, <code>.predict_proba</code>, and
<code>.transform</code>, all return dask arrays instead of immediately computing the
result. We've built up task graph of computations to be performed, which allows
dask to step in and compute things in parallel. When you're ready for the
answer, call <code>compute</code>:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">0.99141094</span><span class="p">,</span> <span class="mf">0.00858906</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.93178389</span><span class="p">,</span> <span class="mf">0.06821611</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.99129105</span><span class="p">,</span> <span class="mf">0.00870895</span><span class="p">],</span>
       <span class="o">...</span><span class="p">,</span>
       <span class="p">[</span><span class="mf">0.97996652</span><span class="p">,</span> <span class="mf">0.02003348</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.98087444</span><span class="p">,</span> <span class="mf">0.01912556</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.99407016</span><span class="p">,</span> <span class="mf">0.00592984</span><span class="p">]])</span>
</pre></div>


<p>At that point the dask scheduler comes in and executes your compute in parallel,
using all the cores of your laptop or workstation, or all the machines on your
cluster.</p>
<p><code>ParallelPostFit</code> "fixes" a couple of issues in scikit-learn outside of
scikit-learn itself</p>
<ul>
<li><a href="https://github.com/scikit-learn/scikit-learn/issues/7448">parallel predict</a></li>
<li><a href="https://github.com/scikit-learn/scikit-learn/issues/7635">parallel transform</a></li>
</ul>
<p>If you're able to depend on dask and dask-ml, consider giving <code>ParallelPostFit</code>
a shot and let me know how it turns out. For estimators whose predict is
relatively expensive and not already parallelized, <code>ParallelPostFit</code> can give
a nice <a href="http://dask-ml.readthedocs.io/en/latest/auto_examples/plot_parallel_postfit.html#sphx-glr-auto-examples-plot-parallel-postfit-py">performance boost</a>.</p>
<p><img alt="" src="images/sphx_glr_plot_parallel_postfit_001.png"></p>
<p>Even if the underlying estimator's <code>predict</code> or <code>tranform</code> method is cheap or
parallelized, <code>ParallelPostFit</code> does still help with distributed the work on all
the machines in your cluster, or doing the operation out-of-core.</p>
<p>Thanks to all the contributors who worked on this release.</p>
  </div>
  <div id="article__comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_identifier = "dask-ml-041.html";
        (function() {
             var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
             dsq.src = '//tomaugspurger.disqus.com/embed.js';
             (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
         })();
    </script>
  </div>

</article>


  </main>
    <footer>
      <div class="author__logo">
          <img src="/theme/images/logo.png" alt="logo">
      </div>
      <section class="author">
        <div class="author__name">
          <a href="/pages/about.html">Tom Augspurger</a>
          <p></p>
        </div>
        <div class="author__link">
          <ul>
            <li><a href="/pages/about.html" title="About"><i class="fa fa-link"></i></a></li>
            <li><a href="/pages/article-1-cluster.html" title="article-1-cluster"><i class="fa fa-link"></i></a></li>
            <li>
              <a href="/feeds/all.atom.xml" target="_blank" title="Feed">
                <i class="fa fa-rss"></i>
              </a>
            </li>
          </ul>
        </div>
      </section>
      <div class="ending-message">
        <p>&copy; Tom Augspurger. Powered by <a href="http://getpelican.com" target="_blank">Pelican</a>, Theme is using <a href="https://github.com/laughk/pelican-hss" target="_blank">HSS</a>. </p>
      </div>
    </footer>
</body>
</html>