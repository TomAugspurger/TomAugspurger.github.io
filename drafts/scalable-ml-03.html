<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="/theme/css/style.less">
  <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="icon" type="image/vnd.microsoft.icon" href="/">
  <link rel="stylesheet" type="text/css" href="/theme/css/normalize.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto+Mono">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/hatena-bookmark-icon.css">


  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Tom Augspurger">
  <meta name="description" content="Posts and writings by Tom Augspurger">

  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="datas-frame Atom" />

<meta name="keywords" content="">

  <title>
    datas-frame
&ndash; Scalable Machine Learning (Part 3): Parallel  </title>

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48304175-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>

<body>
  <main>
    <header>
      <div class="site-name">
        <a href="">datas-frame</a>
      </div>
      <p>
        <a href="/archives.html"><i class="fa fa-archive"></i> Archive</a>
      </p>
    </header>

<article>
  <div class="article__title">
    <h1><a href="/drafts/scalable-ml-03.html">Scalable Machine Learning (Part 3): Parallel</a></h1>
  </div>
  <div class="article__meta">
    <p class="article__meta__post-date">Posted on: Sat 16 September 2017</p>
    </p>
  </div>
  <div class="article__text">
    <p><em>This work is supported by <a href="https://www.anaconda.com/">Anaconda, Inc.</a> and the
Data Driven Discovery Initiative from the <a href="https://www.moore.org/">Moore Foundation</a>.</em></p>
<p>This is part three of my series on scalable machine learning.</p>
<ul>
<li><a href="scalable-ml-01">Small Fit, Big Predict</a></li>
<li><a href="scalable-ml-02">Scikit-Learn Partial Fit</a></li>
<li><a href="scalable-ml-03">Parallel Machine Learning</a></li>
</ul>
<p>You can download a notebook of this post [here][notebook].</p>
<hr>
<p>In <a href="scalable-ml-01">part one</a>, I talked about the type of constraints that push
us to parallelize or distribute a machine learning workload. Today, we'll be
talking about the second constraint, "I'm constrained by time, and would like to
fit more models at once, by using all the cores of my laptop, or all the
machines in my cluster".</p>
<h2>An Aside on Parallelism</h2>
<p>In the case of Python, we have two main avenues of parallelization (which we'll
roughly define as using multiple "workers" to do some "work" in less time).
Those two avenues are</p>
<ol>
<li>multi-threading</li>
<li>multi-processing</li>
</ol>
<p>For python, the most important differences are that</p>
<ol>
<li>multi-threaded code can <em>potentially</em> be limited by the GIL</li>
<li>multi-processing code requires that data be serialized between processes</li>
</ol>
<p>The GIL is the "Global Interpreter Lock", an implementation detail of CPython
that means only one thread in your python process can be executing python code
at once.</p>
<p><a href="https://www.youtube.com/watch?v=9zinZmE3Ogk">This talk</a> by Python
core-developer Raymond Hettinger does a good job summarizing things for Python,
with an important caveat: much of what he says about the GIL doesn't apply to
the <em>scientific</em> python stack. NumPy, scikit-learn, and much of pandas release
the GIL and can run multi-threaded, using shared memory and so avoiding
serialization costs. I'll highlight his quote, which summarizes the
situation:</p>
<blockquote>
<blockquote>
<p>Your weakness is your strength, and your strength is your weakness</p>
</blockquote>
<p>The strength of threads is shared state. The weakness of threads is shared
state.</p>
</blockquote>
<p>Another wrinkle here is that when you move to a distributed cluster, you <em>have</em>
to have multiple processes. And communication between processes becomes even
more expensive since you'll have network overhead to worry about, in addition to
the serialization costs.</p>
<p>Fortunately, modules like <code>concurrent.futures</code> and libraries like <code>dask</code> make it
easy to swap one mode in for another. Let's make a little dask array:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dask.array</span> <span class="kn">as</span> <span class="nn">da</span>
<span class="kn">import</span> <span class="nn">dask</span>
<span class="kn">import</span> <span class="nn">dask.threaded</span>
<span class="kn">import</span> <span class="nn">dask.multiprocessing</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">da</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">chunks</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">X</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<p>We can swap out the scheduler with a context-manager:</p>
<div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="k">with</span> <span class="n">dask</span><span class="o">.</span><span class="n">set_options</span><span class="p">(</span><span class="n">get</span><span class="o">=</span><span class="n">dask</span><span class="o">.</span><span class="n">threaded</span><span class="o">.</span><span class="n">get</span><span class="p">):</span>
    <span class="c1"># threaded is the default for dask.array anyway</span>
    <span class="n">result</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="k">with</span> <span class="n">dask</span><span class="o">.</span><span class="n">set_options</span><span class="p">(</span><span class="n">get</span><span class="o">=</span><span class="n">dask</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">get</span><span class="p">):</span>
    <span class="n">result</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>


<p>Every dask collection (<code>dask.array</code>, <code>dask.dataframe</code>, <code>dask.bag</code>) has a default
scheduler that typically works well for the kinds of operations it does. For
<code>dask.array</code> and <code>dask.dataframe</code>, the shared-memory threaded scheduler is used.</p>
<h2>Cost Models</h2>
<p>In <a href="https://www.youtube.com/watch?v=tC94Mkg-oJU">this talk</a>, Simon Peyton Jones
talks about parallel and distributed computing for Haskell. He stressed
repeatedly that there's no silver bullet when it comes to parallelism. The type
of parallelism appropriate for a web server, say, may be different than the type
of parallelism appropriate for a machine learning algorithm.</p>
<p>I mention all this, since we're about to talk about parallel machine learning.
<em>In general</em>, for small data and many models you'll want to use the threaded
scheduler. For bigger data (larger than memory), you'll want want to use the
distributed scheduler. Assuming the underlying NumPy, SciPy, scikit-learn, or
pandas operation releases the GIL, you'll be able to get nice speedups without
the cost of serialization. But again, there isn't a silver bullet here, and the
best type of parallelism will depend on your particular problem.</p>
<h2>Where to Parallelize</h2>
<p>In a typical machine-learning workflow, there are typically ample opportunities for
parallelism.</p>
<ol>
<li>Over Hyper-parameters (one fit per combination of parameters)</li>
<li>Over Cross-validation folds (one fit per fold)</li>
<li>Within an algorithm (for some algorithms)</li>
</ol>
<p>Scikit-learn already uses parallelism in many places, anywhere you see an
<code>n_jobs</code> keyword.</p>
  </div>
  <div id="article__comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_identifier = "drafts/scalable-ml-03.html";
        (function() {
             var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
             dsq.src = '//tomaugspurger.disqus.com/embed.js';
             (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
         })();
    </script>
  </div>

</article>


  </main>
    <footer>
      <div class="author__logo">
          <img src="/theme/images/logo.png" alt="logo">
      </div>
      <section class="author">
        <div class="author__name">
          <a href="/pages/about.html">Tom Augspurger</a>
          <p></p>
        </div>
        <div class="author__link">
          <ul>
            <li><a href="/pages/about.html" title="About"><i class="fa fa-link"></i></a></li>
            <li><a href="/pages/article-1-cluster.html" title="article-1-cluster"><i class="fa fa-link"></i></a></li>
            <li>
              <a href="/feeds/all.atom.xml" target="_blank" title="Feed">
                <i class="fa fa-rss"></i>
              </a>
            </li>
          </ul>
        </div>
      </section>
      <div class="ending-message">
        <p>&copy; Tom Augspurger. Powered by <a href="http://getpelican.com" target="_blank">Pelican</a>, Theme is using <a href="https://github.com/laughk/pelican-hss" target="_blank">HSS</a>. </p>
      </div>
    </footer>
</body>
</html>