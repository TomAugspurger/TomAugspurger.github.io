<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Tom's Blog (python)</title><link>http://tomaugspurger.github.io/</link><description></description><atom:link type="application/rss+xml" href="http://tomaugspurger.github.io/categories/cat_python.xml" rel="self"></atom:link><language>en</language><lastBuildDate>Wed, 25 Mar 2015 00:34:19 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Using Python to tackle the CPS (Part 4)</title><link>http://tomaugspurger.github.io/posts/tackling the cps (part 4).html</link><dc:creator>Tom Augspurger</dc:creator><description>&lt;div&gt;&lt;p&gt;Last time, we got to where we'd like to have started: One file per month, with each month laid out the same.&lt;/p&gt;
&lt;p&gt;As a reminder, the CPS interviews households 8 times over the course of 16 months. They're interviewed for 4 months, take 8 months off, and are interviewed four more times. So if your first interview was in month $m$, you're also interviewed in months $$m + 1, m + 2, m + 3, m + 12, m + 13, m + 14, m + 15$$.&lt;/p&gt;
&lt;p&gt;I stored the data in &lt;a href="http://pandas-docs.github.io/pandas-docs-travis/dsintro.html#panel"&gt;Panels&lt;/a&gt;, the less well-known, higher-dimensional cousin of the &lt;a href="http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.html"&gt;DataFrame&lt;/a&gt;. Panels are 3-D structures, which is great for this kind of data. The three dimensions are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;items: Month in Survey (0 - 7)&lt;/li&gt;
&lt;li&gt;fields: Things like employment status, earnings, hours worked&lt;/li&gt;
&lt;li&gt;id: An identifier for each household&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Think of each item as a 2-D slice (a DataFrame) into the 3-D Panel. So each household is described by a single Panel (or 8 DataFrames).&lt;/p&gt;
&lt;p&gt;The actual panel construction occurs in &lt;a href="https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/make_panel.py#L151"&gt;&lt;code&gt;make_full_panel&lt;/code&gt;&lt;/a&gt;. Given a starting month, it figures out the months needed to generate that wave's Panel ($m, m + 1, m + 2, \ldots$), and stores these in an iterator called &lt;code&gt;dfs&lt;/code&gt;.
Since each month on disk contains people from 8 different waves (first month, second month, ...), I filter down to just the people in their $i^{th}$ month in the survey, where $i$ is the month I'm interested in.
Everything up until this point is done lazily; nothing has actually be read into memory yet.&lt;/p&gt;
&lt;p&gt;Now we'll read in each month, storing each month's DataFrame in a dictionary, &lt;code&gt;df_dict&lt;/code&gt;. We take the first month as is.
Each subsequent month has to be matched against the first month.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    &lt;span class="n"&gt;df_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;df1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dfn&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dfs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;df_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;match_panel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dfn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;settings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'panel_log'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="c"&gt;# Lose dtype info here if I just do from dict.&lt;/span&gt;
    &lt;span class="c"&gt;# to preserve dtypes:&lt;/span&gt;
    &lt;span class="n"&gt;df_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;wp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Panel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;orient&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'minor'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wp&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;In an ideal world, we just check to see if the indexes match (the unique identifier). However, the unique ID given by the Census Bureau isn't so unique, so we use some heuristics to guess if the person is actually the same as the one interviewed next week. &lt;code&gt;match_panel&lt;/code&gt; basically checks to see if a person's race and gender hasn't changed, and that their age has changed by less than a year or so.&lt;/p&gt;
&lt;p&gt;There's a bit more code that handles special cases, errors, and the writing of the output.
I was especially interested in earnings data, so I wrote that out separately.
But now we're finally to the point where we can do some analysis:&lt;/p&gt;&lt;/div&gt;</description><guid>http://tomaugspurger.github.io/posts/tackling the cps (part 4).html</guid><pubDate>Mon, 19 May 2014 17:01:00 GMT</pubDate></item><item><title>Using Python to tackle the CPS (Part 3)</title><link>http://tomaugspurger.github.io/posts/tackling the cps (part 3).html</link><dc:creator>Tom Augspurger</dc:creator><description>&lt;div&gt;&lt;p&gt;In &lt;a href="http://tomaugspurger.github.io/blog/2014/02/04/tackling%20the%20cps%20(part%202)/"&gt;part 2&lt;/a&gt; of this series, we set the stage to parse the data files themselves.&lt;/p&gt;
&lt;p&gt;As a reminder, we have a dictionary that looks like&lt;/p&gt;
&lt;pre class="code literal-block"&gt;         &lt;span class="nb"&gt;id&lt;/span&gt;  &lt;span class="n"&gt;length&lt;/span&gt;  &lt;span class="n"&gt;start&lt;/span&gt;  &lt;span class="n"&gt;end&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="n"&gt;HRHHID&lt;/span&gt;      &lt;span class="mi"&gt;15&lt;/span&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;15&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="n"&gt;HRMONTH&lt;/span&gt;       &lt;span class="mi"&gt;2&lt;/span&gt;     &lt;span class="mi"&gt;16&lt;/span&gt;   &lt;span class="mi"&gt;17&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="n"&gt;HRYEAR4&lt;/span&gt;       &lt;span class="mi"&gt;4&lt;/span&gt;     &lt;span class="mi"&gt;18&lt;/span&gt;   &lt;span class="mi"&gt;21&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;  &lt;span class="n"&gt;HURESPLI&lt;/span&gt;       &lt;span class="mi"&gt;2&lt;/span&gt;     &lt;span class="mi"&gt;22&lt;/span&gt;   &lt;span class="mi"&gt;23&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="n"&gt;HUFINAL&lt;/span&gt;       &lt;span class="mi"&gt;3&lt;/span&gt;     &lt;span class="mi"&gt;24&lt;/span&gt;   &lt;span class="mi"&gt;26&lt;/span&gt;
         &lt;span class="o"&gt;...&lt;/span&gt;     &lt;span class="o"&gt;...&lt;/span&gt;    &lt;span class="o"&gt;...&lt;/span&gt;  &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;giving the columns of the raw CPS data files. This post (or two) will describe the reading of the actual data files, and the somewhat tricky process of matching individuals across the different files. After that we can (finally) get into analyzing the data. The old joke is that statisticians spend 80% of their time munging their data, and 20% of their time complaining about munging their data. So 4 posts about data cleaning seems reasonable.&lt;/p&gt;
&lt;p&gt;The data files are stored in fixed width format (FWF), one of the least human friendly ways to store data.
We want to get to an &lt;a href="http://www.hdfgroup.org/HDF5/"&gt;HDF5&lt;/a&gt; file, which is extremely fast and convinent with pandas.&lt;/p&gt;
&lt;p&gt;Here's the first line of the raw data:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;head -n 1 /Volumes/HDD/Users/tom/DataStorage/CPS/monthly/cpsb9401
881605952390 2  286-1 2201-1 1 1 1-1 1 5-1-1-1  22436991 1 2 1 6 194 2A61 -1 2 2-1-1-1-1 363 1-15240115 3-1 4 0 1-1 2 1-1660 1 2 2 2 6 236 2 8-1 0 1-1 1 1 1 2 1 2 57 57 57 1 0-1 2 5 3-1-1 2-1-1-1-1-1 2-1-1-1-1-1-1-1-1-1-1-1 -1-1-1-1-1-1-1-1-1-1-1 -1-1  169-1-1-1-1-1-1-1-1-1-1-1-1-1-1 -1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 -1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 2-1 0 4-1-1-1-1-1-1 -1-1-1 0 1 2-1-1-1-1-1-1-1-1-1 -1 -1-1-1 -1 -1-1-1 0-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 0-1-1-1-1-1  -1  -1  -1  0-1-1      0-1-1-1      -1      0-1-1-1-1-1-1-1-1 2-1-1-1-1  22436991        -1         0  22436991  22422317-1         0 0 0 1 0-1 050 0 0 0 011 0 0 0-1-1-1-1 0 0 0-1-1-1-1-1-1 1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 1 1 1 1 1 1 1 1 1 1 1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 1 1 1-1-1-1
&lt;/pre&gt;


&lt;p&gt;We'll use pandas' &lt;a href="http://pandas.pydata.org/pandas-docs/version/0.13.0/generated/pandas.io.parsers.read_fwf.html#pandas.io.parsers.read_fwf"&gt;&lt;code&gt;read_fwf&lt;/code&gt;&lt;/a&gt; parser, passing in the widths we got from last post.
One note of warning, the &lt;code&gt;read_fwf&lt;/code&gt; function is slow. It's written in plain python, and really makes you appreciate &lt;a href="http://wesmckinney.com/blog/?p=543"&gt;all the work&lt;/a&gt; Wes (the creater or pandas) put into making &lt;code&gt;read_csv&lt;/code&gt; fast.&lt;/p&gt;
&lt;p&gt;Start by looking at the &lt;code&gt;__main__&lt;/code&gt; &lt;a href="https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/make_hdf_store.py#L786"&gt;entry point&lt;/a&gt;. The basic idea is to call &lt;code&gt;python make_hdf.py&lt;/code&gt; with an optional argument giving a file with a specific set of months you want to process. Otherwise, it processes every month in your data folder. There's a bit of setup to make sure everything is order, and then we jump to the &lt;a href="https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/make_hdf_store.py#L813"&gt;next important line&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;month&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;months&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;append_to_store&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;settings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skips&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;I'd like to think that &lt;a href="https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/make_hdf_store.py#L725"&gt;this function&lt;/a&gt; is fairly straightforward. We generate the names I use internally (&lt;code&gt;name&lt;/code&gt;), read in the data dictionary that we parsed last time (&lt;code&gt;dd&lt;/code&gt; and &lt;code&gt;widths&lt;/code&gt;), and get to work reading the actual data with&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_fwf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;'.gz'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;widths&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;widths&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;compression&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'gzip'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Rather than stepping through every part of the processing (checking types, making sure indexes are unique, handling missing values, etc.) I want to focus on one specific issue: handling special cases. Since the CPS data aren't consistent month to month, I needed a way transform the data for certain months differently that for others. The design I came up with worked pretty well.&lt;/p&gt;
&lt;p&gt;The solution is in &lt;a href="https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/make_hdf_store.py#L603"&gt;&lt;code&gt;special_by_dd&lt;/code&gt;&lt;/a&gt;. Basically, each data dictionary (which describes the data layout for a month) has its own little quirks.
For example, the data dictionary starting in January 1989 spread the two digits for age across two fields. The fix itself is extremely simple: &lt;code&gt;df["PRTAGE"] = df["AdAGEDG1"] * 10 + df["AdAGEDG2"]&lt;/code&gt;, but knowing when to apply this fix, and how to apply several of these fixes is the interesting part.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/make_hdf_store.py#L603"&gt;&lt;code&gt;special_by_dd&lt;/code&gt;&lt;/a&gt;, I created a handful of closures (basically just functions inside other functions), and a dictionary mapping names to those functions.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;func_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;"expand_year"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;expand_year&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"combine_age"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;combine_age&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
             &lt;span class="s"&gt;"expand_hours"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;expand_hours&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"align_lfsr"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;align_lfsr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
             &lt;span class="s"&gt;"combine_hours"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;combine_hours&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Each one of these functions takes a DataFrame and returns a DataFrame, with the fix applied. The example above is &lt;code&gt;combine_age&lt;/code&gt;.
In a settings file, I had a JSON object mapping the data dictionary name to special functions to apply. For example, January 1989's special case list was:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;"jan1989": ["expand_year", "combine_age", "align_lfsr", "expand_hours", "combine_hours"]
&lt;/pre&gt;


&lt;p&gt;I get the necessary special case functions and apply each with&lt;/p&gt;
&lt;pre class="code literal-block"&gt;specials = special_by_dd(settings["special_by_dd"][dd_name])
for func in specials:
    df = specials[func](df, dd_name)
&lt;/pre&gt;


&lt;p&gt;&lt;code&gt;specials&lt;/code&gt; is just &lt;code&gt;func_dict&lt;/code&gt; from above, but filtered to be only the functions specified in the settings file.
We select the function from the dictionary with &lt;code&gt;specials[func]&lt;/code&gt; and then directly call it with &lt;code&gt;(df, dd_name)&lt;/code&gt;.
Since functions are objects in python, we're able to store them in dictionaries and pass them around like just about anything else.&lt;/p&gt;
&lt;p&gt;This method gave a lot of flexibility. When I discovered a new way that one month's layout differed from what I wanted, I simply wrote a function to handle the special case, added it to &lt;code&gt;func_dict&lt;/code&gt;, and added the new special case to that month's speical case list.&lt;/p&gt;
&lt;p&gt;There's a bit more standardization and other boring stuff that gets us to a good place: each month with the same layout. Now we get get to the tricky alignment, which I'll save for another post.&lt;/p&gt;&lt;/div&gt;</description><guid>http://tomaugspurger.github.io/posts/tackling the cps (part 3).html</guid><pubDate>Mon, 19 May 2014 17:00:00 GMT</pubDate></item><item><title>Using Python to tackle the CPS (Part 2)</title><link>http://tomaugspurger.github.io/posts/tackling the cps (part 2).html</link><dc:creator>Tom Augspurger</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;a href="http://tomaugspurger.github.io/blog/2014/01/27/tackling%20the%20cps/"&gt;Last time&lt;/a&gt;, we used Python to fetch some data from the &lt;a href="http://www.census.gov/cps/"&gt;Current Population Survey&lt;/a&gt;. Today, we'll work on parsing the files we just downloaded.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We downloaded two types of files last time:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPS monthly tables: a fixed-width format text file with the actual data&lt;/li&gt;
&lt;li&gt;Data Dictionaries: a text file describing the layout of the monthly tables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our goal is to parse the monthly tables. Here's the first two lines from the unzipped January 1994 file:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;/V/H/U/t/D/C/monthly head -n &lt;span class="m"&gt;2&lt;/span&gt; cpsb9401
&lt;span class="m"&gt;881605952390&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;  286-1 2201-1 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 1-1 &lt;span class="m"&gt;1&lt;/span&gt; 5-1-1-1  &lt;span class="m"&gt;22436991&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt; &lt;span class="m"&gt;194&lt;/span&gt; 2A61 -1 &lt;span class="m"&gt;2&lt;/span&gt; 2-1-1-1-1 &lt;span class="m"&gt;363&lt;/span&gt; 1-15240115 3-1 &lt;span class="m"&gt;4&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; 1-1 &lt;span class="m"&gt;2&lt;/span&gt; 1-1660 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt; &lt;span class="m"&gt;236&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; 8-1 &lt;span class="m"&gt;0&lt;/span&gt; 1-1 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;57&lt;/span&gt; &lt;span class="m"&gt;57&lt;/span&gt; &lt;span class="m"&gt;57&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 0-1 &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;5&lt;/span&gt; 3-1-1 2-1-1-1-1-1 2-1-1-1-1-1-1-1-1-1-1-1 -1-1-1-1-1-1-1-1-1-1-1 -1-1  169-1-1-1-1-1-1-1-1-1-1-1-1-1-1 -1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 -1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 2-1 &lt;span class="m"&gt;0&lt;/span&gt; 4-1-1-1-1-1-1 -1-1-1 &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 2-1-1-1-1-1-1-1-1-1 -1 -1-1-1 -1 -1-1-1 0-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 0-1-1-1-1-1  -1  -1  -1  0-1-1      0-1-1-1      -1      0-1-1-1-1-1-1-1-1 2-1-1-1-1  &lt;span class="m"&gt;22436991&lt;/span&gt;        -1         &lt;span class="m"&gt;0&lt;/span&gt;  &lt;span class="m"&gt;22436991&lt;/span&gt;  22422317-1         &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 0-1 &lt;span class="m"&gt;050&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;011&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; 0-1-1-1-1 &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; 0-1-1-1-1-1-1 1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 1-1-1-1
&lt;span class="m"&gt;881605952390&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;  286-1 2201-1 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 1-1 &lt;span class="m"&gt;1&lt;/span&gt; 5-1-1-1  &lt;span class="m"&gt;22436991&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt; &lt;span class="m"&gt;194&lt;/span&gt; 2A61 -1 &lt;span class="m"&gt;2&lt;/span&gt; 2-1-1-1-1 &lt;span class="m"&gt;363&lt;/span&gt; 1-15240115 3-1 &lt;span class="m"&gt;4&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; 1-1 &lt;span class="m"&gt;2&lt;/span&gt; 3-1580 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;239&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; 8-1 &lt;span class="m"&gt;0&lt;/span&gt; 2-1 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;57&lt;/span&gt; &lt;span class="m"&gt;57&lt;/span&gt; &lt;span class="m"&gt;57&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 0-1 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 2-140-1-1 40-1-1-1-1 2-1 2-140-1 40-1   -1 &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;5&lt;/span&gt; 5-1 &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="m"&gt;5&lt;/span&gt; 2-1-1-1-1-1-1 -1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 -1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 1-118 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 4-1-1-1 -1 1-1 &lt;span class="m"&gt;1&lt;/span&gt; 2-1-1-1-1-1-1-1 &lt;span class="m"&gt;4&lt;/span&gt; 1242705-1-1-1 -1  3-1-1 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; 4-1 &lt;span class="m"&gt;1&lt;/span&gt; 6-1 6-136-1 &lt;span class="m"&gt;1&lt;/span&gt; 4-110-1 &lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 0-1-1-1-1  -1-1  -1  -1  0-1-1      0-1-1-1            -10-1-1-1-1-1-1-1-1-1-1-1-1-1  &lt;span class="m"&gt;22436991&lt;/span&gt;        -1         &lt;span class="m"&gt;0&lt;/span&gt;  &lt;span class="m"&gt;31870604&lt;/span&gt;  25650291-1         &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 0-1 &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; 0-1-1-1-1 &lt;span class="m"&gt;0&lt;/span&gt; 0-1 &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; 1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1 &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; 0-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1
&lt;/pre&gt;


&lt;p&gt;Clearly, we'll need to parse the data dictionaries before being able to make sense of that.&lt;/p&gt;
&lt;p&gt;Keeping with the CPS's tradition of consistently being inconsistent, the data dictionaries don't have a consistent schema across the years. Here's a typical example for some years (this one is from January 2003):&lt;/p&gt;
&lt;pre class="code literal-block"&gt;NAME         SIZE  DESCRIPTION                          LOCATION

HRHHID          15     HOUSEHOLD IDENTIFIER   (Part 1)             (1 - 15)

                   EDITED UNIVERSE: ALL HHLD's IN SAMPLE

                   Part 1. See Characters 71-75 for Part 2 of the Household Identifier.
                   Use Part 1 only for matching backward in time and use in combination
                   with Part 2 for matching forward in time.
&lt;/pre&gt;


&lt;p&gt;My goal was to extract 4 fields (&lt;code&gt;name&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;end&lt;/code&gt;). Name and size could be taken directly (&lt;code&gt;HRHHID&lt;/code&gt;, and &lt;code&gt;15&lt;/code&gt;). &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; would be pulled from the &lt;code&gt;LOCATION&lt;/code&gt; part.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/generic_data_dictionary_parser.py"&gt;&lt;code&gt;generic_data_dictionary_parser&lt;/code&gt;&lt;/a&gt;, I define a class do this. The main object &lt;code&gt;Parser&lt;/code&gt;, takes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;infile&lt;/code&gt;: the path to a data dictionary we downloaded&lt;/li&gt;
&lt;li&gt;&lt;code&gt;outfile&lt;/code&gt;: path to an &lt;a href="http://pandas.pydata.org/pandas-docs/dev/io.html#hdf5-pytables"&gt;HDF5&lt;/a&gt; file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;style&lt;/code&gt;: A string representing the year of the data dictionary. Different years are formatted differently, so I define a style for each (3 styles in all)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;regex&lt;/code&gt;: This was mostly for testing. If you don't pass a &lt;code&gt;regex&lt;/code&gt; it will be inferred from the style.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The heart of the parser is a regex that matches on lines like &lt;code&gt;HRHHID          15     HOUSEHOLD IDENTIFIER   (Part 1)             (1 - 15)&lt;/code&gt;, but nowhere else. After many hours, failures, and false positives, I came up with something roughly like &lt;code&gt;ur'[\x0c]{0,1}(\w+)[\s\t]*(\d{1,2})[\s\t]*(.*?)[\s\t]*\(*(\d+)\s*-\s*(\d+)\)*$'&lt;/code&gt; &lt;a href="http://regex101.com/r/uH5iH7"&gt;Here's&lt;/a&gt; an explanation, but the gist is that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;\w+&lt;/code&gt; matches words (like &lt;code&gt;HRHHID&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;there's some spaces or tabs &lt;code&gt;[\s\t]*&lt;/code&gt; (yes the CPS mixes spaces and tabs) between that and...&lt;/li&gt;
&lt;li&gt;size &lt;code&gt;\d{1,2}&lt;/code&gt; which is 1 or two columns digits&lt;/li&gt;
&lt;li&gt;the description which we don't really care about&lt;/li&gt;
&lt;li&gt;the start and end positions &lt;code&gt;(*(\d+)\s*-\s*(\d+)\)*$&lt;/code&gt; broken into two groups.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like I said, that's the heart of the parser. Unfortunately I had to pad the file with some 200+ more lines of code to handle special cases, formatting, and mistakes in the data dictionary itself.&lt;/p&gt;
&lt;p&gt;The end result is a nice &lt;code&gt;HDFStore&lt;/code&gt;, with a parsed version of each data dictionary looking like:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;         id  length  start  end
0    HRHHID      15      1   15
1   HRMONTH       2     16   17
2   HRYEAR4       4     18   21
3  HURESPLI       2     22   23
4   HUFINAL       3     24   26
         ...     ...    ...  ...
&lt;/pre&gt;


&lt;p&gt;This can be used as an argument pandas' &lt;a href="http://pandas.pydata.org/pandas-docs/dev/io.html#files-with-fixed-width-columns"&gt;&lt;code&gt;read_fwf&lt;/code&gt;&lt;/a&gt; parser.&lt;/p&gt;
&lt;p&gt;Next time I'll talk about actually parsing the tables and wrangling them into a usable structure. After that, we will finally get to actually analyzing the data.&lt;/p&gt;&lt;/div&gt;</description><guid>http://tomaugspurger.github.io/posts/tackling the cps (part 2).html</guid><pubDate>Tue, 04 Feb 2014 18:00:00 GMT</pubDate></item><item><title>Using Python to tackle the CPS</title><link>http://tomaugspurger.github.io/posts/tackling the cps.html</link><dc:creator>Tom Augspurger</dc:creator><description>&lt;div&gt;&lt;p&gt;The &lt;a href="http://www.census.gov/cps/"&gt;Current Population Survey&lt;/a&gt; is an important source of data for economists. It's modern form took shape in the 70's and unfortunately the data format and distribution shows its age. Some centers like &lt;a href="https://cps.ipums.org/cps/"&gt;IPUMS&lt;/a&gt; have attempted to put a nicer face on accessing the data, but they haven't done everything yet. In this series I'll describe methods I used to fetch, parse, and analyze CPS data for my second year paper. Today I'll describe fetching the data. Everything is available at the paper's &lt;a href="https://github.com/TomAugspurger/dnwr-zlb"&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before diving in, you should know a bit about the data. I was working with the monthly microdata files from the CPS. These are used to estimate things like the unemployment rate you see reported every month. Since around 2002, about 60,000 households are interviewed 8 times each over a year. They're interviewed for 4 months, take 4 months off, and are interviewed for 4 more months after the break. Questions are asked about demographics, education, economic activity (and more).&lt;/p&gt;
&lt;h3&gt;Fetching the Data&lt;/h3&gt;
&lt;p&gt;This was probably the easiest part of the whole project.
The &lt;a href="http://www.nber.org/data/cps_basic.html"&gt;CPS website&lt;/a&gt; has links to all the monthly files and some associated data dictionaries describing the layout of the files (more on this later).&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/monthly_data_downloader.py"&gt;&lt;code&gt;monthly_data_downloader.py&lt;/code&gt;&lt;/a&gt; I fetch files from the CPS website and save them locally.  A common trial was the CPS's inconsistency. Granted, consistency and backwards compatibility are difficult, and sometimes there are valid reasons for making a break, but at times the changes felt excessive and random. Anyway for January 1976 to December 2009 the URL pattern is &lt;code&gt;http://www.nber.org/cps-basic/cpsb****.Z&lt;/code&gt;, and from January 2010 on its &lt;code&gt;http://www.nber.org/cps-basic/jan10&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you're curious the python regex used to match those two patterns is &lt;code&gt;re.compile(r'cpsb\d{4}.Z|\w{3}\d{2}pub.zip|\.[ddf,asc]$')&lt;/code&gt;. Yes that's much clearer.&lt;/p&gt;
&lt;p&gt;I used python's builtin &lt;a href="http://docs.python.org/2/library/urllib2.html"&gt;&lt;code&gt;urllib2&lt;/code&gt;&lt;/a&gt; to fetch the site contents and parse with &lt;code&gt;lxml&lt;/code&gt;. You should &lt;em&gt;really&lt;/em&gt; just use &lt;a href="http://docs.python-requests.org/en/latest/"&gt;requests&lt;/a&gt;, instead of &lt;code&gt;urllib2&lt;/code&gt; but I wanted to keep dependencies for my project slim (I gave up on this hope later).&lt;/p&gt;
&lt;p&gt;A common pattern I used was to parse all of the links on a website, filter out the ones I don't want, and do something with the ones I do want. Here's an example:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ifilter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;partial_matcher&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterlinks&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_fname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt;
    &lt;span class="n"&gt;fname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_fname&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'/'&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;existing&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_dir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fname&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;existing&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;downloader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_dir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'Added {}'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fname&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;&lt;code&gt;root&lt;/code&gt; is just the parsed html from &lt;code&gt;lxml.parse&lt;/code&gt;. &lt;code&gt;iterlinks()&lt;/code&gt; returns an iterable, which I filter through &lt;code&gt;partial_matcher&lt;/code&gt;, a function that matches the filename patterns I described above. Iterators are my favorite feature of Python (not that they are exclusive to Python; I just love easy and flexible they are). The idea of having a list, filtering it, and applying a function to the ones you want is so simple, but so generally applicable. I could have even been a bit more functional and written it as &lt;code&gt;imap(downloader(ifilter(existing, ifilter(partial_matcher, root.iterlinks()))&lt;/code&gt;. Lovely in its own way!&lt;/p&gt;
&lt;p&gt;I do some checking to see if the file exists (so that I can easily download new months). If it is a new month, the filename gets passed to &lt;code&gt;downloader&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;def downloader(link, out_dir, dl_base="http://www.nber.org/cps-basic/"):
    """
    Link is a str like cpsmar06.zip; It is both the end of the url
    and the filename to be used.
    """
    content = urllib2.urlopen(dl_base + link)
    with open(out_dir + link, 'w') as f:
        f.write(content.read())
&lt;/pre&gt;


&lt;p&gt;This reads the data from at url and write writes it do a file.&lt;/p&gt;
&lt;p&gt;Finally, I run &lt;a href="https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/renamer.py"&gt;&lt;code&gt;renamer.py&lt;/code&gt;&lt;/a&gt; to clean up the file names. Just because the CPS is inconsistent doesn't mean that we have to be.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="http://tomaugspurger.github.io/blog/2014/02/04/tackling%20the%20cps%20(part%202)/"&gt;next post&lt;/a&gt; I'll describe parsing the files we just downloaded.&lt;/p&gt;&lt;/div&gt;</description><guid>http://tomaugspurger.github.io/posts/tackling the cps.html</guid><pubDate>Mon, 27 Jan 2014 18:00:00 GMT</pubDate></item></channel></rss>