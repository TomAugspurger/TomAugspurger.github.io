<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Modern Pandas (Part 4): Performance | Tom's Blog</title>
<meta name=keywords content="pandas"><meta name=description content="This is part 4 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Wes McKinney, the creator of pandas, is kind of obsessed with performance. From micro-optimizations for element access, to embedding a fast hash table inside pandas, we all benefit from his and others&rsquo; hard work. This post will focus mainly on making efficient use of pandas and NumPy."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/modern-4-performance/><link crossorigin=anonymous href=/assets/css/stylesheet.ced21e6d3497ee93fed8f8b357448095840179bd510b5ea0e6013078712e6dd1.css integrity="sha256-ztIebTSX7pP+2PizV0SAlYQBeb1RC16g5gEweHEubdE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Modern Pandas (Part 4): Performance"><meta property="og:description" content="This is part 4 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Wes McKinney, the creator of pandas, is kind of obsessed with performance. From micro-optimizations for element access, to embedding a fast hash table inside pandas, we all benefit from his and others&rsquo; hard work. This post will focus mainly on making efficient use of pandas and NumPy."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/modern-4-performance/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-04-08T00:00:00+00:00"><meta property="article:modified_time" content="2016-04-08T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Modern Pandas (Part 4): Performance"><meta name=twitter:description content="This is part 4 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Wes McKinney, the creator of pandas, is kind of obsessed with performance. From micro-optimizations for element access, to embedding a fast hash table inside pandas, we all benefit from his and others&rsquo; hard work. This post will focus mainly on making efficient use of pandas and NumPy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":3,"name":"Modern Pandas (Part 4): Performance","item":"https://tomaugspurger.net/posts/modern-4-performance/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Modern Pandas (Part 4): Performance","name":"Modern Pandas (Part 4): Performance","description":"This is part 4 in my series on writing modern idiomatic pandas.\nModern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Wes McKinney, the creator of pandas, is kind of obsessed with performance. From micro-optimizations for element access, to embedding a fast hash table inside pandas, we all benefit from his and others\u0026rsquo; hard work. This post will focus mainly on making efficient use of pandas and NumPy.","keywords":["pandas"],"articleBody":" This is part 4 in my series on writing modern idiomatic pandas.\nModern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Wes McKinney, the creator of pandas, is kind of obsessed with performance. From micro-optimizations for element access, to embedding a fast hash table inside pandas, we all benefit from his and others’ hard work. This post will focus mainly on making efficient use of pandas and NumPy.\nOne thing I’ll explicitly not touch on is storage formats. Performance is just one of many factors that go into choosing a storage format. Just know that pandas can talk to many formats, and the format that strikes the right balance between performance, portability, data-types, metadata handling, etc., is an ongoing topic of discussion.\n%matplotlib inline import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns if int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)): import prep # noqa sns.set_style('ticks') sns.set_context('talk') pd.options.display.max_rows = 10 Constructors It’s pretty common to have many similar sources (say a bunch of CSVs) that need to be combined into a single DataFrame. There are two routes to the same end:\nInitialize one DataFrame and append to that Make many smaller DataFrames and concatenate at the end For pandas, the second option is faster. DataFrame appends are expensive relative to a list append. Depending on the values, pandas might have to recast the data to a different type. And indexes are immutable, so each time you append pandas has to create an entirely new one.\nIn the last section we downloaded a bunch of weather files, one per state, writing each to a separate CSV. One could imagine coming back later to read them in, using the following code.\nThe idiomatic python way\nfiles = glob.glob('weather/*.csv') columns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp', 'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3'] # init empty DataFrame, like you might for a list weather = pd.DataFrame(columns=columns) for fp in files: city = pd.read_csv(fp, columns=columns) weather.append(city) This is pretty standard code, quite similar to building up a list of tuples, say. The only nitpick is that you’d probably use a list-comprehension if you were just making a list. But we don’t have special syntax for DataFrame-comprehensions (if only), so you’d fall back to the “initialize empty container, append to said container” pattern.\nBut there’s a better, pandorable, way\nfiles = glob.glob('weather/*.csv') weather_dfs = [pd.read_csv(fp, names=columns) for fp in files] weather = pd.concat(weather_dfs) Subjectively this is cleaner and more beautiful. There’s fewer lines of code. You don’t have this extraneous detail of building an empty DataFrame. And objectively the pandorable way is faster, as we’ll test next.\nWe’ll define two functions for building an identical DataFrame. The first append_df, creates an empty DataFrame and appends to it. The second, concat_df, creates many DataFrames, and concatenates them at the end. We also write a short decorator that runs the functions a handful of times and records the results.\nimport time size_per = 5000 N = 100 cols = list('abcd') def timed(n=30): ''' Running a microbenchmark. Never use this. ''' def deco(func): def wrapper(*args, **kwargs): timings = [] for i in range(n): t0 = time.time() func(*args, **kwargs) t1 = time.time() timings.append(t1 - t0) return timings return wrapper return deco @timed(60) def append_df(): ''' The pythonic (bad) way ''' df = pd.DataFrame(columns=cols) for _ in range(N): df.append(pd.DataFrame(np.random.randn(size_per, 4), columns=cols)) return df @timed(60) def concat_df(): ''' The pandorabe (good) way ''' dfs = [pd.DataFrame(np.random.randn(size_per, 4), columns=cols) for _ in range(N)] return pd.concat(dfs, ignore_index=True) t_append = append_df() t_concat = concat_df() timings = (pd.DataFrame({\"Append\": t_append, \"Concat\": t_concat}) .stack() .reset_index() .rename(columns={0: 'Time (s)', 'level_1': 'Method'})) timings.head() level_0 Method Time (s) 0 0 Append 0.171326 1 0 Concat 0.096445 2 1 Append 0.155903 3 1 Concat 0.095105 4 2 Append 0.155185 plt.figure(figsize=(4, 6)) sns.boxplot(x='Method', y='Time (s)', data=timings) sns.despine() plt.tight_layout() Datatypes The pandas type system essentially NumPy’s with a few extensions (categorical, datetime64 with timezone, timedelta64). An advantage of the DataFrame over a 2-dimensional NumPy array is that the DataFrame can have columns of various types within a single table. That said, each column should have a specific dtype; you don’t want to be mixing bools with ints with strings within a single column. For one thing, this is slow. It forces the column to be have an object dtype (the fallback python-object container type), which means you don’t get any of the type-specific optimizations in pandas or NumPy. For another, it means you’re probably violating the maxims of tidy data, which we’ll discuss next time.\nWhen should you have object columns? There are a few places where the NumPy / pandas type system isn’t as rich as you might like. There’s no integer NA (at the moment anyway), so if you have any missing values, represented by NaN, your otherwise integer column will be floats. There’s also no date dtype (distinct from datetime). Consider the needs of your application: can you treat an integer 1 as 1.0? Can you treat date(2016, 1, 1) as datetime(2016, 1, 1, 0, 0)? In my experience, this is rarely a problem other than when writing to something with a stricter schema like a database. But at that point it’s fine to cast to one of the less performant types, since you’re just not doing numeric operations anymore.\nThe last case of object dtype data is text data. Pandas doesn’t have any fixed-width string dtypes, so you’re stuck with python objects. There is an important exception here, and that’s low-cardinality text data, for which you’ll want to use the category dtype (see below).\nIf you have object data (either strings or python objects) that needs to be converted, checkout the to_numeric, to_datetime and to_timedelta methods.\nIteration, Apply, And Vectorization We know that “Python is slow” (scare quotes since that statement is too broad to be meaningful). There are various steps that can be taken to improve your code’s performance from relatively simple changes, to rewriting your code in a lower-level language, to trying to parallelize it. And while you might have many options, there’s typically an order you would proceed in.\nFirst (and I know it’s cliché to say so, but still) benchmark your code. Make sure you actually need to spend time optimizing it. There are many options for benchmarking and visualizing where things are slow.\nSecond, consider your algorithm. Make sure you aren’t doing more work than you need to. A common one I see is doing a full sort on an array, just to select the N largest or smallest items. Pandas has methods for that.\ndf = pd.read_csv(\"data/347136217_T_ONTIME.csv\") delays = df['DEP_DELAY'] # Select the 5 largest delays delays.nlargest(5).sort_values() 112623 1480.0 158136 1545.0 152911 1934.0 60246 1970.0 59719 2755.0 Name: DEP_DELAY, dtype: float64 delays.nsmallest(5).sort_values() 300895 -59.0 235921 -58.0 197897 -56.0 332533 -56.0 344542 -55.0 Name: DEP_DELAY, dtype: float64 We follow up the nlargest or nsmallest with a sort (the result of nlargest/smallest is unordered), but it’s much easier to sort 5 items that 500,000. The timings bear this out:\n%timeit delays.sort_values().tail(5) 31 ms ± 1.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit delays.nlargest(5).sort_values() 7.87 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) “Use the right algorithm” is easy to say, but harder to apply in practice since you have to actually figure out the best algorithm to use. That one comes down to experience.\nAssuming you’re at a spot that needs optimizing, and you’ve got the correct algorithm, and there isn’t a readily available optimized version of what you need in pandas/numpy/scipy/scikit-learn/statsmodels/…, then what?\nThe first place to turn is probably a vectorized NumPy implementation. Vectorization here means operating directly on arrays, rather than looping over lists scalars. This is generally much less work than rewriting it in something like Cython, and you can get pretty good results just by making effective use of NumPy and pandas. While not every operation can be vectorized, many can.\nLet’s work through an example calculating the Great-circle distance between airports. Grab the table of airport latitudes and longitudes from the BTS website and extract it to a CSV.\nfrom utils import download_airports import zipfile if not os.path.exists(\"data/airports.csv.zip\"): download_airports() coord = (pd.read_csv(\"data/airports.csv.zip\", index_col=['AIRPORT'], usecols=['AIRPORT', 'LATITUDE', 'LONGITUDE']) .groupby(level=0).first() .dropna() .sample(n=500, random_state=42) .sort_index()) coord.head() LATITUDE LONGITUDE AIRPORT 8F3 33.623889 -101.240833 A03 58.457500 -154.023333 A09 60.482222 -146.582222 A18 63.541667 -150.993889 A24 59.331667 -135.896667 For whatever reason, suppose we’re interested in all the pairwise distances (I’ve limited it to just a sample of 500 airports to make this manageable. In the real world you probably don’t need all the pairwise distances and would be better off with a tree. Remember: think about what you actually need, and find the right algorithm for that).\nMultiIndexes have an alternative from_product constructor for getting the Cartesian product of the arrays you pass in. We’ll give it coords.index twice (to get its Cartesian product with itself). That gives a MultiIndex of all the combination. With some minor reshaping of coords we’ll have a DataFrame with all the latitude/longitude pairs.\nidx = pd.MultiIndex.from_product([coord.index, coord.index], names=['origin', 'dest']) pairs = pd.concat([coord.add_suffix('_1').reindex(idx, level='origin'), coord.add_suffix('_2').reindex(idx, level='dest')], axis=1) pairs.head() LATITUDE_1 LONGITUDE_1 LATITUDE_2 LONGITUDE_2 origin dest 8F3 8F3 33.623889 -101.240833 33.623889 -101.240833 A03 33.623889 -101.240833 58.457500 -154.023333 A09 33.623889 -101.240833 60.482222 -146.582222 A18 33.623889 -101.240833 63.541667 -150.993889 A24 33.623889 -101.240833 59.331667 -135.896667 idx = idx[idx.get_level_values(0) \u003c= idx.get_level_values(1)] len(idx) 125250 We’ll break that down a bit, but don’t lose sight of the real target: our great-circle distance calculation.\nThe add_suffix (and add_prefix) method is handy for quickly renaming the columns.\ncoord.add_suffix('_1').head() LATITUDE_1 LONGITUDE_1 AIRPORT 8F3 33.623889 -101.240833 A03 58.457500 -154.023333 A09 60.482222 -146.582222 A18 63.541667 -150.993889 A24 59.331667 -135.896667 Alternatively you could use the more general .rename like coord.rename(columns=lambda x: x + '_1').\nNext, we have the reindex. Like I mentioned in the prior chapter, indexes are crucial to pandas. .reindex is all about aligning a Series or DataFrame to a given index. In this case we use .reindex to align our original DataFrame to the new MultiIndex of combinations. By default, the output will have the original value if that index label was already present, and NaN otherwise. If we just called coord.reindex(idx), with no additional arguments, we’d get a DataFrame of all NaNs.\ncoord.reindex(idx).head() LATITUDE LONGITUDE origin dest 8F3 8F3 NaN NaN A03 NaN NaN A09 NaN NaN A18 NaN NaN A24 NaN NaN That’s because there weren’t any values of idx that were in coord.index, which makes sense since coord.index is just a regular one-level Index, while idx is a MultiIndex. We use the level keyword to handle the transition from the original single-level Index, to the two-leveled idx.\nlevel : int or name\nBroadcast across a level, matching Index values on the passed MultiIndex level\ncoord.reindex(idx, level='dest').head() LATITUDE LONGITUDE origin dest 8F3 8F3 33.623889 -101.240833 A03 58.457500 -154.023333 A09 60.482222 -146.582222 A18 63.541667 -150.993889 A24 59.331667 -135.896667 If you ever need to do an operation that mixes regular single-level indexes with Multilevel Indexes, look for a level keyword argument. For example, all the arithmatic methods (.mul, .add, etc.) have them.\nThis is a bit wasteful since the distance from airport A to B is the same as B to A. We could easily fix this with a idx = idx[idx.get_level_values(0) \u003c= idx.get_level_values(1)], but we’ll ignore that for now.\nQuick tangent, I got some… let’s say skepticism, on my last piece about the value of indexes. Here’s an alternative version for the skeptics\nfrom itertools import product, chain coord2 = coord.reset_index() x = product(coord2.add_suffix('_1').itertuples(index=False), coord2.add_suffix('_2').itertuples(index=False)) y = [list(chain.from_iterable(z)) for z in x] df2 = (pd.DataFrame(y, columns=['origin', 'LATITUDE_1', 'LONGITUDE_1', 'dest', 'LATITUDE_1', 'LONGITUDE_2']) .set_index(['origin', 'dest'])) df2.head() LATITUDE_1 LONGITUDE_1 LATITUDE_1 LONGITUDE_2 origin dest 8F3 8F3 33.623889 -101.240833 33.623889 -101.240833 A03 33.623889 -101.240833 58.457500 -154.023333 A09 33.623889 -101.240833 60.482222 -146.582222 A18 33.623889 -101.240833 63.541667 -150.993889 A24 33.623889 -101.240833 59.331667 -135.896667 It’s also readable (it’s Python after all), though a bit slower. To me the .reindex method seems more natural. My thought process was, “I need all the combinations of origin \u0026 destination (MultiIndex.from_product). Now I need to align this original DataFrame to this new MultiIndex (coords.reindex).”\nWith that diversion out of the way, let’s turn back to our great-circle distance calculation. Our first implementation is pure python. The algorithm itself isn’t too important, all that matters is that we’re doing math operations on scalars.\nimport math def gcd_py(lat1, lng1, lat2, lng2): ''' Calculate great circle distance between two points. http://www.johndcook.com/blog/python_longitude_latitude/ Parameters ---------- lat1, lng1, lat2, lng2: float Returns ------- distance: distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers. ''' # python2 users will have to use ascii identifiers (or upgrade) degrees_to_radians = math.pi / 180.0 ϕ1 = (90 - lat1) * degrees_to_radians ϕ2 = (90 - lat2) * degrees_to_radians θ1 = lng1 * degrees_to_radians θ2 = lng2 * degrees_to_radians cos = (math.sin(ϕ1) * math.sin(ϕ2) * math.cos(θ1 - θ2) + math.cos(ϕ1) * math.cos(ϕ2)) # round to avoid precision issues on identical points causing ValueErrors cos = round(cos, 8) arc = math.acos(cos) return arc * 6373 # radius of earth, in kilometers The second implementation uses NumPy. Aside from numpy having a builtin deg2rad convenience function (which is probably a bit slower than multiplying by a constant $\\frac{\\pi}{180}$), basically all we’ve done is swap the math prefix for np. Thanks to NumPy’s broadcasting, we can write code that works on scalars or arrays of conformable shape.\ndef gcd_vec(lat1, lng1, lat2, lng2): ''' Calculate great circle distance. http://www.johndcook.com/blog/python_longitude_latitude/ Parameters ---------- lat1, lng1, lat2, lng2: float or array of float Returns ------- distance: distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers. ''' # python2 users will have to use ascii identifiers ϕ1 = np.deg2rad(90 - lat1) ϕ2 = np.deg2rad(90 - lat2) θ1 = np.deg2rad(lng1) θ2 = np.deg2rad(lng2) cos = (np.sin(ϕ1) * np.sin(ϕ2) * np.cos(θ1 - θ2) + np.cos(ϕ1) * np.cos(ϕ2)) arc = np.arccos(cos) return arc * 6373 To use the python version on our DataFrame, we can either iterate…\n%%time pd.Series([gcd_py(*x) for x in pairs.itertuples(index=False)], index=pairs.index) CPU times: user 833 ms, sys: 12.7 ms, total: 846 ms Wall time: 847 ms origin dest 8F3 8F3 0.000000 A03 4744.967448 A09 4407.533212 A18 4744.593127 A24 3820.092688 ... ZZU YUY 12643.665960 YYL 13687.592278 ZBR 4999.647307 ZXO 14925.531303 ZZU 0.000000 Length: 250000, dtype: float64 Or use DataFrame.apply.\n%%time r = pairs.apply(lambda x: gcd_py(x['LATITUDE_1'], x['LONGITUDE_1'], x['LATITUDE_2'], x['LONGITUDE_2']), axis=1); CPU times: user 14.4 s, sys: 61.2 ms, total: 14.4 s Wall time: 14.4 s But as you can see, you don’t want to use apply, especially with axis=1 (calling the function on each row). It’s doing a lot more work handling dtypes in the background, and trying to infer the correct output shape that are pure overhead in this case. On top of that, it has to essentially use a for loop internally.\nYou rarely want to use DataFrame.apply and almost never should use it with axis=1. Better to write functions that take arrays, and pass those in directly. Like we did with the vectorized version\n%%time r = gcd_vec(pairs['LATITUDE_1'], pairs['LONGITUDE_1'], pairs['LATITUDE_2'], pairs['LONGITUDE_2']) CPU times: user 31.1 ms, sys: 26.4 ms, total: 57.5 ms Wall time: 37.2 ms /Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in arccos r.head() origin dest 8F3 8F3 0.000000 A03 4744.967484 A09 4407.533240 A18 4744.593111 A24 3820.092639 dtype: float64 I try not to use the word “easy” when teaching, but that optimization was easy right? Why then, do I come across uses of apply, in my code and others’, even when the vectorized version is available? The difficulty lies in knowing about broadcasting, and seeing where to apply it.\nFor example, the README for lifetimes (by Cam Davidson Pilon, also author of Bayesian Methods for Hackers, lifelines, and Data Origami) used to have an example of passing this method into a DataFrame.apply.\ndata.apply(lambda r: bgf.conditional_expected_number_of_purchases_up_to_time( t, r['frequency'], r['recency'], r['T']), axis=1 ) If you look at the function I linked to, it’s doing a fairly complicated computation involving a negative log likelihood and the Gamma function from scipy.special. But crucially, it was already vectorized. We were able to change the example to just pass the arrays (Series in this case) into the function, rather than applying the function to each row.\nbgf.conditional_expected_number_of_purchases_up_to_time( t, data['frequency'], data['recency'], data['T'] ) This got us another 30x speedup on the example dataset. I bring this up because it’s very natural to have to translate an equation to code and think, “Ok now I need to apply this function to each row”, so you reach for DataFrame.apply. See if you can just pass in the NumPy array or Series itself instead.\nNot all operations this easy to vectorize. Some operations are iterative by nature, and rely on the results of surrounding computations to proceed. In cases like this you can hope that one of the scientific python libraries has implemented it efficiently for you, or write your own solution using Numba / C / Cython / Fortran.\nOther examples take a bit more thought or knowledge to vectorize. Let’s look at this example, taken from Jeff Reback’s PyData London talk, that groupwise normalizes a dataset by subtracting the mean and dividing by the standard deviation for each group.\nimport random def create_frame(n, n_groups): # just setup code, not benchmarking this stamps = pd.date_range('20010101', periods=n, freq='ms') random.shuffle(stamps.values) return pd.DataFrame({'name': np.random.randint(0,n_groups,size=n), 'stamp': stamps, 'value': np.random.randint(0,n,size=n), 'value2': np.random.randn(n)}) df = create_frame(1000000,10000) def f_apply(df): # Typical transform return df.groupby('name').value2.apply(lambda x: (x-x.mean())/x.std()) def f_unwrap(df): # \"unwrapped\" g = df.groupby('name').value2 v = df.value2 return (v-g.transform(np.mean))/g.transform(np.std) Timing it we see that the “unwrapped” version, get’s quite a bit better performance.\n%timeit f_apply(df) 4.28 s ± 161 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit f_unwrap(df) 53.3 ms ± 1.97 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) Pandas GroupBy objects intercept calls for common functions like mean, sum, etc. and substitutes them with optimized Cython versions. So the unwrapped .transform(np.mean) and .transform(np.std) are fast, while the x.mean and x.std in the .apply(lambda x: x - x.mean()/x.std()) aren’t.\nGroupby.apply is always going to be around, beacuse it offers maximum flexibility. If you need to fit a model on each group and create additional columns in the process, it can handle that. It just might not be the fastest (which may be OK sometimes).\nThis last example is admittedly niche. I’d like to think that there aren’t too many places in pandas where the natural thing to do .transform((x - x.mean()) / x.std()) is slower than the less obvious alternative. Ideally the user wouldn’t have to know about GroupBy having special fast implementations of common methods. But that’s where we are now.\nCategoricals Thanks to some great work by Jan Schulz, Jeff Reback, and others, pandas 0.15 gained a new Categorical data type. Categoricals are nice for many reasons beyond just efficiency, but we’ll focus on that here.\nCategoricals are an efficient way of representing data (typically strings) that have a low cardinality, i.e. relatively few distinct values relative to the size of the array. Internally, a Categorical stores the categories once, and an array of codes, which are just integers that indicate which category belongs there. Since it’s cheaper to store a code than a category, we save on memory (shown next).\nimport string s = pd.Series(np.random.choice(list(string.ascii_letters), 100000)) print('{:0.2f} KB'.format(s.memory_usage(index=False) / 1000)) 800.00 KB c = s.astype('category') print('{:0.2f} KB'.format(c.memory_usage(index=False) / 1000)) 102.98 KB Beyond saving memory, having codes and a fixed set of categories offers up a bunch of algorithmic optimizations that pandas and others can take advantage of.\nMatthew Rocklin has a very nice post on using categoricals, and optimizing code in general.\nGoing Further The pandas documentation has a section on enhancing performance, focusing on using Cython or numba to speed up a computation. I’ve focused more on the lower-hanging fruit of picking the right algorithm, vectorizing your code, and using pandas or numpy more effetively. There are further optimizations availble if these aren’t enough.\nSummary This post was more about how to make effective use of numpy and pandas, than writing your own highly-optimized code. In my day-to-day work of data analysis it’s not worth the time to write and compile a cython extension. I’d rather rely on pandas to be fast at what matters (label lookup on large arrays, factorizations for groupbys and merges, numerics). If you want to learn more about what pandas does to make things fast, checkout Jeff Tratner’ talk from PyData Seattle talk on pandas’ internals.\nNext time we’ll look at a differnt kind of optimization: using the Tidy Data principles to facilitate efficient data analysis.\n","wordCount":"3411","inLanguage":"en","datePublished":"2016-04-08T00:00:00Z","dateModified":"2016-04-08T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/modern-4-performance/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Modern Pandas (Part 4): Performance</h1><div class=post-meta><span title='2016-04-08 00:00:00 +0000 UTC'>April 8, 2016</span></div></header><div class=post-content><hr><p>This is part 4 in my series on writing modern idiomatic pandas.</p><ul><li><a href=/posts/modern-1-intro>Modern Pandas</a></li><li><a href=/posts/method-chaining>Method Chaining</a></li><li><a href=/posts/modern-3-indexes>Indexes</a></li><li><a href=/posts/modern-4-performance>Fast Pandas</a></li><li><a href=/posts/modern-5-tidy>Tidy Data</a></li><li><a href=/posts/modern-6-visualization>Visualization</a></li><li><a href=/posts/modern-7-timeseries>Time Series</a></li><li><a href=/posts/modern-8-scaling>Scaling</a></li></ul><hr><p><a href=https://twitter.com/wesmckinn>Wes McKinney</a>, the creator of pandas, is kind of obsessed with performance. From micro-optimizations for element access, to <a href=https://github.com/pydata/pandas/tree/master/pandas/src/klib>embedding</a> a fast hash table inside pandas, we all benefit from his and others&rsquo; hard work.
This post will focus mainly on making efficient use of pandas and NumPy.</p><p>One thing I&rsquo;ll explicitly not touch on is storage formats.
Performance is just one of many factors that go into choosing a storage format.
Just know that pandas can talk to <a href=http://pandas.pydata.org/pandas-docs/version/0.18.0/io.html>many formats</a>, and the format that strikes the right balance between performance, portability, data-types, metadata handling, etc., is an <a href=http://blog.cloudera.com/blog/2016/03/feather-a-fast-on-disk-format-for-data-frames-for-r-and-python-powered-by-apache-arrow/>ongoing</a> topic of discussion.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> int(os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;MODERN_PANDAS_EPUB&#34;</span>, <span style=color:#ae81ff>0</span>)):
</span></span><span style=display:flex><span>    <span style=color:#f92672>import</span> prep <span style=color:#75715e># noqa</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>set_style(<span style=color:#e6db74>&#39;ticks&#39;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>set_context(<span style=color:#e6db74>&#39;talk&#39;</span>)
</span></span><span style=display:flex><span>pd<span style=color:#f92672>.</span>options<span style=color:#f92672>.</span>display<span style=color:#f92672>.</span>max_rows <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><h2 id=constructors>Constructors<a hidden class=anchor aria-hidden=true href=#constructors>#</a></h2><p>It&rsquo;s pretty common to have many similar sources (say a bunch of CSVs) that need to be combined into a single DataFrame. There are two routes to the same end:</p><ol><li>Initialize one DataFrame and append to that</li><li>Make many smaller DataFrames and concatenate at the end</li></ol><p>For pandas, the second option is faster.
DataFrame appends are expensive relative to a list append.
Depending on the values, pandas might have to recast the data to a different type.
And indexes are immutable, so each time you append pandas has to create an entirely new one.</p><p>In the last section we downloaded a bunch of weather files, one per state, writing each to a separate CSV.
One could imagine coming back later to read them in, using the following code.</p><p>The idiomatic python way</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>files <span style=color:#f92672>=</span> glob<span style=color:#f92672>.</span>glob(<span style=color:#e6db74>&#39;weather/*.csv&#39;</span>)
</span></span><span style=display:flex><span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;station&#39;</span>, <span style=color:#e6db74>&#39;date&#39;</span>, <span style=color:#e6db74>&#39;tmpf&#39;</span>, <span style=color:#e6db74>&#39;relh&#39;</span>, <span style=color:#e6db74>&#39;sped&#39;</span>, <span style=color:#e6db74>&#39;mslp&#39;</span>,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#39;p01i&#39;</span>, <span style=color:#e6db74>&#39;vsby&#39;</span>, <span style=color:#e6db74>&#39;gust_mph&#39;</span>, <span style=color:#e6db74>&#39;skyc1&#39;</span>, <span style=color:#e6db74>&#39;skyc2&#39;</span>, <span style=color:#e6db74>&#39;skyc3&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># init empty DataFrame, like you might for a list</span>
</span></span><span style=display:flex><span>weather <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(columns<span style=color:#f92672>=</span>columns)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> fp <span style=color:#f92672>in</span> files:
</span></span><span style=display:flex><span>    city <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(fp, columns<span style=color:#f92672>=</span>columns)
</span></span><span style=display:flex><span>    weather<span style=color:#f92672>.</span>append(city)
</span></span></code></pre></div><p>This is pretty standard code, quite similar to building up a list of tuples, say.
The only nitpick is that you&rsquo;d probably use a list-comprehension if you were just making a list.
But we don&rsquo;t have special syntax for DataFrame-comprehensions (if only), so you&rsquo;d fall back to the &ldquo;initialize empty container, append to said container&rdquo; pattern.</p><p>But there&rsquo;s a better, pandorable, way</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>files <span style=color:#f92672>=</span> glob<span style=color:#f92672>.</span>glob(<span style=color:#e6db74>&#39;weather/*.csv&#39;</span>)
</span></span><span style=display:flex><span>weather_dfs <span style=color:#f92672>=</span> [pd<span style=color:#f92672>.</span>read_csv(fp, names<span style=color:#f92672>=</span>columns) <span style=color:#66d9ef>for</span> fp <span style=color:#f92672>in</span> files]
</span></span><span style=display:flex><span>weather <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>concat(weather_dfs)
</span></span></code></pre></div><p>Subjectively this is cleaner and more beautiful.
There&rsquo;s fewer lines of code.
You don&rsquo;t have this extraneous detail of building an empty DataFrame.
And objectively the pandorable way is faster, as we&rsquo;ll test next.</p><p>We&rsquo;ll define two functions for building an identical DataFrame. The first <code>append_df</code>, creates an empty DataFrame and appends to it. The second, <code>concat_df</code>, creates many DataFrames, and concatenates them at the end. We also write a short decorator that runs the functions a handful of times and records the results.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>size_per <span style=color:#f92672>=</span> <span style=color:#ae81ff>5000</span>
</span></span><span style=display:flex><span>N <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>cols <span style=color:#f92672>=</span> list(<span style=color:#e6db74>&#39;abcd&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>timed</span>(n<span style=color:#f92672>=</span><span style=color:#ae81ff>30</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Running a microbenchmark. Never use this.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>deco</span>(func):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wrapper</span>(<span style=color:#f92672>*</span>args, <span style=color:#f92672>**</span>kwargs):
</span></span><span style=display:flex><span>            timings <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n):
</span></span><span style=display:flex><span>                t0 <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>                func(<span style=color:#f92672>*</span>args, <span style=color:#f92672>**</span>kwargs)
</span></span><span style=display:flex><span>                t1 <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>                timings<span style=color:#f92672>.</span>append(t1 <span style=color:#f92672>-</span> t0)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> timings
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> wrapper
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> deco
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#a6e22e>@timed</span>(<span style=color:#ae81ff>60</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>append_df</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    The pythonic (bad) way
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(columns<span style=color:#f92672>=</span>cols)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(N):
</span></span><span style=display:flex><span>        df<span style=color:#f92672>.</span>append(pd<span style=color:#f92672>.</span>DataFrame(np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(size_per, <span style=color:#ae81ff>4</span>), columns<span style=color:#f92672>=</span>cols))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> df
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@timed</span>(<span style=color:#ae81ff>60</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>concat_df</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    The pandorabe (good) way
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    dfs <span style=color:#f92672>=</span> [pd<span style=color:#f92672>.</span>DataFrame(np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(size_per, <span style=color:#ae81ff>4</span>), columns<span style=color:#f92672>=</span>cols)
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(N)]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>concat(dfs, ignore_index<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>t_append <span style=color:#f92672>=</span> append_df()
</span></span><span style=display:flex><span>t_concat <span style=color:#f92672>=</span> concat_df()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>timings <span style=color:#f92672>=</span> (pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#34;Append&#34;</span>: t_append, <span style=color:#e6db74>&#34;Concat&#34;</span>: t_concat})
</span></span><span style=display:flex><span>             <span style=color:#f92672>.</span>stack()
</span></span><span style=display:flex><span>             <span style=color:#f92672>.</span>reset_index()
</span></span><span style=display:flex><span>             <span style=color:#f92672>.</span>rename(columns<span style=color:#f92672>=</span>{<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#39;Time (s)&#39;</span>,
</span></span><span style=display:flex><span>                              <span style=color:#e6db74>&#39;level_1&#39;</span>: <span style=color:#e6db74>&#39;Method&#39;</span>}))
</span></span><span style=display:flex><span>timings<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>level_0</th><th>Method</th><th>Time (s)</th></tr></thead><tbody><tr><th>0</th><td>0</td><td>Append</td><td>0.171326</td></tr><tr><th>1</th><td>0</td><td>Concat</td><td>0.096445</td></tr><tr><th>2</th><td>1</td><td>Append</td><td>0.155903</td></tr><tr><th>3</th><td>1</td><td>Concat</td><td>0.095105</td></tr><tr><th>4</th><td>2</td><td>Append</td><td>0.155185</td></tr></tbody></table></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>boxplot(x<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Method&#39;</span>, y<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Time (s)&#39;</span>, data<span style=color:#f92672>=</span>timings)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>tight_layout()
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_4_performance_6_0.png alt=png></p><h2 id=datatypes>Datatypes<a hidden class=anchor aria-hidden=true href=#datatypes>#</a></h2><p>The pandas type system essentially <a href=http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html>NumPy&rsquo;s</a> with a few extensions (<code>categorical</code>, <code>datetime64</code> with timezone, <code>timedelta64</code>).
An advantage of the DataFrame over a 2-dimensional NumPy array is that the DataFrame can have columns of various types within a single table.
That said, each column should have a specific dtype; you don&rsquo;t want to be mixing bools with ints with strings within a single column.
For one thing, this is slow.
It forces the column to be have an <code>object</code> dtype (the fallback python-object container type), which means you don&rsquo;t get any of the type-specific optimizations in pandas or NumPy.
For another, it means you&rsquo;re probably violating the maxims of tidy data, which we&rsquo;ll discuss next time.</p><p>When should you have <code>object</code> columns?
There are a few places where the NumPy / pandas type system isn&rsquo;t as rich as you might like.
There&rsquo;s no integer NA (at the moment anyway), so if you have any missing values, represented by <code>NaN</code>, your otherwise integer column will be floats.
There&rsquo;s also no <code>date</code> dtype (distinct from <code>datetime</code>).
Consider the needs of your application: can you treat an integer <code>1</code> as <code>1.0</code>?
Can you treat <code>date(2016, 1, 1)</code> as <code>datetime(2016, 1, 1, 0, 0)</code>?
In my experience, this is rarely a problem other than when writing to something with a stricter schema like a database.
But at that point it&rsquo;s fine to cast to one of the less performant types, since you&rsquo;re just not doing numeric operations anymore.</p><p>The last case of <code>object</code> dtype data is text data.
Pandas doesn&rsquo;t have any fixed-width string dtypes, so you&rsquo;re stuck with python objects.
There is an important exception here, and that&rsquo;s low-cardinality text data, for which you&rsquo;ll want to use the <code>category</code> dtype (see below).</p><p>If you have object data (either strings or python objects) that needs to be converted, checkout the <a href=http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.to_numeric.html><code>to_numeric</code></a>, <a href=http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.to_datetime.html><code>to_datetime</code></a> and <a href=http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.to_timedelta.html><code>to_timedelta</code></a> methods.</p><h2 id=iteration-apply-and-vectorization>Iteration, Apply, And Vectorization<a hidden class=anchor aria-hidden=true href=#iteration-apply-and-vectorization>#</a></h2><p>We know that <a href=https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/>&ldquo;Python is slow&rdquo;</a> (scare quotes since that statement is too broad to be meaningful).
There are various steps that can be taken to improve your code&rsquo;s performance from relatively simple changes, to rewriting your code in a lower-level language, to trying to parallelize it.
And while you might have many options, there&rsquo;s typically an order you would proceed in.</p><p>First (and I know it&rsquo;s cliché to say so, but still) benchmark your code.
Make sure you actually need to spend time optimizing it.
There are <a href=https://github.com/nvdv/vprof>many</a> <a href=https://jiffyclub.github.io/snakeviz/>options</a> <a href=https://github.com/rkern/line_profiler>for</a> <a href=https://docs.python.org/3.5/library/timeit.html>benchmarking</a> and visualizing where things are slow.</p><p>Second, consider your algorithm.
Make sure you aren&rsquo;t doing more work than you need to.
A common one I see is doing a full sort on an array, just to select the <code>N</code> largest or smallest items.
Pandas has methods for that.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;data/347136217_T_ONTIME.csv&#34;</span>)
</span></span><span style=display:flex><span>delays <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;DEP_DELAY&#39;</span>]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Select the 5 largest delays</span>
</span></span><span style=display:flex><span>delays<span style=color:#f92672>.</span>nlargest(<span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>sort_values()
</span></span></code></pre></div><pre><code>112623    1480.0
158136    1545.0
152911    1934.0
60246     1970.0
59719     2755.0
Name: DEP_DELAY, dtype: float64
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>delays<span style=color:#f92672>.</span>nsmallest(<span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>sort_values()
</span></span></code></pre></div><pre><code>300895   -59.0
235921   -58.0
197897   -56.0
332533   -56.0
344542   -55.0
Name: DEP_DELAY, dtype: float64
</code></pre><p>We follow up the <code>nlargest</code> or <code>nsmallest</code> with a sort (the result of <code>nlargest/smallest</code> is unordered), but it&rsquo;s much easier to sort 5 items that 500,000. The timings bear this out:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>timeit delays<span style=color:#f92672>.</span>sort_values()<span style=color:#f92672>.</span>tail(<span style=color:#ae81ff>5</span>)
</span></span></code></pre></div><pre><code>31 ms ± 1.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>timeit delays<span style=color:#f92672>.</span>nlargest(<span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>sort_values()
</span></span></code></pre></div><pre><code>7.87 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre><p>&ldquo;Use the right algorithm&rdquo; is easy to say, but harder to apply in practice since you have to actually figure out the best algorithm to use.
That one comes down to experience.</p><p>Assuming you&rsquo;re at a spot that needs optimizing, and you&rsquo;ve got the correct algorithm, <em>and</em> there isn&rsquo;t a readily available optimized version of what you need in pandas/numpy/scipy/scikit-learn/statsmodels/&mldr;, then what?</p><p>The first place to turn is probably a vectorized NumPy implementation.
Vectorization here means operating directly on arrays, rather than looping over lists scalars.
This is generally much less work than rewriting it in something like Cython, and you can get pretty good results just by making <em>effective</em> use of NumPy and pandas.
While not every operation can be vectorized, many can.</p><p>Let&rsquo;s work through an example calculating the <a href=https://en.wikipedia.org/wiki/Great-circle_distance>Great-circle distance</a> between airports.
Grab the table of airport latitudes and longitudes from the <a href="http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=288&amp;DB_Short_Name=Aviation%20Support%20Tables">BTS website</a> and extract it to a CSV.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> utils <span style=color:#f92672>import</span> download_airports
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> zipfile
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(<span style=color:#e6db74>&#34;data/airports.csv.zip&#34;</span>):
</span></span><span style=display:flex><span>    download_airports()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>coord <span style=color:#f92672>=</span> (pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;data/airports.csv.zip&#34;</span>, index_col<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;AIRPORT&#39;</span>],
</span></span><span style=display:flex><span>                     usecols<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;AIRPORT&#39;</span>, <span style=color:#e6db74>&#39;LATITUDE&#39;</span>, <span style=color:#e6db74>&#39;LONGITUDE&#39;</span>])
</span></span><span style=display:flex><span>           <span style=color:#f92672>.</span>groupby(level<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>first()
</span></span><span style=display:flex><span>           <span style=color:#f92672>.</span>dropna()
</span></span><span style=display:flex><span>           <span style=color:#f92672>.</span>sample(n<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>           <span style=color:#f92672>.</span>sort_index())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>coord<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>LATITUDE</th><th>LONGITUDE</th></tr><tr><th>AIRPORT</th><th></th><th></th></tr></thead><tbody><tr><th>8F3</th><td>33.623889</td><td>-101.240833</td></tr><tr><th>A03</th><td>58.457500</td><td>-154.023333</td></tr><tr><th>A09</th><td>60.482222</td><td>-146.582222</td></tr><tr><th>A18</th><td>63.541667</td><td>-150.993889</td></tr><tr><th>A24</th><td>59.331667</td><td>-135.896667</td></tr></tbody></table></div><p>For whatever reason, suppose we&rsquo;re interested in all the pairwise distances (I&rsquo;ve limited it to just a sample of 500 airports to make this manageable.
In the real world you <em>probably</em> don&rsquo;t need <em>all</em> the pairwise distances and would be better off with a <a href=http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html>tree</a>. Remember: think about what you actually need, and find the right algorithm for that).</p><p>MultiIndexes have an alternative <code>from_product</code> constructor for getting the <a href=https://en.wikipedia.org/wiki/Cartesian_product>Cartesian product</a> of the arrays you pass in.
We&rsquo;ll give it <code>coords.index</code> twice (to get its Cartesian product with itself).
That gives a MultiIndex of all the combination.
With some minor reshaping of <code>coords</code> we&rsquo;ll have a DataFrame with all the latitude/longitude pairs.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>idx <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>MultiIndex<span style=color:#f92672>.</span>from_product([coord<span style=color:#f92672>.</span>index, coord<span style=color:#f92672>.</span>index],
</span></span><span style=display:flex><span>                                 names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;origin&#39;</span>, <span style=color:#e6db74>&#39;dest&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pairs <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>concat([coord<span style=color:#f92672>.</span>add_suffix(<span style=color:#e6db74>&#39;_1&#39;</span>)<span style=color:#f92672>.</span>reindex(idx, level<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;origin&#39;</span>),
</span></span><span style=display:flex><span>                   coord<span style=color:#f92672>.</span>add_suffix(<span style=color:#e6db74>&#39;_2&#39;</span>)<span style=color:#f92672>.</span>reindex(idx, level<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dest&#39;</span>)],
</span></span><span style=display:flex><span>                  axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>pairs<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th></th><th>LATITUDE_1</th><th>LONGITUDE_1</th><th>LATITUDE_2</th><th>LONGITUDE_2</th></tr><tr><th>origin</th><th>dest</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th rowspan=5 valign=top>8F3</th><th>8F3</th><td>33.623889</td><td>-101.240833</td><td>33.623889</td><td>-101.240833</td></tr><tr><th>A03</th><td>33.623889</td><td>-101.240833</td><td>58.457500</td><td>-154.023333</td></tr><tr><th>A09</th><td>33.623889</td><td>-101.240833</td><td>60.482222</td><td>-146.582222</td></tr><tr><th>A18</th><td>33.623889</td><td>-101.240833</td><td>63.541667</td><td>-150.993889</td></tr><tr><th>A24</th><td>33.623889</td><td>-101.240833</td><td>59.331667</td><td>-135.896667</td></tr></tbody></table></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>idx <span style=color:#f92672>=</span> idx[idx<span style=color:#f92672>.</span>get_level_values(<span style=color:#ae81ff>0</span>) <span style=color:#f92672>&lt;=</span> idx<span style=color:#f92672>.</span>get_level_values(<span style=color:#ae81ff>1</span>)]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>len(idx)
</span></span></code></pre></div><pre><code>125250
</code></pre><p>We&rsquo;ll break that down a bit, but don&rsquo;t lose sight of the real target: our great-circle distance calculation.</p><p>The <code>add_suffix</code> (and <code>add_prefix</code>) method is handy for quickly renaming the columns.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>coord<span style=color:#f92672>.</span>add_suffix(<span style=color:#e6db74>&#39;_1&#39;</span>)<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>LATITUDE_1</th><th>LONGITUDE_1</th></tr><tr><th>AIRPORT</th><th></th><th></th></tr></thead><tbody><tr><th>8F3</th><td>33.623889</td><td>-101.240833</td></tr><tr><th>A03</th><td>58.457500</td><td>-154.023333</td></tr><tr><th>A09</th><td>60.482222</td><td>-146.582222</td></tr><tr><th>A18</th><td>63.541667</td><td>-150.993889</td></tr><tr><th>A24</th><td>59.331667</td><td>-135.896667</td></tr></tbody></table></div><p>Alternatively you could use the more general <code>.rename</code> like <code>coord.rename(columns=lambda x: x + '_1')</code>.</p><p>Next, we have the <code>reindex</code>.
Like I mentioned in the prior chapter, indexes are crucial to pandas.
<code>.reindex</code> is all about aligning a Series or DataFrame to a given index.
In this case we use <code>.reindex</code> to align our original DataFrame to the new
MultiIndex of combinations.
By default, the output will have the original value if that index label was already present, and <code>NaN</code> otherwise.
If we just called <code>coord.reindex(idx)</code>, with no additional arguments, we&rsquo;d get a DataFrame of all <code>NaN</code>s.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>coord<span style=color:#f92672>.</span>reindex(idx)<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th></th><th>LATITUDE</th><th>LONGITUDE</th></tr><tr><th>origin</th><th>dest</th><th></th><th></th></tr></thead><tbody><tr><th rowspan=5 valign=top>8F3</th><th>8F3</th><td>NaN</td><td>NaN</td></tr><tr><th>A03</th><td>NaN</td><td>NaN</td></tr><tr><th>A09</th><td>NaN</td><td>NaN</td></tr><tr><th>A18</th><td>NaN</td><td>NaN</td></tr><tr><th>A24</th><td>NaN</td><td>NaN</td></tr></tbody></table></div><p>That&rsquo;s because there weren&rsquo;t any values of <code>idx</code> that were in <code>coord.index</code>,
which makes sense since <code>coord.index</code> is just a regular one-level Index, while <code>idx</code> is a MultiIndex.
We use the <code>level</code> keyword to handle the transition from the original single-level Index, to the two-leveled <code>idx</code>.</p><blockquote><p><code>level</code> : int or name</p></blockquote><p>Broadcast across a level, matching Index values on the
passed MultiIndex level</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>coord<span style=color:#f92672>.</span>reindex(idx, level<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dest&#39;</span>)<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th></th><th>LATITUDE</th><th>LONGITUDE</th></tr><tr><th>origin</th><th>dest</th><th></th><th></th></tr></thead><tbody><tr><th rowspan=5 valign=top>8F3</th><th>8F3</th><td>33.623889</td><td>-101.240833</td></tr><tr><th>A03</th><td>58.457500</td><td>-154.023333</td></tr><tr><th>A09</th><td>60.482222</td><td>-146.582222</td></tr><tr><th>A18</th><td>63.541667</td><td>-150.993889</td></tr><tr><th>A24</th><td>59.331667</td><td>-135.896667</td></tr></tbody></table></div><p>If you ever need to do an operation that mixes regular single-level indexes with Multilevel Indexes, look for a level keyword argument.
For example, all the arithmatic methods (<code>.mul</code>, <code>.add</code>, etc.) have them.</p><p>This is a bit wasteful since the distance from airport <code>A</code> to <code>B</code> is the same as <code>B</code> to <code>A</code>.
We could easily fix this with a <code>idx = idx[idx.get_level_values(0) &lt;= idx.get_level_values(1)]</code>, but we&rsquo;ll ignore that for now.</p><p>Quick tangent, I got some&mldr; let&rsquo;s say skepticism, on my last piece about the value of indexes.
Here&rsquo;s an alternative version for the skeptics</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> itertools <span style=color:#f92672>import</span> product, chain
</span></span><span style=display:flex><span>coord2 <span style=color:#f92672>=</span> coord<span style=color:#f92672>.</span>reset_index()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> product(coord2<span style=color:#f92672>.</span>add_suffix(<span style=color:#e6db74>&#39;_1&#39;</span>)<span style=color:#f92672>.</span>itertuples(index<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),
</span></span><span style=display:flex><span>            coord2<span style=color:#f92672>.</span>add_suffix(<span style=color:#e6db74>&#39;_2&#39;</span>)<span style=color:#f92672>.</span>itertuples(index<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>))
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> [list(chain<span style=color:#f92672>.</span>from_iterable(z)) <span style=color:#66d9ef>for</span> z <span style=color:#f92672>in</span> x]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df2 <span style=color:#f92672>=</span> (pd<span style=color:#f92672>.</span>DataFrame(y, columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;origin&#39;</span>, <span style=color:#e6db74>&#39;LATITUDE_1&#39;</span>, <span style=color:#e6db74>&#39;LONGITUDE_1&#39;</span>,
</span></span><span style=display:flex><span>                                <span style=color:#e6db74>&#39;dest&#39;</span>, <span style=color:#e6db74>&#39;LATITUDE_1&#39;</span>, <span style=color:#e6db74>&#39;LONGITUDE_2&#39;</span>])
</span></span><span style=display:flex><span>       <span style=color:#f92672>.</span>set_index([<span style=color:#e6db74>&#39;origin&#39;</span>, <span style=color:#e6db74>&#39;dest&#39;</span>]))
</span></span><span style=display:flex><span>df2<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th></th><th>LATITUDE_1</th><th>LONGITUDE_1</th><th>LATITUDE_1</th><th>LONGITUDE_2</th></tr><tr><th>origin</th><th>dest</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th rowspan=5 valign=top>8F3</th><th>8F3</th><td>33.623889</td><td>-101.240833</td><td>33.623889</td><td>-101.240833</td></tr><tr><th>A03</th><td>33.623889</td><td>-101.240833</td><td>58.457500</td><td>-154.023333</td></tr><tr><th>A09</th><td>33.623889</td><td>-101.240833</td><td>60.482222</td><td>-146.582222</td></tr><tr><th>A18</th><td>33.623889</td><td>-101.240833</td><td>63.541667</td><td>-150.993889</td></tr><tr><th>A24</th><td>33.623889</td><td>-101.240833</td><td>59.331667</td><td>-135.896667</td></tr></tbody></table></div><p>It&rsquo;s also readable (it&rsquo;s Python after all), though a bit slower.
To me the <code>.reindex</code> method seems more natural.
My thought process was, &ldquo;I need all the combinations of origin & destination (<code>MultiIndex.from_product</code>).
Now I need to align this original DataFrame to this new MultiIndex (<code>coords.reindex</code>).&rdquo;</p><p>With that diversion out of the way, let&rsquo;s turn back to our great-circle distance calculation.
Our first implementation is pure python.
The algorithm itself isn&rsquo;t too important, all that matters is that we&rsquo;re doing math operations on scalars.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>gcd_py</span>(lat1, lng1, lat2, lng2):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Calculate great circle distance between two points.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    http://www.johndcook.com/blog/python_longitude_latitude/
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ----------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    lat1, lng1, lat2, lng2: float
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    -------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    distance:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># python2 users will have to use ascii identifiers (or upgrade)</span>
</span></span><span style=display:flex><span>    degrees_to_radians <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>pi <span style=color:#f92672>/</span> <span style=color:#ae81ff>180.0</span>
</span></span><span style=display:flex><span>    ϕ1 <span style=color:#f92672>=</span> (<span style=color:#ae81ff>90</span> <span style=color:#f92672>-</span> lat1) <span style=color:#f92672>*</span> degrees_to_radians
</span></span><span style=display:flex><span>    ϕ2 <span style=color:#f92672>=</span> (<span style=color:#ae81ff>90</span> <span style=color:#f92672>-</span> lat2) <span style=color:#f92672>*</span> degrees_to_radians
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    θ1 <span style=color:#f92672>=</span> lng1 <span style=color:#f92672>*</span> degrees_to_radians
</span></span><span style=display:flex><span>    θ2 <span style=color:#f92672>=</span> lng2 <span style=color:#f92672>*</span> degrees_to_radians
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    cos <span style=color:#f92672>=</span> (math<span style=color:#f92672>.</span>sin(ϕ1) <span style=color:#f92672>*</span> math<span style=color:#f92672>.</span>sin(ϕ2) <span style=color:#f92672>*</span> math<span style=color:#f92672>.</span>cos(θ1 <span style=color:#f92672>-</span> θ2) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>           math<span style=color:#f92672>.</span>cos(ϕ1) <span style=color:#f92672>*</span> math<span style=color:#f92672>.</span>cos(ϕ2))
</span></span><span style=display:flex><span>    <span style=color:#75715e># round to avoid precision issues on identical points causing ValueErrors</span>
</span></span><span style=display:flex><span>    cos <span style=color:#f92672>=</span> round(cos, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>    arc <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>acos(cos)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> arc <span style=color:#f92672>*</span> <span style=color:#ae81ff>6373</span>  <span style=color:#75715e># radius of earth, in kilometers</span>
</span></span></code></pre></div><p>The second implementation uses NumPy.
Aside from numpy having a builtin <code>deg2rad</code> convenience function (which is probably a bit slower than multiplying by a constant $\frac{\pi}{180}$), basically all we&rsquo;ve done is swap the <code>math</code> prefix for <code>np</code>.
Thanks to NumPy&rsquo;s broadcasting, we can write code that works on scalars or arrays of conformable shape.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>gcd_vec</span>(lat1, lng1, lat2, lng2):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Calculate great circle distance.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    http://www.johndcook.com/blog/python_longitude_latitude/
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ----------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    lat1, lng1, lat2, lng2: float or array of float
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    -------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    distance:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># python2 users will have to use ascii identifiers</span>
</span></span><span style=display:flex><span>    ϕ1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>deg2rad(<span style=color:#ae81ff>90</span> <span style=color:#f92672>-</span> lat1)
</span></span><span style=display:flex><span>    ϕ2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>deg2rad(<span style=color:#ae81ff>90</span> <span style=color:#f92672>-</span> lat2)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    θ1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>deg2rad(lng1)
</span></span><span style=display:flex><span>    θ2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>deg2rad(lng2)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    cos <span style=color:#f92672>=</span> (np<span style=color:#f92672>.</span>sin(ϕ1) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sin(ϕ2) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>cos(θ1 <span style=color:#f92672>-</span> θ2) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>           np<span style=color:#f92672>.</span>cos(ϕ1) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>cos(ϕ2))
</span></span><span style=display:flex><span>    arc <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arccos(cos)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> arc <span style=color:#f92672>*</span> <span style=color:#ae81ff>6373</span>
</span></span></code></pre></div><p>To use the python version on our DataFrame, we can either iterate&mldr;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span>pd<span style=color:#f92672>.</span>Series([gcd_py(<span style=color:#f92672>*</span>x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> pairs<span style=color:#f92672>.</span>itertuples(index<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)],
</span></span><span style=display:flex><span>          index<span style=color:#f92672>=</span>pairs<span style=color:#f92672>.</span>index)
</span></span></code></pre></div><pre><code>CPU times: user 833 ms, sys: 12.7 ms, total: 846 ms
Wall time: 847 ms





origin  dest
8F3     8F3         0.000000
        A03      4744.967448
        A09      4407.533212
        A18      4744.593127
        A24      3820.092688
                    ...     
ZZU     YUY     12643.665960
        YYL     13687.592278
        ZBR      4999.647307
        ZXO     14925.531303
        ZZU         0.000000
Length: 250000, dtype: float64
</code></pre><p>Or use <code>DataFrame.apply</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span>r <span style=color:#f92672>=</span> pairs<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> x: gcd_py(x[<span style=color:#e6db74>&#39;LATITUDE_1&#39;</span>], x[<span style=color:#e6db74>&#39;LONGITUDE_1&#39;</span>],
</span></span><span style=display:flex><span>                                 x[<span style=color:#e6db74>&#39;LATITUDE_2&#39;</span>], x[<span style=color:#e6db74>&#39;LONGITUDE_2&#39;</span>]), axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>);
</span></span></code></pre></div><pre><code>CPU times: user 14.4 s, sys: 61.2 ms, total: 14.4 s
Wall time: 14.4 s
</code></pre><p>But as you can see, you don&rsquo;t want to use apply, especially with <code>axis=1</code> (calling the function on each row). It&rsquo;s doing a lot more work handling dtypes in the background, and trying to infer the correct output shape that are pure overhead in this case. On top of that, it has to essentially use a for loop internally.</p><p>You <em>rarely</em> want to use <code>DataFrame.apply</code> and almost never should use it with <code>axis=1</code>. Better to write functions that take arrays, and pass those in directly. Like we did with the vectorized version</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span>r <span style=color:#f92672>=</span> gcd_vec(pairs[<span style=color:#e6db74>&#39;LATITUDE_1&#39;</span>], pairs[<span style=color:#e6db74>&#39;LONGITUDE_1&#39;</span>],
</span></span><span style=display:flex><span>            pairs[<span style=color:#e6db74>&#39;LATITUDE_2&#39;</span>], pairs[<span style=color:#e6db74>&#39;LONGITUDE_2&#39;</span>])
</span></span></code></pre></div><pre><code>CPU times: user 31.1 ms, sys: 26.4 ms, total: 57.5 ms
Wall time: 37.2 ms


/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in arccos
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>r<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><pre><code>origin  dest
8F3     8F3        0.000000
        A03     4744.967484
        A09     4407.533240
        A18     4744.593111
        A24     3820.092639
dtype: float64
</code></pre><p>I try not to use the word &ldquo;easy&rdquo; when teaching, but that optimization was easy right?
Why then, do I come across uses of <code>apply</code>, in my code and others&rsquo;, even when the vectorized version is available?
The difficulty lies in knowing about broadcasting, and seeing where to apply it.</p><p>For example, the README for <a href=https://github.com/CamDavidsonPilon/lifetimes>lifetimes</a> (by Cam Davidson Pilon, also author of <a href=https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers>Bayesian Methods for Hackers</a>, <a href=https://github.com/CamDavidsonPilon/lifelines>lifelines</a>, and <a href=https://dataorigami.net>Data Origami</a>) used to have an example of passing <a href=https://github.com/CamDavidsonPilon/lifetimes/blob/5b4f7de0720413b6951ac0a4b0082bd50255a231/lifetimes/estimation.py#L249>this method</a> into a <code>DataFrame.apply</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>data<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> r: bgf<span style=color:#f92672>.</span>conditional_expected_number_of_purchases_up_to_time(
</span></span><span style=display:flex><span>    t, r[<span style=color:#e6db74>&#39;frequency&#39;</span>], r[<span style=color:#e6db74>&#39;recency&#39;</span>], r[<span style=color:#e6db74>&#39;T&#39;</span>]), axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>If you look at the function <a href=https://github.com/CamDavidsonPilon/lifetimes/blob/5b4f7de0720413b6951ac0a4b0082bd50255a231/lifetimes/estimation.py#L249>I linked to</a>, it&rsquo;s doing a fairly complicated computation involving a negative log likelihood and the Gamma function from <code>scipy.special</code>.
But crucially, it was already vectorized.
We were able to change the example to just pass the arrays (Series in this case) into the function, rather than applying the function to each row.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>bgf<span style=color:#f92672>.</span>conditional_expected_number_of_purchases_up_to_time(
</span></span><span style=display:flex><span>    t, data[<span style=color:#e6db74>&#39;frequency&#39;</span>], data[<span style=color:#e6db74>&#39;recency&#39;</span>], data[<span style=color:#e6db74>&#39;T&#39;</span>]
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>This got us another 30x speedup on the example dataset.
I bring this up because it&rsquo;s very natural to have to translate an equation to code and think, &ldquo;Ok now I need to apply this function to each row&rdquo;, so you reach for <code>DataFrame.apply</code>.
See if you can just pass in the NumPy array or Series itself instead.</p><p>Not all operations this easy to vectorize.
Some operations are iterative by nature, and rely on the results of surrounding computations to proceed. In cases like this you can hope that one of the scientific python libraries has implemented it efficiently for you, or write your own solution using Numba / C / Cython / Fortran.</p><p>Other examples take a bit more thought or knowledge to vectorize.
Let&rsquo;s look at <a href=http://nbviewer.jupyter.org/github/jreback/pydata2015-london/blob/master/notebooks/idioms.ipynb>this</a>
example, taken from Jeff Reback&rsquo;s PyData London talk, that groupwise normalizes a dataset by subtracting the mean and dividing by the standard deviation for each group.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_frame</span>(n, n_groups):
</span></span><span style=display:flex><span>    <span style=color:#75715e># just setup code, not benchmarking this</span>
</span></span><span style=display:flex><span>    stamps <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>date_range(<span style=color:#e6db74>&#39;20010101&#39;</span>, periods<span style=color:#f92672>=</span>n, freq<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ms&#39;</span>)
</span></span><span style=display:flex><span>    random<span style=color:#f92672>.</span>shuffle(stamps<span style=color:#f92672>.</span>values)    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;name&#39;</span>: np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>,n_groups,size<span style=color:#f92672>=</span>n),
</span></span><span style=display:flex><span>                         <span style=color:#e6db74>&#39;stamp&#39;</span>: stamps,
</span></span><span style=display:flex><span>                         <span style=color:#e6db74>&#39;value&#39;</span>: np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>,n,size<span style=color:#f92672>=</span>n),
</span></span><span style=display:flex><span>                         <span style=color:#e6db74>&#39;value2&#39;</span>: np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(n)})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> create_frame(<span style=color:#ae81ff>1000000</span>,<span style=color:#ae81ff>10000</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f_apply</span>(df):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Typical transform</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> df<span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#39;name&#39;</span>)<span style=color:#f92672>.</span>value2<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> x: (x<span style=color:#f92672>-</span>x<span style=color:#f92672>.</span>mean())<span style=color:#f92672>/</span>x<span style=color:#f92672>.</span>std())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f_unwrap</span>(df):
</span></span><span style=display:flex><span>    <span style=color:#75715e># &#34;unwrapped&#34;</span>
</span></span><span style=display:flex><span>    g <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#39;name&#39;</span>)<span style=color:#f92672>.</span>value2
</span></span><span style=display:flex><span>    v <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>value2
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (v<span style=color:#f92672>-</span>g<span style=color:#f92672>.</span>transform(np<span style=color:#f92672>.</span>mean))<span style=color:#f92672>/</span>g<span style=color:#f92672>.</span>transform(np<span style=color:#f92672>.</span>std)
</span></span></code></pre></div><p>Timing it we see that the &ldquo;unwrapped&rdquo; version, get&rsquo;s quite a bit better performance.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>timeit f_apply(df)
</span></span></code></pre></div><pre><code>4.28 s ± 161 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>timeit f_unwrap(df)
</span></span></code></pre></div><pre><code>53.3 ms ± 1.97 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre><p>Pandas GroupBy objects intercept calls for common functions like mean, sum, etc. and substitutes them with optimized Cython versions.
So the unwrapped <code>.transform(np.mean)</code> and <code>.transform(np.std)</code> are fast, while the <code>x.mean</code> and <code>x.std</code> in the <code>.apply(lambda x: x - x.mean()/x.std())</code> aren&rsquo;t.</p><p><code>Groupby.apply</code> is always going to be around, beacuse it offers maximum flexibility. If you need to <a href=http://stackoverflow.com/q/35924126/1889400>fit a model on each group and create additional columns in the process</a>, it can handle that. It just might not be the fastest (which may be OK sometimes).</p><p>This last example is admittedly niche.
I&rsquo;d like to think that there aren&rsquo;t too many places in pandas where the natural thing to do <code>.transform((x - x.mean()) / x.std())</code> is slower than the less obvious alternative.
Ideally the user wouldn&rsquo;t have to know about GroupBy having special fast implementations of common methods.
But that&rsquo;s where we are now.</p><h2 id=categoricals>Categoricals<a hidden class=anchor aria-hidden=true href=#categoricals>#</a></h2><p>Thanks to some great work by <a href=https://twitter.com/janschulz>Jan Schulz</a>, <a href=https://twitter.com/janschulz>Jeff Reback</a>, and others, pandas 0.15 gained a new <a href=http://pandas.pydata.org/pandas-docs/version/0.18.0/categorical.html>Categorical</a> data type. Categoricals are nice for many reasons beyond just efficiency, but we&rsquo;ll focus on that here.</p><p>Categoricals are an efficient way of representing data (typically strings) that have a low <em>cardinality</em>, i.e. relatively few distinct values relative to the size of the array. Internally, a Categorical stores the categories once, and an array of <code>codes</code>, which are just integers that indicate which category belongs there. Since it&rsquo;s cheaper to store a <code>code</code> than a <code>category</code>, we save on memory (shown next).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> string
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>s <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>Series(np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice(list(string<span style=color:#f92672>.</span>ascii_letters), <span style=color:#ae81ff>100000</span>))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{:0.2f}</span><span style=color:#e6db74> KB&#39;</span><span style=color:#f92672>.</span>format(s<span style=color:#f92672>.</span>memory_usage(index<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>1000</span>))
</span></span></code></pre></div><pre><code>800.00 KB
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>c <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#39;category&#39;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{:0.2f}</span><span style=color:#e6db74> KB&#39;</span><span style=color:#f92672>.</span>format(c<span style=color:#f92672>.</span>memory_usage(index<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>1000</span>))
</span></span></code></pre></div><pre><code>102.98 KB
</code></pre><p>Beyond saving memory, having codes and a fixed set of categories offers up a bunch of algorithmic optimizations that pandas and others can take advantage of.</p><p><a href=https://twitter.com/mrocklin>Matthew Rocklin</a> has a very nice <a href=http://matthewrocklin.com/blog/work/2015/06/18/Categoricals>post</a> on using categoricals, and optimizing code in general.</p><h2 id=going-further>Going Further<a hidden class=anchor aria-hidden=true href=#going-further>#</a></h2><p>The pandas documentation has a section on <a href=http://pandas.pydata.org/pandas-docs/version/0.18.0/enhancingperf.html>enhancing performance</a>, focusing on using Cython or <code>numba</code> to speed up a computation. I&rsquo;ve focused more on the lower-hanging fruit of picking the right algorithm, vectorizing your code, and using pandas or numpy more effetively. There are further optimizations availble if these aren&rsquo;t enough.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>This post was more about how to make effective use of numpy and pandas, than writing your own highly-optimized code.
In my day-to-day work of data analysis it&rsquo;s not worth the time to write and compile a cython extension.
I&rsquo;d rather rely on pandas to be fast at what matters (label lookup on large arrays, factorizations for groupbys and merges, numerics).
If you want to learn more about what pandas does to make things fast, checkout Jeff Tratner&rsquo; talk from PyData Seattle <a href=http://www.jeffreytratner.com/slides/pandas-under-the-hood-pydata-seattle-2015.pdf>talk</a> on pandas&rsquo; internals.</p><p>Next time we&rsquo;ll look at a differnt kind of optimization: using the Tidy Data principles to facilitate efficient data analysis.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tomaugspurger.net/tags/pandas/>pandas</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://tomaugspurger.net>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>