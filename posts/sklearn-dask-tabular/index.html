<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Tabular Data in Scikit-Learn and Dask-ML | Tom's Blog</title><meta name=keywords content><meta name=description content="Scikit-Learn 0.20.0 will contain some nice new features for working with tabular data. This blogpost will introduce those improvements with a small demo. We&rsquo;ll then see how Dask-ML was able to piggyback on the work done by scikit-learn to offer a version that works well with Dask Arrays and DataFrames.
import dask import dask.array as da import dask.dataframe as dd import numpy as np import pandas as pd import seaborn as sns import fastparquet from distributed import Client from distributed."><meta name=author content><link rel=canonical href=https://tomaugspurger.github.io/posts/sklearn-dask-tabular/><link crossorigin=anonymous href=/assets/css/stylesheet.67b4a5a78be69ca812d40e5543fbd83efd0b0cc5025c2505fd7758fd5d32ec2e.css integrity="sha256-Z7Slp4vmnKgS1A5VQ/vYPv0LDMUCXCUF/XdY/V0y7C4=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tomaugspurger.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Tabular Data in Scikit-Learn and Dask-ML"><meta property="og:description" content="Scikit-Learn 0.20.0 will contain some nice new features for working with tabular data. This blogpost will introduce those improvements with a small demo. We&rsquo;ll then see how Dask-ML was able to piggyback on the work done by scikit-learn to offer a version that works well with Dask Arrays and DataFrames.
import dask import dask.array as da import dask.dataframe as dd import numpy as np import pandas as pd import seaborn as sns import fastparquet from distributed import Client from distributed."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.github.io/posts/sklearn-dask-tabular/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-09-17T00:00:00+00:00"><meta property="article:modified_time" content="2018-09-17T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Tabular Data in Scikit-Learn and Dask-ML"><meta name=twitter:description content="Scikit-Learn 0.20.0 will contain some nice new features for working with tabular data. This blogpost will introduce those improvements with a small demo. We&rsquo;ll then see how Dask-ML was able to piggyback on the work done by scikit-learn to offer a version that works well with Dask Arrays and DataFrames.
import dask import dask.array as da import dask.dataframe as dd import numpy as np import pandas as pd import seaborn as sns import fastparquet from distributed import Client from distributed."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Tabular Data in Scikit-Learn and Dask-ML","item":"https://tomaugspurger.github.io/posts/sklearn-dask-tabular/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Tabular Data in Scikit-Learn and Dask-ML","name":"Tabular Data in Scikit-Learn and Dask-ML","description":"Scikit-Learn 0.20.0 will contain some nice new features for working with tabular data. This blogpost will introduce those improvements with a small demo. We\u0026rsquo;ll then see how Dask-ML was able to piggyback on the work done by scikit-learn to offer a version that works well with Dask Arrays and DataFrames.\nimport dask import dask.array as da import dask.dataframe as dd import numpy as np import pandas as pd import seaborn as sns import fastparquet from distributed import Client from distributed.","keywords":[],"articleBody":"Scikit-Learn 0.20.0 will contain some nice new features for working with tabular data. This blogpost will introduce those improvements with a small demo. We’ll then see how Dask-ML was able to piggyback on the work done by scikit-learn to offer a version that works well with Dask Arrays and DataFrames.\nimport dask import dask.array as da import dask.dataframe as dd import numpy as np import pandas as pd import seaborn as sns import fastparquet from distributed import Client from distributed.utils import format_bytes Background For the most part, Scikit-Learn uses NumPy ndarrays or SciPy sparse matricies for its in-memory data structures. This is great for many reasons, but one major drawback is that you can’t store heterogenous (AKA tabular) data in these containers. These are datasets where different columns of the table have different data types (some ints, some floats, some strings, etc.).\nPandas was built to work with tabular data. Scikit-Learn was built to work with NumPy ndarrays and SciPy sparse matricies. So there’s some friction when you use the two together. Perhaps someday things will be perfectly smooth, but it’s a challenging problem that will require work from several communities to fix. In this PyData Chicago talk, I discuss the differences between the two data models of scikit-learn and pandas, and some ways of working through it. The second half of the talk is mostly irrelevant now that ColumnTransformer is in scikit-learn.\nColumnTransformer in Scikit-Learn At SciPy 2018, Joris Van den Bossche (a scikit-learn and pandas core developer) gives an update on some recent improvements to scikit-learn to make using pandas and scikit-learn together better.\nThe biggest addition is sklearn.compose.ColumnTransformer, a transformer for working with tabular data. The basic idea is to specify pairs of (column_selection, transformer). The transformer will be applied just to the selected columns, and the remaining columns can be passed through or dropped. Column selections can be integer positions (for arrays), names (for DataFrames) or a callable.\nHere’s a small example on the “tips” dataset.\ndf = sns.load_dataset('tips') df.head() total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 Our target is whether the tip was larger than 15%.\nX = df.drop(\"tip\", axis='columns') y = df.tip / df.total_bill \u003e 0.15 We’ll make a small pipeline that one-hot encodes the categorical columns (sex, smoker, day, time) before fitting a random forest. The numeric columns (total_bill, size) will be passed through as-is.\nimport sklearn.compose import sklearn.ensemble import sklearn.pipeline import sklearn.preprocessing We use make_column_transformer to create the ColumnTransformer.\ncategorical_columns = ['sex', 'smoker', 'day', 'time'] categorical_encoder = sklearn.preprocessing.OneHotEncoder(sparse=False) transformers = sklearn.compose.make_column_transformer( (categorical_columns, categorical_encoder), remainder='passthrough' ) This is just a regular scikit-learn estimator, which can be placed in a pipeline.\npipe = sklearn.pipeline.make_pipeline( transformers, sklearn.ensemble.RandomForestClassifier(n_estimators=100) ) pipe.fit(X, y) pipe.score(X, y) 1.0 We’ve likely overfitted, but that’s not really the point of this article. We’re more interested in the pre-processing side of things.\nColumnTransformer in Dask-ML ColumnTransfomrer was added to Dask-ML in https://github.com/dask/dask-ml/pull/315. Ideally, we wouldn’t need that PR at all. We would prefer for dask’s collections (and pandas dataframes) to just be handled gracefully by scikit-learn. The main blocking issue is that the Python community doesn’t currently have a way to write “concatenate this list of array-like objects together” in a generic way. That’s being worked on in NEP-18.\nSo for now, if you want to use ColumnTransformer with dask objects, you’ll have to use dask_ml.compose.ColumnTransformer, otherwise your large Dask Array or DataFrame would be converted to an in-memory NumPy array.\nAs a footnote to this section, the initial PR in Dask-ML was much longer. I only needed to override one thing (the function _hstack used to glue the results back together). But that was being called from several places, and so I had to override all those places as well. I was able to work with the scikit-learn developers to make _hstack a staticmethod on ColumnTranformer, so any library wishing to extend ColumnTransformer can do so more easily now. The Dask project values working with the existing community.\nChallenges with Scaling Many strategies for dealing with large datasets rely on processing the data in chunks. That’s the basic idea behind Dask DataFrame: a Dask DataFrame consists of many pandas DataFrames. When you write ddf.column.value_counts(), Dask builds a task graph with many pandas.value_counts, and a final aggregation step so that you end up with the same end result.\nBut chunking can cause issues when there are variations in your dataset and the operation you’re applying depends on the data. For example, consider scikit-learn’s OneHotEncoder. By default, it looks at the data and creates a column for each unique value.\nenc = sklearn.preprocessing.OneHotEncoder(sparse=False) enc.fit_transform([['a'], ['a'], ['b'], ['c']]) array([[1., 0., 0.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) But let’s suppose we wanted to process that in chunks of two, first [['a'], ['a']], then [['b'], ['c']].\nenc.fit_transform([['a'], ['a']]) array([[1.], [1.]]) enc.fit_transform([['b'], ['c']]) array([[1., 0.], [0., 1.]]) We have a problem! Two in fact:\nThe shapes don’t match. The first batch only saw “a”, so the output shape is (2, 1). We can’t concatenate these results vertically The meaning of the first column of the output has changed. In the first batch, the first column meant “a” was present. In the second batch, it meant “b” was present. If we happened to know the set of possible values ahead of time, we could pass those to CategoricalEncoder. But storing that set of possible values separate from the data is fragile. It’d be better to store the possible values in the data type itself.\nThat’s exactly what pandas Categorical does. We can confidently know the number of columns in the categorical-encoded data by just looking at the type. Because this is so important in a distributed dataset context, dask_ml.preprocessing.OneHotEncoder differs from scikit-learn when passed categorical data: we use pandas’ categorical information.\nA larger Example We’ll work with the Criteo dataset. This has a mixture of numeric and categorical features. It’s also a large dataset, which presents some challenges for many pre-processing methods.\nThe full dataset is from http://labs.criteo.com/2013/12/download-terabyte-click-logs/. We’ll work with a sample.\nclient = Client() ordinal_columns = [ 'category_0', 'category_1', 'category_2', 'category_3', 'category_4', 'category_6', 'category_7', 'category_9', 'category_10', 'category_11', 'category_13', 'category_14', 'category_17', 'category_19', 'category_20', 'category_21', 'category_22', 'category_23', ] onehot_columns = [ 'category_5', 'category_8', 'category_12', 'category_15', 'category_16', 'category_18', 'category_24', 'category_25', ] numeric_columns = [f'numeric_{i}' for i in range(13)] columns = ['click'] + numeric_columns + onehot_columns + ordinal_columns The raw data is a single large CSV. That’s been split with this script and I took a 10% sample with this script, which was written to a directory of parquet files. That’s what we’ll work with.\nsample = dd.read_parquet(\"data/sample-10.parquet/\") # Convert unknown categorical to known. # See note later on. pf = fastparquet.ParquetFile(\"data/sample-10.parquet/part.0.parquet\") cats = pf.grab_cats(onehot_columns) sample = sample.assign(**{ col: sample[col].cat.set_categories(cats[col]) for col in onehot_columns }) Our goal is to predict ‘click’ using the other columns.\ny = sample['click'] X = sample.drop(\"click\", axis='columns') Now, let’s lay out our pre-processing pipeline. We have three types of columns\nNumeric columns Low-cardinality categorical columns High-cardinality categorical columns Each of those will be processed differently.\nNumeric columns will have missing values filled with the column average and standard scaled Low-cardinality categorical columns will be one-hot encoded High-cardinality categorical columns will be deterministically hashed and standard scaled You’ll probably want to quibble with some of these choices, but right now, I’m just interested in the ability to do these kinds of transformations at all.\nWe need to define a couple custom estimators, one for hashing the values of a dask dataframe, and one for converting a dask dataframe to a dask array.\nimport sklearn.base def hash_block(x: pd.DataFrame) -\u003e pd.DataFrame: \"\"\"Hash the values in a DataFrame.\"\"\" hashed = [ pd.Series(pd.util.hash_array(data.values), index=x.index, name=col) for col, data in x.iteritems() ] return pd.concat(hashed, axis='columns') class HashingEncoder(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin): def fit(self, X, y=None): return self def transform(self, X, y=None): if isinstance(X, pd.DataFrame): return hash_block(X) elif isinstance(X, dd.DataFrame): return X.map_partitions(hash_block) else: raise ValueError(\"Unexpected type '{}' for 'X'\".format(type(X))) class ArrayConverter(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin): \"\"\"Convert a Dask DataFrame to a Dask Array with known lengths\"\"\" def __init__(self, lengths=None): self.lengths = lengths def fit(self, X, y=None): return self def transform(self, X, y=None): return X.to_dask_array(lengths=self.lengths) For the final stage, Dask-ML needs to have a Dask Array with known chunk lengths. So let’s compute those ahead of time, and get a bit of info about how large the dataset is while we’re at it.\nlengths = sample['click'].map_partitions(len) nbytes = sample.memory_usage(deep=True).sum() lengths, nbytes = dask.compute(lengths, nbytes) lengths = tuple(lengths) format_bytes(nbytes) '19.20 GB' We we’ll be working with about 20GB of data on a laptop with 16GB of RAM. We’ll clearly be relying on Dask to do the operations in parallel, while keeping things in a small memory footprint.\nfrom dask_ml.compose import make_column_transformer from dask_ml.preprocessing import StandardScaler, OneHotEncoder from dask_ml.wrappers import Incremental from dask_ml.impute import SimpleImputer from sklearn.pipeline import make_pipeline from sklearn.preprocessing import FunctionTransformer from sklearn.linear_model import SGDClassifier Now for the pipeline.\nonehot_encoder = OneHotEncoder(sparse=False) hashing_encoder = HashingEncoder() nan_imputer = SimpleImputer() to_numeric = make_column_transformer( (onehot_columns, onehot_encoder), (ordinal_columns, hashing_encoder), remainder='passthrough', ) fill_na = make_column_transformer( (numeric_columns, nan_imputer), remainder='passthrough' ) scaler = make_column_transformer( (list(numeric_columns) + list(ordinal_columns), StandardScaler()), remainder='passthrough' ) clf = Incremental( SGDClassifier(loss='log', random_state=0, max_iter=1000) ) pipe = make_pipeline(to_numeric, fill_na, scaler, ArrayConverter(lengths=lengths), clf) pipe Pipeline(memory=None, steps=[('columntransformer-1', ColumnTransformer(n_jobs=1, preserve_dataframe=True, remainder='passthrough', sparse_threshold=0.3, transformer_weights=None, transformers=[('onehotencoder', OneHotEncoder(categorical_features=None, categories='auto', dtype=","wordCount":"1934","inLanguage":"en","datePublished":"2018-09-17T00:00:00Z","dateModified":"2018-09-17T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.github.io/posts/sklearn-dask-tabular/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.github.io accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.github.io/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.github.io/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Tabular Data in Scikit-Learn and Dask-ML</h1><div class=post-meta><span title='2018-09-17 00:00:00 +0000 UTC'>September 17, 2018</span></div></header><div class=post-content><p>Scikit-Learn 0.20.0 will contain some nice new features for working with tabular data.
This blogpost will introduce those improvements with a small demo.
We&rsquo;ll then see how Dask-ML was able to piggyback on the work done by scikit-learn to offer a version that works well with Dask Arrays and DataFrames.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> dask
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> dask.array <span style=color:#66d9ef>as</span> da
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> dask.dataframe <span style=color:#66d9ef>as</span> dd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> fastparquet
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> distributed <span style=color:#f92672>import</span> Client
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> distributed.utils <span style=color:#f92672>import</span> format_bytes
</span></span></code></pre></div><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>For the most part, Scikit-Learn uses NumPy ndarrays or SciPy sparse matricies for its in-memory data structures.
This is great for many reasons, but one major drawback is that you can&rsquo;t store <em>heterogenous</em> (AKA <em>tabular</em>) data in these containers. These are datasets where different columns of the table have different data types (some ints, some floats, some strings, etc.).</p><p>Pandas was built to work with tabular data.
Scikit-Learn was built to work with NumPy ndarrays and SciPy sparse matricies.
So there&rsquo;s some friction when you use the two together.
Perhaps someday things will be perfectly smooth, but it&rsquo;s a challenging problem that will require work from several communities to fix.
In <a href="https://www.youtube.com/watch?v=KLPtEBokqQ0">this PyData Chicago talk</a>, I discuss the differences between the two data models of scikit-learn and pandas, and some ways of working through it. The second half of the talk is mostly irrelevant now that <code>ColumnTransformer</code> is in scikit-learn.</p><h2 id=columntransformer-in-scikit-learn><code>ColumnTransformer</code> in Scikit-Learn<a hidden class=anchor aria-hidden=true href=#columntransformer-in-scikit-learn>#</a></h2><p>At <a href="https://www.youtube.com/watch?v=lXGcPbmxx8Q">SciPy 2018</a>, Joris Van den Bossche (a scikit-learn and pandas core developer) gives an update on some recent improvements to scikit-learn to make using pandas and scikit-learn together better.</p><p>The biggest addition is <a href=http://scikit-learn.org/dev/modules/generated/sklearn.compose.ColumnTransformer.html><code>sklearn.compose.ColumnTransformer</code></a>, a transformer for working with tabular data.
The basic idea is to specify pairs of <code>(column_selection, transformer)</code>. The transformer will be applied just to the selected columns, and the remaining columns can be passed through or dropped. Column selections can be integer positions (for arrays), names (for DataFrames) or a callable.</p><p>Here&rsquo;s a small example on the &ldquo;tips&rdquo; dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df <span style=color:#f92672>=</span> sns<span style=color:#f92672>.</span>load_dataset(<span style=color:#e6db74>&#39;tips&#39;</span>)
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div class=output><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>total_bill</th><th>tip</th><th>sex</th><th>smoker</th><th>day</th><th>time</th><th>size</th></tr></thead><tbody><tr><th>0</th><td>16.99</td><td>1.01</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr><tr><th>1</th><td>10.34</td><td>1.66</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr><tr><th>2</th><td>21.01</td><td>3.50</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr><tr><th>3</th><td>23.68</td><td>3.31</td><td>Male</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr><tr><th>4</th><td>24.59</td><td>3.61</td><td>Female</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr></tbody></table></div><p>Our target is whether the tip was larger than 15%.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>X <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>drop(<span style=color:#e6db74>&#34;tip&#34;</span>, axis<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;columns&#39;</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>tip <span style=color:#f92672>/</span> df<span style=color:#f92672>.</span>total_bill <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.15</span>
</span></span></code></pre></div><p>We&rsquo;ll make a small pipeline that one-hot encodes the categorical columns (sex, smoker, day, time) before fitting a random forest. The numeric columns (total_bill, size) will be passed through as-is.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sklearn.compose
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sklearn.ensemble
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sklearn.pipeline
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sklearn.preprocessing
</span></span></code></pre></div><p>We use <code>make_column_transformer</code> to create the <code>ColumnTransformer</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>categorical_columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;sex&#39;</span>, <span style=color:#e6db74>&#39;smoker&#39;</span>, <span style=color:#e6db74>&#39;day&#39;</span>, <span style=color:#e6db74>&#39;time&#39;</span>]
</span></span><span style=display:flex><span>categorical_encoder <span style=color:#f92672>=</span> sklearn<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>OneHotEncoder(sparse<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>transformers <span style=color:#f92672>=</span> sklearn<span style=color:#f92672>.</span>compose<span style=color:#f92672>.</span>make_column_transformer(
</span></span><span style=display:flex><span>    (categorical_columns, categorical_encoder),
</span></span><span style=display:flex><span>    remainder<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;passthrough&#39;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>This is just a regular scikit-learn estimator, which can be placed in a pipeline.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pipe <span style=color:#f92672>=</span> sklearn<span style=color:#f92672>.</span>pipeline<span style=color:#f92672>.</span>make_pipeline(
</span></span><span style=display:flex><span>    transformers,
</span></span><span style=display:flex><span>    sklearn<span style=color:#f92672>.</span>ensemble<span style=color:#f92672>.</span>RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipe<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>pipe<span style=color:#f92672>.</span>score(X, y)
</span></span></code></pre></div><div class=output><pre>
1.0
    </pre></div><p>We&rsquo;ve likely overfitted, but that&rsquo;s not really the point of this article. We&rsquo;re more interested in the pre-processing side of things.</p><h2 id=columntransformer-in-dask-ml><code>ColumnTransformer</code> in Dask-ML<a hidden class=anchor aria-hidden=true href=#columntransformer-in-dask-ml>#</a></h2><p><code>ColumnTransfomrer</code> was added to Dask-ML in <a href=https://github.com/dask/dask-ml/pull/315>https://github.com/dask/dask-ml/pull/315</a>.
Ideally, we wouldn&rsquo;t need that PR at all. We would prefer for dask&rsquo;s collections (and pandas dataframes) to just be handled gracefully by scikit-learn. The main blocking issue is that the Python community doesn&rsquo;t currently have a way to write &ldquo;concatenate this list of array-like objects together&rdquo; in a generic way. That&rsquo;s being worked on in <a href=http://www.numpy.org/neps/nep-0018-array-function-protocol.html>NEP-18</a>.</p><p>So for now, if you want to use <code>ColumnTransformer</code> with dask objects, you&rsquo;ll have to use <code>dask_ml.compose.ColumnTransformer</code>, otherwise your large Dask Array or DataFrame would be converted to an in-memory NumPy array.</p><p>As a footnote to this section, the initial PR in Dask-ML was much longer.
I only needed to override one thing (the function <code>_hstack</code> used to glue the results back together). But that was being called from several places, and so I had to override all <em>those</em> places as well. I was able to work with the scikit-learn developers to make <code>_hstack</code> a staticmethod on <code>ColumnTranformer</code>, so any library wishing to extend <code>ColumnTransformer</code> can do so more easily now. The Dask project values working with the existing community.</p><h2 id=challenges-with-scaling>Challenges with Scaling<a hidden class=anchor aria-hidden=true href=#challenges-with-scaling>#</a></h2><p>Many strategies for dealing with large datasets rely on processing the data in chunks.
That&rsquo;s the basic idea behind Dask DataFrame: a Dask DataFrame consists of many pandas DataFrames.
When you write <code>ddf.column.value_counts()</code>, Dask builds a task graph with many <code>pandas.value_counts</code>, and a final aggregation step so that you end up with the same end result.</p><p>But chunking can cause issues when there are variations in your dataset and the operation you&rsquo;re applying depends on the data. For example, consider scikit-learn&rsquo;s <code>OneHotEncoder</code>. By default, it looks at the data and creates a column for each unique value.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>enc <span style=color:#f92672>=</span> sklearn<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>OneHotEncoder(sparse<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>enc<span style=color:#f92672>.</span>fit_transform([[<span style=color:#e6db74>&#39;a&#39;</span>], [<span style=color:#e6db74>&#39;a&#39;</span>], [<span style=color:#e6db74>&#39;b&#39;</span>], [<span style=color:#e6db74>&#39;c&#39;</span>]])
</span></span></code></pre></div><div class=output><pre>
array([[1., 0., 0.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
</pre></div><p>But let&rsquo;s suppose we wanted to process that in chunks of two, first <code>[['a'], ['a']]</code>, then <code>[['b'], ['c']]</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>enc<span style=color:#f92672>.</span>fit_transform([[<span style=color:#e6db74>&#39;a&#39;</span>], [<span style=color:#e6db74>&#39;a&#39;</span>]])
</span></span></code></pre></div><div class="highlight output"><pre>
array([[1.],
       [1.]])
</pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>enc<span style=color:#f92672>.</span>fit_transform([[<span style=color:#e6db74>&#39;b&#39;</span>], [<span style=color:#e6db74>&#39;c&#39;</span>]])
</span></span></code></pre></div><div class="highlight output"><pre>
array([[1., 0.],
       [0., 1.]])
</pre></div><p>We have a problem! Two in fact:</p><ol><li>The shapes don&rsquo;t match. The first batch only saw &ldquo;a&rdquo;, so the output shape is <code>(2, 1)</code>. We can&rsquo;t concatenate these results vertically</li><li>The meaning of the first column of the output has changed. In the first batch, the first column meant &ldquo;a&rdquo; was present. In the second batch, it meant &ldquo;b&rdquo; was present.</li></ol><p>If we happened to know the set of possible values <em>ahead</em> of time, we could pass those to <code>CategoricalEncoder</code>. But storing that set of possible values separate from the data is fragile. It&rsquo;d be better to store the possible values in the <em>data type</em> itself.</p><p>That&rsquo;s exactly what pandas Categorical does. We can confidently know the number of columns in the categorical-encoded data by just looking at the type. Because this is so important in a distributed dataset context, <code>dask_ml.preprocessing.OneHotEncoder</code> differs from scikit-learn when passed categorical data: we use pandas&rsquo; categorical information.</p><h2 id=a-larger-example>A larger Example<a hidden class=anchor aria-hidden=true href=#a-larger-example>#</a></h2><p>We&rsquo;ll work with the Criteo dataset. This has a mixture of numeric and categorical features. It&rsquo;s also a large dataset, which presents some challenges for many pre-processing methods.</p><p>The full dataset is from <a href=http://labs.criteo.com/2013/12/download-terabyte-click-logs/>http://labs.criteo.com/2013/12/download-terabyte-click-logs/</a>.
We&rsquo;ll work with a sample.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> Client()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ordinal_columns <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;category_0&#39;</span>, <span style=color:#e6db74>&#39;category_1&#39;</span>, <span style=color:#e6db74>&#39;category_2&#39;</span>, <span style=color:#e6db74>&#39;category_3&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;category_4&#39;</span>, <span style=color:#e6db74>&#39;category_6&#39;</span>, <span style=color:#e6db74>&#39;category_7&#39;</span>, <span style=color:#e6db74>&#39;category_9&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;category_10&#39;</span>, <span style=color:#e6db74>&#39;category_11&#39;</span>, <span style=color:#e6db74>&#39;category_13&#39;</span>, <span style=color:#e6db74>&#39;category_14&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;category_17&#39;</span>, <span style=color:#e6db74>&#39;category_19&#39;</span>, <span style=color:#e6db74>&#39;category_20&#39;</span>, <span style=color:#e6db74>&#39;category_21&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;category_22&#39;</span>, <span style=color:#e6db74>&#39;category_23&#39;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>onehot_columns <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;category_5&#39;</span>, <span style=color:#e6db74>&#39;category_8&#39;</span>, <span style=color:#e6db74>&#39;category_12&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;category_15&#39;</span>, <span style=color:#e6db74>&#39;category_16&#39;</span>, <span style=color:#e6db74>&#39;category_18&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;category_24&#39;</span>, <span style=color:#e6db74>&#39;category_25&#39;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>numeric_columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;numeric_</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>13</span>)]
</span></span><span style=display:flex><span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;click&#39;</span>] <span style=color:#f92672>+</span> numeric_columns <span style=color:#f92672>+</span> onehot_columns <span style=color:#f92672>+</span> ordinal_columns
</span></span></code></pre></div><p>The raw data is a single large CSV. That&rsquo;s been split with <a href=https://gist.github.com/TomAugspurger/4a058f00b32fc049ab5f2860d03fd579#file-split_csv-py>this script</a> and I took a 10% sample with <a href=https://gist.github.com/TomAugspurger/4a058f00b32fc049ab5f2860d03fd579#file-sample-py>this script</a>, which was written to a directory of parquet files. That&rsquo;s what we&rsquo;ll work with.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sample <span style=color:#f92672>=</span> dd<span style=color:#f92672>.</span>read_parquet(<span style=color:#e6db74>&#34;data/sample-10.parquet/&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Convert unknown categorical to known.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># See note later on.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pf <span style=color:#f92672>=</span> fastparquet<span style=color:#f92672>.</span>ParquetFile(<span style=color:#e6db74>&#34;data/sample-10.parquet/part.0.parquet&#34;</span>)
</span></span><span style=display:flex><span>cats <span style=color:#f92672>=</span> pf<span style=color:#f92672>.</span>grab_cats(onehot_columns)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample <span style=color:#f92672>=</span> sample<span style=color:#f92672>.</span>assign(<span style=color:#f92672>**</span>{
</span></span><span style=display:flex><span>    col: sample[col]<span style=color:#f92672>.</span>cat<span style=color:#f92672>.</span>set_categories(cats[col]) <span style=color:#66d9ef>for</span> col <span style=color:#f92672>in</span> onehot_columns
</span></span><span style=display:flex><span>})
</span></span></code></pre></div><p>Our goal is to predict &lsquo;click&rsquo; using the other columns.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y <span style=color:#f92672>=</span> sample[<span style=color:#e6db74>&#39;click&#39;</span>]
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> sample<span style=color:#f92672>.</span>drop(<span style=color:#e6db74>&#34;click&#34;</span>, axis<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;columns&#39;</span>)
</span></span></code></pre></div><p>Now, let&rsquo;s lay out our pre-processing pipeline. We have three types of columns</p><ol><li>Numeric columns</li><li>Low-cardinality categorical columns</li><li>High-cardinality categorical columns</li></ol><p>Each of those will be processed differently.</p><ol><li>Numeric columns will have missing values filled with the column average and standard scaled</li><li>Low-cardinality categorical columns will be one-hot encoded</li><li>High-cardinality categorical columns will be deterministically hashed and standard scaled</li></ol><p>You&rsquo;ll probably want to quibble with some of these choices, but right now, I&rsquo;m
just interested in the ability to do these kinds of transformations at all.</p><p>We need to define a couple custom estimators, one for hashing the values of a dask dataframe, and one for converting a dask dataframe to a dask array.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sklearn.base
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>hash_block</span>(x: pd<span style=color:#f92672>.</span>DataFrame) <span style=color:#f92672>-&gt;</span> pd<span style=color:#f92672>.</span>DataFrame:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Hash the values in a DataFrame.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    hashed <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        pd<span style=color:#f92672>.</span>Series(pd<span style=color:#f92672>.</span>util<span style=color:#f92672>.</span>hash_array(data<span style=color:#f92672>.</span>values), index<span style=color:#f92672>=</span>x<span style=color:#f92672>.</span>index, name<span style=color:#f92672>=</span>col)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> col, data <span style=color:#f92672>in</span> x<span style=color:#f92672>.</span>iteritems()
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>concat(hashed, axis<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;columns&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>HashingEncoder</span>(sklearn<span style=color:#f92672>.</span>base<span style=color:#f92672>.</span>BaseEstimator, sklearn<span style=color:#f92672>.</span>base<span style=color:#f92672>.</span>TransformerMixin):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> isinstance(X, pd<span style=color:#f92672>.</span>DataFrame):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> hash_block(X)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> isinstance(X, dd<span style=color:#f92672>.</span>DataFrame):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> X<span style=color:#f92672>.</span>map_partitions(hash_block)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;Unexpected type &#39;</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#39; for &#39;X&#39;&#34;</span><span style=color:#f92672>.</span>format(type(X)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ArrayConverter</span>(sklearn<span style=color:#f92672>.</span>base<span style=color:#f92672>.</span>BaseEstimator, sklearn<span style=color:#f92672>.</span>base<span style=color:#f92672>.</span>TransformerMixin):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Convert a Dask DataFrame to a Dask Array with known lengths&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, lengths<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lengths <span style=color:#f92672>=</span> lengths
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> X<span style=color:#f92672>.</span>to_dask_array(lengths<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>lengths)
</span></span></code></pre></div><p>For the final stage, Dask-ML needs to have a Dask Array with known chunk lengths.
So let&rsquo;s compute those ahead of time, and get a bit of info about how large the dataset is while we&rsquo;re at it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lengths <span style=color:#f92672>=</span> sample[<span style=color:#e6db74>&#39;click&#39;</span>]<span style=color:#f92672>.</span>map_partitions(len)
</span></span><span style=display:flex><span>nbytes <span style=color:#f92672>=</span> sample<span style=color:#f92672>.</span>memory_usage(deep<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lengths, nbytes <span style=color:#f92672>=</span> dask<span style=color:#f92672>.</span>compute(lengths, nbytes)
</span></span><span style=display:flex><span>lengths <span style=color:#f92672>=</span> tuple(lengths)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>format_bytes(nbytes)
</span></span></code></pre></div><div class=output><pre>
'19.20 GB'
</pre></div><p>We we&rsquo;ll be working with about 20GB of data on a laptop with 16GB of RAM. We&rsquo;ll clearly be relying on Dask to do the operations in parallel, while keeping things in a small memory footprint.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> dask_ml.compose <span style=color:#f92672>import</span> make_column_transformer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dask_ml.preprocessing <span style=color:#f92672>import</span> StandardScaler, OneHotEncoder
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dask_ml.wrappers <span style=color:#f92672>import</span> Incremental
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dask_ml.impute <span style=color:#f92672>import</span> SimpleImputer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.pipeline <span style=color:#f92672>import</span> make_pipeline
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> FunctionTransformer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> SGDClassifier
</span></span></code></pre></div><p>Now for the pipeline.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>onehot_encoder <span style=color:#f92672>=</span> OneHotEncoder(sparse<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>hashing_encoder <span style=color:#f92672>=</span> HashingEncoder()
</span></span><span style=display:flex><span>nan_imputer <span style=color:#f92672>=</span> SimpleImputer()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>to_numeric <span style=color:#f92672>=</span> make_column_transformer(
</span></span><span style=display:flex><span>    (onehot_columns, onehot_encoder),
</span></span><span style=display:flex><span>    (ordinal_columns, hashing_encoder),
</span></span><span style=display:flex><span>    remainder<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;passthrough&#39;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fill_na <span style=color:#f92672>=</span> make_column_transformer(
</span></span><span style=display:flex><span>    (numeric_columns, nan_imputer),
</span></span><span style=display:flex><span>    remainder<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;passthrough&#39;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>scaler <span style=color:#f92672>=</span> make_column_transformer(
</span></span><span style=display:flex><span>    (list(numeric_columns) <span style=color:#f92672>+</span> list(ordinal_columns), StandardScaler()),
</span></span><span style=display:flex><span>    remainder<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;passthrough&#39;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>clf <span style=color:#f92672>=</span> Incremental(
</span></span><span style=display:flex><span>    SGDClassifier(loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;log&#39;</span>,
</span></span><span style=display:flex><span>                  random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>                  max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> make_pipeline(to_numeric, fill_na, scaler, ArrayConverter(lengths<span style=color:#f92672>=</span>lengths), clf)
</span></span><span style=display:flex><span>pipe
</span></span></code></pre></div><pre><code>Pipeline(memory=None,
     steps=[('columntransformer-1', ColumnTransformer(n_jobs=1, preserve_dataframe=True, remainder='passthrough',
         sparse_threshold=0.3, transformer_weights=None,
         transformers=[('onehotencoder', OneHotEncoder(categorical_features=None, categories='auto',
       dtype=&lt;class 'numpy.float6...ion=0.1, verbose=0, warm_start=False),
      random_state=None, scoring=None, shuffle_blocks=True))])
</code></pre><p>Overall it reads pretty similarly to how we described it in prose.
We specify</p><ol><li>Onehot the low-cardinality categoricals, hash the others</li><li>Fill missing values in the numeric columns</li><li>Standard scale the numeric and hashed columns</li><li>Fit the incremental SGD</li></ol><p>And again, these ColumnTransformers are just estimators so we stick them in a regular scikit-learn <code>Pipeline</code> before calling <code>.fit</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>time pipe<span style=color:#f92672>.</span>fit(X, y<span style=color:#f92672>.</span>to_dask_array(lengths<span style=color:#f92672>=</span>lengths), incremental__classes<span style=color:#f92672>=</span>[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>])
</span></span></code></pre></div><div class=output><pre>
CPU times: user 7min 7s, sys: 41.6 s, total: 7min 48s
Wall time: 16min 42s
<p>Pipeline(memory=None,
steps=[(&lsquo;columntransformer-1&rsquo;, ColumnTransformer(n_jobs=1, preserve_dataframe=True, remainder=&lsquo;passthrough&rsquo;,
sparse_threshold=0.3, transformer_weights=None,
transformers=[(&lsquo;onehotencoder&rsquo;, OneHotEncoder(categorical_features=None, categories=&lsquo;auto&rsquo;,
dtype=&lt;class &rsquo;numpy.float6&hellip;ion=0.1, verbose=0, warm_start=False),
random_state=None, scoring=None, shuffle_blocks=True))])
</pre></p></div><h2 id=discussion>Discussion<a hidden class=anchor aria-hidden=true href=#discussion>#</a></h2><p>Some aspects of this workflow could be improved.</p><ol><li><p>Dask, fastparquet, pyarrow, and pandas don&rsquo;t currently have a way to
specify the categorical dtype of a column split across many files.
Each file (parition) is treated independently. This results in categorials
with unknown categories in the Dask DataFrame.
Since <em>we</em> know that the categories are all the same, we&rsquo;re able to read in
the first files categories and assign those to the entire DataFrame. But this
is a bit fragile, as it relies on an assumption not necessarily guaranteed
by the file structure.</p></li><li><p>There&rsquo;s of IO. As written, each stage of the pipeline that
has to see the data does a full read of the dataset. We end up reading the
entire dataset something like 5 times.
<a href=https://github.com/dask/dask-ml/issues/192>https://github.com/dask/dask-ml/issues/192</a> has some discussion on ways
we can progress through a pipeline. If your pipeline consists entirely of
estimators that learn incrementally, it may make sense to send each block
of data through the entire pipeline, rather than sending all the data to
the first step, then all the data to the second, and so on.
I&rsquo;ll note, however, that you can avoid the redundant IO by loading your
data into distributed RAM on a Dask cluster. But I was just trying things
out on my laptop.</p></li></ol><p>Still, it&rsquo;s worth noting that we&rsquo;ve successfully fit a reasonably complex pipeline on a larger-than-RAM dataset using our laptop. That&rsquo;s something!</p><p>ColumnTransformer will be available in scikit-learn 0.20.0.
This also contains the changes for <a href=sklearn-dask-tabular>distributed joblib</a> I blogged about earlier.
The <a href=https://pypi.org/project/scikit-learn/0.20rc1/>first release candidate</a> is available now.</p><p>For more, visit the <a href=http://docs.dask.org>Dask</a>, <a href=http://ml.dask.org>Dask-ML</a>, and <a href=http://scikit-learn.org/dev/modules/generated/sklearn.compose.ColumnTransformer.html>scikit-learn</a> documentation.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://tomaugspurger.github.io>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>