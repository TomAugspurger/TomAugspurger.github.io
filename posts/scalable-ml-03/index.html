<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scalable Machine Learning (Part 3): Parallel | Tom's Blog</title><meta name=keywords content><meta name=description content="This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
This is part three of my series on scalable machine learning.
Small Fit, Big Predict Scikit-Learn Partial Fit Parallel Machine Learning You can download a notebook of this post [here][notebook].
In part one, I talked about the type of constraints that push us to parallelize or distribute a machine learning workload. Today, we&rsquo;ll be talking about the second constraint, &ldquo;I&rsquo;m constrained by time, and would like to fit more models at once, by using all the cores of my laptop, or all the machines in my cluster&rdquo;."><meta name=author content><link rel=canonical href=https://tomaugspurger.github.io/posts/scalable-ml-03/><link crossorigin=anonymous href=/assets/css/stylesheet.67b4a5a78be69ca812d40e5543fbd83efd0b0cc5025c2505fd7758fd5d32ec2e.css integrity="sha256-Z7Slp4vmnKgS1A5VQ/vYPv0LDMUCXCUF/XdY/V0y7C4=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tomaugspurger.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Scalable Machine Learning (Part 3): Parallel"><meta property="og:description" content="This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
This is part three of my series on scalable machine learning.
Small Fit, Big Predict Scikit-Learn Partial Fit Parallel Machine Learning You can download a notebook of this post [here][notebook].
In part one, I talked about the type of constraints that push us to parallelize or distribute a machine learning workload. Today, we&rsquo;ll be talking about the second constraint, &ldquo;I&rsquo;m constrained by time, and would like to fit more models at once, by using all the cores of my laptop, or all the machines in my cluster&rdquo;."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.github.io/posts/scalable-ml-03/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-09-16T00:00:00+00:00"><meta property="article:modified_time" content="2017-09-16T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scalable Machine Learning (Part 3): Parallel"><meta name=twitter:description content="This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
This is part three of my series on scalable machine learning.
Small Fit, Big Predict Scikit-Learn Partial Fit Parallel Machine Learning You can download a notebook of this post [here][notebook].
In part one, I talked about the type of constraints that push us to parallelize or distribute a machine learning workload. Today, we&rsquo;ll be talking about the second constraint, &ldquo;I&rsquo;m constrained by time, and would like to fit more models at once, by using all the cores of my laptop, or all the machines in my cluster&rdquo;."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Scalable Machine Learning (Part 3): Parallel","item":"https://tomaugspurger.github.io/posts/scalable-ml-03/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scalable Machine Learning (Part 3): Parallel","name":"Scalable Machine Learning (Part 3): Parallel","description":"This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.\nThis is part three of my series on scalable machine learning.\nSmall Fit, Big Predict Scikit-Learn Partial Fit Parallel Machine Learning You can download a notebook of this post [here][notebook].\nIn part one, I talked about the type of constraints that push us to parallelize or distribute a machine learning workload. Today, we\u0026rsquo;ll be talking about the second constraint, \u0026ldquo;I\u0026rsquo;m constrained by time, and would like to fit more models at once, by using all the cores of my laptop, or all the machines in my cluster\u0026rdquo;.","keywords":[],"articleBody":"This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.\nThis is part three of my series on scalable machine learning.\nSmall Fit, Big Predict Scikit-Learn Partial Fit Parallel Machine Learning You can download a notebook of this post [here][notebook].\nIn part one, I talked about the type of constraints that push us to parallelize or distribute a machine learning workload. Today, we’ll be talking about the second constraint, “I’m constrained by time, and would like to fit more models at once, by using all the cores of my laptop, or all the machines in my cluster”.\nAn Aside on Parallelism In the case of Python, we have two main avenues of parallelization (which we’ll roughly define as using multiple “workers” to do some “work” in less time). Those two avenues are\nmulti-threading multi-processing For python, the most important differences are that\nmulti-threaded code can potentially be limited by the GIL multi-processing code requires that data be serialized between processes The GIL is the “Global Interpreter Lock”, an implementation detail of CPython that means only one thread in your python process can be executing python code at once.\nThis talk by Python core-developer Raymond Hettinger does a good job summarizing things for Python, with an important caveat: much of what he says about the GIL doesn’t apply to the scientific python stack. NumPy, scikit-learn, and much of pandas release the GIL and can run multi-threaded, using shared memory and so avoiding serialization costs. I’ll highlight his quote, which summarizes the situation:\nYour weakness is your strength, and your strength is your weakness\nThe strength of threads is shared state. The weakness of threads is shared state.\nAnother wrinkle here is that when you move to a distributed cluster, you have to have multiple processes. And communication between processes becomes even more expensive since you’ll have network overhead to worry about, in addition to the serialization costs.\nFortunately, modules like concurrent.futures and libraries like dask make it easy to swap one mode in for another. Let’s make a little dask array:\nimport dask.array as da import dask import dask.threaded import dask.multiprocessing X = da.random.uniform(size=(10000, 10), chunks=(1000, 10)) result = X / (X.T @ X).sum(1) We can swap out the scheduler with a context-manager:\n%%time with dask.set_options(get=dask.threaded.get): # threaded is the default for dask.array anyway result.compute() %%time with dask.set_options(get=dask.multiprocessing.get): result.compute() Every dask collection (dask.array, dask.dataframe, dask.bag) has a default scheduler that typically works well for the kinds of operations it does. For dask.array and dask.dataframe, the shared-memory threaded scheduler is used.\nCost Models In this talk, Simon Peyton Jones talks about parallel and distributed computing for Haskell. He stressed repeatedly that there’s no silver bullet when it comes to parallelism. The type of parallelism appropriate for a web server, say, may be different than the type of parallelism appropriate for a machine learning algorithm.\nI mention all this, since we’re about to talk about parallel machine learning. In general, for small data and many models you’ll want to use the threaded scheduler. For bigger data (larger than memory), you’ll want want to use the distributed scheduler. Assuming the underlying NumPy, SciPy, scikit-learn, or pandas operation releases the GIL, you’ll be able to get nice speedups without the cost of serialization. But again, there isn’t a silver bullet here, and the best type of parallelism will depend on your particular problem.\nWhere to Parallelize In a typical machine-learning workflow, there are typically ample opportunities for parallelism.\nOver Hyper-parameters (one fit per combination of parameters) Over Cross-validation folds (one fit per fold) Within an algorithm (for some algorithms) Scikit-learn already uses parallelism in many places, anywhere you see an n_jobs keyword.\n","wordCount":"614","inLanguage":"en","datePublished":"2017-09-16T00:00:00Z","dateModified":"2017-09-16T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.github.io/posts/scalable-ml-03/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.github.io accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.github.io/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.github.io/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Scalable Machine Learning (Part 3): Parallel</h1><div class=post-meta><span title='2017-09-16 00:00:00 +0000 UTC'>September 16, 2017</span></div></header><div class=post-content><p><em>This work is supported by <a href=https://www.anaconda.com/>Anaconda, Inc.</a> and the
Data Driven Discovery Initiative from the <a href=https://www.moore.org/>Moore Foundation</a>.</em></p><p>This is part three of my series on scalable machine learning.</p><ul><li><a href=scalable-ml-01>Small Fit, Big Predict</a></li><li><a href=scalable-ml-02>Scikit-Learn Partial Fit</a></li><li><a href=scalable-ml-03>Parallel Machine Learning</a></li></ul><p>You can download a notebook of this post [here][notebook].</p><hr><p>In <a href=scalable-ml-01>part one</a>, I talked about the type of constraints that push
us to parallelize or distribute a machine learning workload. Today, we&rsquo;ll be
talking about the second constraint, &ldquo;I&rsquo;m constrained by time, and would like to
fit more models at once, by using all the cores of my laptop, or all the
machines in my cluster&rdquo;.</p><h2 id=an-aside-on-parallelism>An Aside on Parallelism<a hidden class=anchor aria-hidden=true href=#an-aside-on-parallelism>#</a></h2><p>In the case of Python, we have two main avenues of parallelization (which we&rsquo;ll
roughly define as using multiple &ldquo;workers&rdquo; to do some &ldquo;work&rdquo; in less time).
Those two avenues are</p><ol><li>multi-threading</li><li>multi-processing</li></ol><p>For python, the most important differences are that</p><ol><li>multi-threaded code can <em>potentially</em> be limited by the GIL</li><li>multi-processing code requires that data be serialized between processes</li></ol><p>The GIL is the &ldquo;Global Interpreter Lock&rdquo;, an implementation detail of CPython
that means only one thread in your python process can be executing python code
at once.</p><p><a href="https://www.youtube.com/watch?v=9zinZmE3Ogk">This talk</a> by Python
core-developer Raymond Hettinger does a good job summarizing things for Python,
with an important caveat: much of what he says about the GIL doesn&rsquo;t apply to
the <em>scientific</em> python stack. NumPy, scikit-learn, and much of pandas release
the GIL and can run multi-threaded, using shared memory and so avoiding
serialization costs. I&rsquo;ll highlight his quote, which summarizes the
situation:</p><blockquote><blockquote><p>Your weakness is your strength, and your strength is your weakness</p></blockquote></blockquote><blockquote><p>The strength of threads is shared state. The weakness of threads is shared
state.</p></blockquote><p>Another wrinkle here is that when you move to a distributed cluster, you <em>have</em>
to have multiple processes. And communication between processes becomes even
more expensive since you&rsquo;ll have network overhead to worry about, in addition to
the serialization costs.</p><p>Fortunately, modules like <code>concurrent.futures</code> and libraries like <code>dask</code> make it
easy to swap one mode in for another. Let&rsquo;s make a little dask array:</p><pre tabindex=0><code>import dask.array as da
import dask
import dask.threaded
import dask.multiprocessing

X = da.random.uniform(size=(10000, 10), chunks=(1000, 10))
result = X / (X.T @ X).sum(1)
</code></pre><p>We can swap out the scheduler with a context-manager:</p><pre tabindex=0><code>%%time
with dask.set_options(get=dask.threaded.get):
    # threaded is the default for dask.array anyway
    result.compute()
</code></pre><pre tabindex=0><code>%%time
with dask.set_options(get=dask.multiprocessing.get):
    result.compute()
</code></pre><p>Every dask collection (<code>dask.array</code>, <code>dask.dataframe</code>, <code>dask.bag</code>) has a default
scheduler that typically works well for the kinds of operations it does. For
<code>dask.array</code> and <code>dask.dataframe</code>, the shared-memory threaded scheduler is used.</p><h2 id=cost-models>Cost Models<a hidden class=anchor aria-hidden=true href=#cost-models>#</a></h2><p>In <a href="https://www.youtube.com/watch?v=tC94Mkg-oJU">this talk</a>, Simon Peyton Jones
talks about parallel and distributed computing for Haskell. He stressed
repeatedly that there&rsquo;s no silver bullet when it comes to parallelism. The type
of parallelism appropriate for a web server, say, may be different than the type
of parallelism appropriate for a machine learning algorithm.</p><p>I mention all this, since we&rsquo;re about to talk about parallel machine learning.
<em>In general</em>, for small data and many models you&rsquo;ll want to use the threaded
scheduler. For bigger data (larger than memory), you&rsquo;ll want want to use the
distributed scheduler. Assuming the underlying NumPy, SciPy, scikit-learn, or
pandas operation releases the GIL, you&rsquo;ll be able to get nice speedups without
the cost of serialization. But again, there isn&rsquo;t a silver bullet here, and the
best type of parallelism will depend on your particular problem.</p><h2 id=where-to-parallelize>Where to Parallelize<a hidden class=anchor aria-hidden=true href=#where-to-parallelize>#</a></h2><p>In a typical machine-learning workflow, there are typically ample opportunities for
parallelism.</p><ol><li>Over Hyper-parameters (one fit per combination of parameters)</li><li>Over Cross-validation folds (one fit per fold)</li><li>Within an algorithm (for some algorithms)</li></ol><p>Scikit-learn already uses parallelism in many places, anywhere you see an
<code>n_jobs</code> keyword.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://tomaugspurger.github.io>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>