<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Modern Pandas (Part 8): Scaling | Tom's Blog</title>
<meta name=keywords content="pandas"><meta name=description content="This is part 1 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling As I sit down to write this, the third-most popular pandas question on StackOverflow covers how to use pandas for large datasets. This is in tension with the fact that a pandas DataFrame is an in memory container. You can&rsquo;t have a DataFrame larger than your machine&rsquo;s RAM."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/modern-8-scaling/><link crossorigin=anonymous href=/assets/css/stylesheet.ced21e6d3497ee93fed8f8b357448095840179bd510b5ea0e6013078712e6dd1.css integrity="sha256-ztIebTSX7pP+2PizV0SAlYQBeb1RC16g5gEweHEubdE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tomaugspurger.net/posts/modern-8-scaling/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Modern Pandas (Part 8): Scaling"><meta property="og:description" content="This is part 1 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling As I sit down to write this, the third-most popular pandas question on StackOverflow covers how to use pandas for large datasets. This is in tension with the fact that a pandas DataFrame is an in memory container. You can&rsquo;t have a DataFrame larger than your machine&rsquo;s RAM."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/modern-8-scaling/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-04-23T00:00:00+00:00"><meta property="article:modified_time" content="2018-04-23T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Modern Pandas (Part 8): Scaling"><meta name=twitter:description content="This is part 1 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling As I sit down to write this, the third-most popular pandas question on StackOverflow covers how to use pandas for large datasets. This is in tension with the fact that a pandas DataFrame is an in memory container. You can&rsquo;t have a DataFrame larger than your machine&rsquo;s RAM."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":2,"name":"Modern Pandas (Part 8): Scaling","item":"https://tomaugspurger.net/posts/modern-8-scaling/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Modern Pandas (Part 8): Scaling","name":"Modern Pandas (Part 8): Scaling","description":"This is part 1 in my series on writing modern idiomatic pandas.\nModern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling As I sit down to write this, the third-most popular pandas question on StackOverflow covers how to use pandas for large datasets. This is in tension with the fact that a pandas DataFrame is an in memory container. You can\u0026rsquo;t have a DataFrame larger than your machine\u0026rsquo;s RAM.","keywords":["pandas"],"articleBody":" This is part 1 in my series on writing modern idiomatic pandas.\nModern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling As I sit down to write this, the third-most popular pandas question on StackOverflow covers how to use pandas for large datasets. This is in tension with the fact that a pandas DataFrame is an in memory container. You can’t have a DataFrame larger than your machine’s RAM. In practice, your available RAM should be several times the size of your dataset, as you or pandas will have to make intermediate copies as part of the analysis.\nHistorically, pandas users have scaled to larger datasets by switching away from pandas or using iteration. Both of these are perfectly valid approaches, but changing your workflow in response to scaling data is unfortunate. I use pandas because it’s a pleasant experience, and I would like that experience to scale to larger datasets. That’s what Dask, a parallel computing library, enables. We’ll discuss Dask in detail later. But first, let’s work through scaling a simple analysis to a larger than memory dataset.\nOur task is to find the 100 most-common occupations reported in the FEC’s individual contributions dataest. The files are split by election cycle (2007-2008, 2009-2010, …). You can find some scripts for downloading the data in this repository. My laptop can read in each cycle’s file individually, but the full dataset is too large to read in at once. Let’s read in just 2010’s file, and do the “small data” version.\nfrom pathlib import Path import pandas as pd import seaborn as sns df = pd.read_parquet(\"data/indiv-10.parq\", columns=['occupation'], engine='pyarrow') most_common = df.occupation.value_counts().nlargest(100) most_common RETIRED 279775 ATTORNEY 166768 PRESIDENT 81336 PHYSICIAN 73015 HOMEMAKER 66057 ... C.E.O. 1945 EMERGENCY PHYSICIAN 1944 BUSINESS EXECUTIVE 1924 BUSINESS REPRESENTATIVE 1879 GOVERNMENT AFFAIRS 1867 Name: occupation, Length: 100, dtype: int64 After reading in the file, our actual analysis is a simple 1-liner using two operations built into pandas. Truly, the best of all possible worlds.\nNext, we’ll do the analysis for the entire dataset, which is larger than memory, in two ways. First we’ll use just pandas and iteration. Then we’ll use Dask.\nUsing Iteration To do this with just pandas we have to rewrite our code, taking care to never have too much data in RAM at once. We will\nCreate a global total_counts Series that contains the counts from all of the files processed so far Read in a file Compute a temporary variable counts with the counts for just this file Add that temporary counts into the global total_counts Select the 100 largest with .nlargest This works since the total_counts Series is relatively small, and each year’s data fits in RAM individually. Our peak memory usage should be the size of the largest individual cycle (2015-2016) plus the size of total_counts (which we can essentially ignore).\nfiles = sorted(Path(\"data/\").glob(\"indiv-*.parq\")) total_counts = pd.Series() for year in files: df = pd.read_parquet(year, columns=['occupation'], engine=\"pyarrow\") counts = df.occupation.value_counts() total_counts = total_counts.add(counts, fill_value=0) total_counts = total_counts.nlargest(100).sort_values(ascending=False) RETIRED 4769520 NOT EMPLOYED 2656988 ATTORNEY 1340434 PHYSICIAN 659082 HOMEMAKER 494187 ... CHIEF EXECUTIVE OFFICER 26551 SURGEON 25521 EDITOR 25457 OPERATOR 25151 ORTHOPAEDIC SURGEON 24384 Name: occupation, Length: 100, dtype: int64 While this works, our small one-liner has ballooned in size (and complexity; should you really have to know about Series.add’s fill_value parameter for this simple analysis?). If only there was a better way…\nUsing Dask With Dask, we essentially recover our original code. We’ll change our import to use dask.dataframe.read_parquet, which returns a Dask DataFrame.\nimport dask.dataframe as dd df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation']) most_common = df.occupation.value_counts().nlargest(100) most_common.compute().sort_values(ascending=False) RETIRED 4769520 NOT EMPLOYED 2656988 ATTORNEY 1340434 PHYSICIAN 659082 HOMEMAKER 494187 ... CHIEF EXECUTIVE OFFICER 26551 SURGEON 25521 EDITOR 25457 OPERATOR 25151 ORTHOPAEDIC SURGEON 24384 Name: occupation, Length: 100, dtype: int64 There are a couple differences from the original pandas version, which we’ll discuss next, but overall I hope you agree that the Dask version is nicer than the version using iteration.\nDask Now that we’ve seen dask.dataframe in action, let’s step back and discuss Dask a bit. Dask is an open-source project that natively parallizes Python. I’m a happy user of and contributor to Dask.\nAt a high-level, Dask provides familiar APIs for large N-dimensional arrays, large DataFrames, and familiar ways to parallelize custom algorithms.\nAt a low-level, each of these is built on high-performance task scheduling that executes operations in parallel. The low-level details aren’t too important; all we care about is that\nDask works with task graphs (tasks: functions to call on data, and graphs: the relationships between tasks). This is a flexible and performant way to parallelize many different kinds of problems. To understand point 1, let’s examine the difference between a Dask DataFrame and a pandas DataFrame. When we read in df with dd.read_parquet, we received a Dask DataFrame.\ndf Dask DataFrame Structure: occupation npartitions=35 object ... ... ... ... ... Dask Name: read-parquet, 35 tasks A Dask DataFrame consists of many pandas DataFrames arranged by the index. Dask is really just coordinating these pandas DataFrames.\nAll the actual computation (reading from disk, computing the value counts, etc.) eventually use pandas internally. If I do df.occupation.str.len, Dask will coordinate calling pandas.Series.str.len on each of the pandas DataFrames.\nThose reading carefully will notice a problem with the statement “A Dask DataFrame consists of many pandas DataFrames”. Our initial problem was that we didn’t have enough memory for those DataFrames! How can Dask be coordinating DataFrames if there isn’t enough memory? This brings us to the second major difference: Dask DataFrames (and arrays) are lazy. Operations on them don’t execute and produce the final result immediately. Rather, calling methods on them builds up a task graph.\nWe can visualize task graphs using graphviz. For the blog, I’ve trimmed down the example to be a subset of the entire graph.\ndf.visualize(rankdir='LR') df (the dask DataFrame consisting of many pandas DataFrames) has a task graph with 5 calls to a parquet reader (one for each file), each of which produces a DataFrame when called.\nCalling additional methods on df adds additional tasks to this graph. For example, our most_common Series has three additional calls\nSelect the occupation column (__getitem__) Perform the value counts Select the 100 largest values most_common = df.occupation.value_counts().nlargest(100) most_common Dask Series Structure: npartitions=1 int64 ... Name: occupation, dtype: int64 Dask Name: series-nlargest-agg, 113 tasks Which we can visualize.\nmost_common.visualize(rankdir='LR') So most_common doesn’t hold the actual answer yet. Instead, it holds a recipe for the answer; a list of all the steps to take to get the concrete result. One way to ask for the result is with the compute method.\nmost_common.compute() RETIRED 4769520 NOT EMPLOYED 2656988 ATTORNEY 1340434 PHYSICIAN 659082 HOMEMAKER 494187 ... CHIEF EXECUTIVE OFFICER 26551 SURGEON 25521 EDITOR 25457 OPERATOR 25151 ORTHOPAEDIC SURGEON 24384 Name: occupation, Length: 100, dtype: int64 At this point, the task graph is handed to a scheduler, which is responsible for executing a task graph. Schedulers can analyze a task graph and find sections that can run in parallel. (Dask includes several schedulers. See the scheduling documentation for how to choose, though Dask has good defaults.)\nSo that’s a high-level tour of how Dask works:\nVarious collections collections like dask.dataframe and dask.array provide users familiar APIs for working with large datasets. Computations are represented as a task graph. These graphs could be built by hand, or more commonly built by one of the collections. Dask schedulers run task graphs in parallel (potentially distributed across a cluster), reusing libraries like NumPy and pandas to do the computations. Let’s finish off this post by continuing to explore the FEC dataset with Dask. At this point, we’ll use the distributed scheduler for it’s nice diagnostics.\nimport dask.dataframe as dd from dask import compute from dask.distributed import Client import seaborn as sns client = Client(processes=False) Calling Client without providing a scheduler address will make a local “cluster” of threads or processes on your machine. There are many ways to deploy a Dask cluster onto an actual cluster of machines, though we’re particularly fond of Kubernetes. This highlights one of my favorite features of Dask: it scales down to use a handful of threads on a laptop or up to a cluster with thousands of nodes. Dask can comfortably handle medium-sized datasets (dozens of GBs, so larger than RAM) on a laptop. Or it can scale up to very large datasets with a cluster.\nindividual_cols = ['cmte_id', 'entity_tp', 'employer', 'occupation', 'transaction_dt', 'transaction_amt'] indiv = dd.read_parquet('data/indiv-*.parq', columns=individual_cols, engine=\"pyarrow\") indiv Dask DataFrame Structure: cmte_id entity_tp employer occupation transaction_dt transaction_amt npartitions=5 object object object object datetime64[ns] int64 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Dask Name: read-parquet, 5 tasks We can compute summary statistics like the average mean and standard deviation of the transaction amount:\navg_transaction = indiv.transaction_amt.mean() We can answer questions like “Which employer’s employees donated the most?”\ntotal_by_employee = ( indiv.groupby('employer') .transaction_amt.sum() .nlargest(10) ) Or “what is the average amount donated per occupation?”\navg_by_occupation = ( indiv.groupby(\"occupation\") .transaction_amt.mean() .nlargest(10) ) Since Dask is lazy, we haven’t actually computed anything.\ntotal_by_employee Dask Series Structure: npartitions=1 int64 ... Name: transaction_amt, dtype: int64 Dask Name: series-nlargest-agg, 13 tasks avg_transaction, avg_by_occupation and total_by_employee are three separate computations (they have different task graphs), but we know they share some structure: they’re all reading in the same data, they might select the same subset of columns, and so on. Dask is able to avoid redundant computation when you use the top-level dask.compute function.\n%%time avg_transaction, by_employee, by_occupation = compute( avg_transaction, total_by_employee, avg_by_occupation ) CPU times: user 57.5 s, sys: 14.4 s, total: 1min 11s Wall time: 54.9 s avg_transaction 566.0899206077507 by_employee employer RETIRED 1019973117 SELF-EMPLOYED 834547641 SELF 537402882 SELF EMPLOYED 447363032 NONE 418011322 HOMEMAKER 355195126 NOT EMPLOYED 345770418 FAHR, LLC 166679844 CANDIDATE 75186830 ADELSON DRUG CLINIC 53358500 Name: transaction_amt, dtype: int64 by_occupation occupation CHAIRMAN CEO \u0026 FOUNDER 1,023,333.33 PAULSON AND CO., INC. 1,000,000.00 CO-FOUNDING DIRECTOR 875,000.00 CHAIRMAN/CHIEF TECHNOLOGY OFFICER 750,350.00 CO-FOUNDER, DIRECTOR, CHIEF INFORMATIO 675,000.00 CO-FOUNDER, DIRECTOR 550,933.33 MOORE CAPITAL GROUP, LP 500,000.00 PERRY HOMES 500,000.00 OWNER, FOUNDER AND CEO 500,000.00 CHIEF EXECUTIVE OFFICER/PRODUCER 500,000.00 Name: transaction_amt, dtype: float64 Things like filtering work well. Let’s find the 10 most common occupations and filter the dataset down to just those.\ntop_occupations = ( indiv.occupation.value_counts() .nlargest(10).index ).compute() top_occupations Index(['RETIRED', 'NOT EMPLOYED', 'ATTORNEY', 'PHYSICIAN', 'HOMEMAKER', 'PRESIDENT', 'PROFESSOR', 'CONSULTANT', 'EXECUTIVE', 'ENGINEER'], dtype='object') We’ll filter the raw records down to just the ones from those occupations. Then we’ll compute a few summary statistics on the transaction amounts for each group.\ndonations = ( indiv[indiv.occupation.isin(top_occupations)] .groupby(\"occupation\") .transaction_amt .agg(['count', 'mean', 'sum', 'max']) ) total_avg, occupation_avg = compute(indiv.transaction_amt.mean(), donations['mean']) These are small, concrete results so we can turn to familiar tools like matplotlib to visualize the result.\nax = occupation_avg.sort_values(ascending=False).plot.barh(color='k', width=0.9); lim = ax.get_ylim() ax.vlines(total_avg, *lim, color='C1', linewidth=3) ax.legend(['Average donation']) ax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\") sns.despine() Dask inherits all of pandas’ great time-series support. We can get the total amount donated per day using a resample.\ndaily = ( indiv[['transaction_dt', 'transaction_amt']].dropna() .set_index('transaction_dt')['transaction_amt'] .resample(\"D\") .sum() ).compute() daily 1916-01-23 1000 1916-01-24 0 1916-01-25 0 1916-01-26 0 1916-01-27 0 ... 2201-05-29 0 2201-05-30 0 2201-05-31 0 2201-06-01 0 2201-06-02 2000 Name: transaction_amt, Length: 104226, dtype: int64 It seems like we have some bad data. This should just be 2007-2016. We’ll filter it down to the real subset before plotting. Notice that the seamless transition from dask.dataframe operations above, to pandas operations below.\nsubset = daily.loc['2011':'2016'] ax = subset.div(1000).plot(figsize=(12, 6)) ax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",) sns.despine(); Joining Like pandas, Dask supports joining together multiple datasets.\nIndividual donations are made to committees. Committees are what make the actual expenditures (buying a TV ad). Some committees are directly tied to a candidate (this are campaign committees). Other committees are tied to a group (like the Republican National Committee). Either may be tied to a party.\nLet’s read in the committees. The total number of committees is small, so we’ll .compute immediately to get a pandas DataFrame (the reads still happen in parallel!).\ncommittee_cols = ['cmte_id', 'cmte_nm', 'cmte_tp', 'cmte_pty_affiliation'] cm = dd.read_parquet(\"data/cm-*.parq\", columns=committee_cols).compute() # Some committees change thier name, but the ID stays the same cm = cm.groupby('cmte_id').last() cm cmte_nm cmte_tp cmte_pty_affiliation cmte_id C00000042 ILLINOIS TOOL WORKS INC. FOR BETTER GOVERNMENT... Q NaN C00000059 HALLMARK CARDS PAC Q UNK C00000422 AMERICAN MEDICAL ASSOCIATION POLITICAL ACTION ... Q NaN C00000489 D R I V E POLITICAL FUND CHAPTER 886 N NaN C00000547 KANSAS MEDICAL SOCIETY POLITICAL ACTION COMMITTEE Q UNK ... ... ... ... C90017237 ORGANIZE NOW I NaN C90017245 FRANCISCO AGUILAR I NaN C90017336 LUDWIG, EUGENE I NaN C99002396 AMERICAN POLITICAL ACTION COMMITTEE Q NaN C99003428 THIRD DISTRICT REPUBLICAN PARTY Y REP 28612 rows × 3 columns\nWe’ll use dd.merge, which is analogous to pd.merge for joining a Dask DataFrame with a pandas or Dask DataFrame.\nindiv = indiv[(indiv.transaction_dt \u003e= pd.Timestamp(\"2007-01-01\")) \u0026 (indiv.transaction_dt \u003c= pd.Timestamp(\"2018-01-01\"))] df2 = dd.merge(indiv, cm.reset_index(), on='cmte_id') df2 Dask DataFrame Structure: cmte_id entity_tp employer occupation transaction_dt transaction_amt cmte_nm cmte_tp cmte_pty_affiliation npartitions=20 object object object object datetime64[ns] int64 object object object ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Dask Name: merge, 141 tasks Now we can find which party raised more over the course of each election. We’ll group by the day and party and sum the transaction amounts.\nindiv = indiv.repartition(npartitions=10) df2 = dd.merge(indiv, cm.reset_index(), on='cmte_id') df2 Dask DataFrame Structure: cmte_id entity_tp employer occupation transaction_dt transaction_amt cmte_nm cmte_tp cmte_pty_affiliation npartitions=10 object object object object datetime64[ns] int64 object object object ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Dask Name: merge, 141 tasks party_donations = ( df2.groupby([df2.transaction_dt, 'cmte_pty_affiliation']) .transaction_amt.sum() ).compute().sort_index() We’ll filter that down to just Republican and Democrats and plot.\nax = ( party_donations.loc[:, ['REP', 'DEM']] .unstack(\"cmte_pty_affiliation\").iloc[1:-2] .rolling('30D').mean().plot(color=['C0', 'C3'], figsize=(12, 6), linewidth=3) ) sns.despine() ax.set(title=\"Daily Donations (30-D Moving Average)\", xlabel=\"Date\"); Try It Out! So that’s a taste of Dask. Next time you hit a scaling problem with pandas (or NumPy, scikit-learn, or your custom code), feel free to\npip install dask[complete] or\nconda install dask The dask homepage has links to all the relevant documentation, and binder notebooks where you can try out Dask before installing.\nAs always, reach out to me on Twitter or in the comments if you have anything to share.\n","wordCount":"2428","inLanguage":"en","datePublished":"2018-04-23T00:00:00Z","dateModified":"2018-04-23T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/modern-8-scaling/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net/ accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Modern Pandas (Part 8): Scaling</h1><div class=post-meta><span title='2018-04-23 00:00:00 +0000 UTC'>April 23, 2018</span></div></header><div class=post-content><hr><p>This is part 1 in my series on writing modern idiomatic pandas.</p><ul><li><a href=/posts/modern-1-intro>Modern Pandas</a></li><li><a href=/posts/method-chaining>Method Chaining</a></li><li><a href=/posts/modern-3-indexes>Indexes</a></li><li><a href=/posts/modern-4-performance>Fast Pandas</a></li><li><a href=/posts/modern-5-tidy>Tidy Data</a></li><li><a href=/posts/modern-6-visualization>Visualization</a></li><li><a href=/posts/modern-7-timeseries>Time Series</a></li><li><a href=/posts/modern-8-scaling>Scaling</a></li></ul><hr><p>As I sit down to write this, the third-most popular pandas question on StackOverflow covers <a href=https://stackoverflow.com/q/14262433/1889400>how to use pandas for large datasets</a>. This is in tension with the fact that a pandas DataFrame is an in memory container. <em>You can&rsquo;t have a <code>DataFrame</code> larger than your machine&rsquo;s RAM</em>. In practice, your available RAM should be several times the size of your dataset, as you or pandas will have to make intermediate copies as part of the analysis.</p><p>Historically, pandas users have scaled to larger datasets by switching away from pandas or using iteration. Both of these are perfectly valid approaches, but changing your workflow in response to scaling data is unfortunate. I use pandas because it&rsquo;s a pleasant experience, and I would like that experience to scale to larger datasets. That&rsquo;s what <a href=dask.pydata.org/>Dask</a>, a parallel computing library, enables. We&rsquo;ll discuss Dask in detail later. But first, let&rsquo;s work through scaling a simple analysis to a larger than memory dataset.</p><p>Our task is to find the 100 most-common occupations reported in the FEC&rsquo;s <a href=https://classic.fec.gov/finance/disclosure/ftpdet.shtml>individual contributions dataest</a>. The files are split by election cycle (2007-2008, 2009-2010, &mldr;). You can find some scripts for downloading the data in <a href=https://github.com/tomaugspurger/scalable-ml-fec>this repository</a>. My laptop can read in each cycle&rsquo;s file individually, but the full dataset is too large to read in at once. Let&rsquo;s read in just 2010&rsquo;s file, and do the &ldquo;small data&rdquo; version.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_parquet(<span style=color:#e6db74>&#34;data/indiv-10.parq&#34;</span>, columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;occupation&#39;</span>], engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pyarrow&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>most_common <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>occupation<span style=color:#f92672>.</span>value_counts()<span style=color:#f92672>.</span>nlargest(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>most_common
</span></span></code></pre></div><pre>
    RETIRED                    279775
    ATTORNEY                   166768
    PRESIDENT                   81336
    PHYSICIAN                   73015
    HOMEMAKER                   66057
                                ...  
    C.E.O.                       1945
    EMERGENCY PHYSICIAN          1944
    BUSINESS EXECUTIVE           1924
    BUSINESS REPRESENTATIVE      1879
    GOVERNMENT AFFAIRS           1867
    Name: occupation, Length: 100, dtype: int64
</pre><p>After reading in the file, our actual analysis is a simple 1-liner using two operations built into pandas. Truly, the best of all possible worlds.</p><p>Next, we&rsquo;ll do the analysis for the entire dataset, which is larger than memory, in two ways. First we&rsquo;ll use just pandas and iteration. Then we&rsquo;ll use Dask.</p><h3 id=using-iteration>Using Iteration<a hidden class=anchor aria-hidden=true href=#using-iteration>#</a></h3><p>To do this with just pandas we have to rewrite our code, taking care to never have too much data in RAM at once. We will</p><ol><li>Create a global <code>total_counts</code> Series that contains the counts from all of the files processed so far</li><li>Read in a file</li><li>Compute a temporary variable <code>counts</code> with the counts for just this file</li><li>Add that temporary <code>counts</code> into the global <code>total_counts</code></li><li>Select the 100 largest with <code>.nlargest</code></li></ol><p>This works since the <code>total_counts</code> Series is relatively small, and each year&rsquo;s data fits in RAM individually. Our peak memory usage should be the size of the largest individual cycle (2015-2016) plus the size of <code>total_counts</code> (which we can essentially ignore).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>files <span style=color:#f92672>=</span> sorted(Path(<span style=color:#e6db74>&#34;data/&#34;</span>)<span style=color:#f92672>.</span>glob(<span style=color:#e6db74>&#34;indiv-*.parq&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>total_counts <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>Series()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> year <span style=color:#f92672>in</span> files:
</span></span><span style=display:flex><span>    df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_parquet(year, columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;occupation&#39;</span>],
</span></span><span style=display:flex><span>                         engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pyarrow&#34;</span>)
</span></span><span style=display:flex><span>    counts <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>occupation<span style=color:#f92672>.</span>value_counts()
</span></span><span style=display:flex><span>    total_counts <span style=color:#f92672>=</span> total_counts<span style=color:#f92672>.</span>add(counts, fill_value<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>total_counts <span style=color:#f92672>=</span> total_counts<span style=color:#f92672>.</span>nlargest(<span style=color:#ae81ff>100</span>)<span style=color:#f92672>.</span>sort_values(ascending<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><pre>
RETIRED                    4769520
NOT EMPLOYED               2656988
ATTORNEY                   1340434
PHYSICIAN                   659082
HOMEMAKER                   494187
                            ...   
CHIEF EXECUTIVE OFFICER      26551
SURGEON                      25521
EDITOR                       25457
OPERATOR                     25151
ORTHOPAEDIC SURGEON          24384
Name: occupation, Length: 100, dtype: int64
</pre><p>While this works, our small one-liner has ballooned in size (and complexity; should you <em>really</em> have to know about <code>Series.add</code>&rsquo;s <code>fill_value</code> parameter for this simple analysis?). If only there was a better way&mldr;</p><h3 id=using-dask>Using Dask<a hidden class=anchor aria-hidden=true href=#using-dask>#</a></h3><p>With Dask, we essentially recover our original code. We&rsquo;ll change our import to use <code>dask.dataframe.read_parquet</code>, which returns a Dask DataFrame.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> dask.dataframe <span style=color:#66d9ef>as</span> dd
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df <span style=color:#f92672>=</span> dd<span style=color:#f92672>.</span>read_parquet(<span style=color:#e6db74>&#34;data/indiv-*.parquet&#34;</span>, engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pyarrow&#39;</span>, columns<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;occupation&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>most_common <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>occupation<span style=color:#f92672>.</span>value_counts()<span style=color:#f92672>.</span>nlargest(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>most_common<span style=color:#f92672>.</span>compute()<span style=color:#f92672>.</span>sort_values(ascending<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><pre>
RETIRED                    4769520
NOT EMPLOYED               2656988
ATTORNEY                   1340434
PHYSICIAN                   659082
HOMEMAKER                   494187
                            ...   
CHIEF EXECUTIVE OFFICER      26551
SURGEON                      25521
EDITOR                       25457
OPERATOR                     25151
ORTHOPAEDIC SURGEON          24384
Name: occupation, Length: 100, dtype: int64
</pre><p>There are a couple differences from the original pandas version, which we&rsquo;ll discuss next, but overall I hope you agree that the Dask version is nicer than the version using iteration.</p><h2 id=dask>Dask<a hidden class=anchor aria-hidden=true href=#dask>#</a></h2><p>Now that we&rsquo;ve seen <code>dask.dataframe</code> in action, let&rsquo;s step back and discuss Dask a bit. Dask is an open-source project that natively parallizes Python. I&rsquo;m a happy user of and contributor to Dask.</p><p>At a high-level, Dask provides familiar APIs for <a href=https://dask.pydata.org/en/latest/array.html>large N-dimensional arrays</a>, <a href=https://dask.pydata.org/en/latest/dataframe.html>large DataFrames</a>, and <a href=https://distributed.readthedocs.io/en/latest/quickstart.html#map-and-submit-functions>familiar</a> ways to parallelize <a href=https://dask.pydata.org/en/latest/delayed.html>custom algorithms</a>.</p><p>At a low-level, each of these is built on high-performance <a href=http://dask.pydata.org/en/latest/scheduling.html>task scheduling</a> that executes operations in parallel. The <a href=http://dask.pydata.org/en/latest/spec.html>low-level details</a> aren&rsquo;t too important; all we care about is that</p><ol><li>Dask works with <em>task graphs</em> (<em>tasks</em>: functions to call on data, and <em>graphs</em>: the relationships between tasks).</li><li>This is a flexible and performant way to parallelize many different kinds of problems.</li></ol><p>To understand point 1, let&rsquo;s examine the difference between a Dask DataFrame and a pandas DataFrame. When we read in <code>df</code> with <code>dd.read_parquet</code>, we received a Dask DataFrame.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df
</span></span></code></pre></div><div class=output><div><strong>Dask DataFrame Structure:</strong></div><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table class=dataframe><thead><tr style=text-align:right><th></th><th>occupation</th></tr><tr><th>npartitions=35</th><th></th></tr></thead><tbody><tr><th></th><td>object</td></tr><tr><th></th><td>...</td></tr><tr><th>...</th><td>...</td></tr><tr><th></th><td>...</td></tr><tr><th></th><td>...</td></tr></tbody></table></div><div>Dask Name: read-parquet, 35 tasks</div></div><p>A Dask DataFrame consists of many pandas DataFrames arranged by the index. Dask is really just coordinating these pandas DataFrames.</p><img src=http://dask.pydata.org/en/latest/_images/dask-dataframe.svg width=50%><p>All the actual computation (reading from disk, computing the value counts, etc.) eventually use pandas internally. If I do <code>df.occupation.str.len</code>, Dask will coordinate calling <code>pandas.Series.str.len</code> on each of the pandas DataFrames.</p><p>Those reading carefully will notice a problem with the statement &ldquo;A Dask DataFrame consists of many pandas DataFrames&rdquo;. Our initial problem was that we didn&rsquo;t have enough memory for those DataFrames! How can Dask be coordinating DataFrames if there isn&rsquo;t enough memory? This brings us to the second major difference: Dask DataFrames (and arrays) are lazy. Operations on them don&rsquo;t execute and produce the final result immediately. Rather, calling methods on them builds up a task graph.</p><p>We can visualize task graphs using <code>graphviz</code>. For the blog, I&rsquo;ve trimmed down the example to be a subset of the entire graph.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df<span style=color:#f92672>.</span>visualize(rankdir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;LR&#39;</span>)
</span></span></code></pre></div><p><img loading=lazy src=/images/scalable-read-simple.svg alt></p><p><code>df</code> (the dask DataFrame consisting of many pandas DataFrames) has a task graph with 5 calls to a parquet reader (one for each file), each of which produces a DataFrame when called.</p><p>Calling additional methods on <code>df</code> adds additional tasks to this graph. For example, our <code>most_common</code> Series has three additional calls</p><ul><li>Select the <code>occupation</code> column (<code>__getitem__</code>)</li><li>Perform the value counts</li><li>Select the 100 largest values</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>most_common <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>occupation<span style=color:#f92672>.</span>value_counts()<span style=color:#f92672>.</span>nlargest(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>most_common
</span></span></code></pre></div><pre>

    Dask Series Structure:
    npartitions=1
        int64
          ...
    Name: occupation, dtype: int64
    Dask Name: series-nlargest-agg, 113 tasks
</pre><p>Which we can visualize.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>most_common<span style=color:#f92672>.</span>visualize(rankdir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;LR&#39;</span>)
</span></span></code></pre></div><p><img loading=lazy src=/images/scalable-most-common.svg alt></p><p>So <code>most_common</code> doesn&rsquo;t hold the actual answer yet. Instead, it holds a recipe for the answer; a list of all the steps to take to get the concrete result. One way to ask for the result is with the <code>compute</code> method.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>most_common<span style=color:#f92672>.</span>compute()
</span></span></code></pre></div><pre>
    RETIRED                    4769520
    NOT EMPLOYED               2656988
    ATTORNEY                   1340434
    PHYSICIAN                   659082
    HOMEMAKER                   494187
                                ...   
    CHIEF EXECUTIVE OFFICER      26551
    SURGEON                      25521
    EDITOR                       25457
    OPERATOR                     25151
    ORTHOPAEDIC SURGEON          24384
    Name: occupation, Length: 100, dtype: int64
</pre><p>At this point, the task graph is handed to a <a href=https://dask.pydata.org/en/latest/scheduling.html>scheduler</a>, which is responsible for executing a task graph. Schedulers can analyze a task graph and find sections that can run <em>in parallel</em>. (Dask includes several schedulers. See <a href=http://dask.pydata.org/en/latest/scheduling.html>the scheduling documentation</a> for how to choose, though Dask has good defaults.)</p><p>So that&rsquo;s a high-level tour of how Dask works:</p><p><img loading=lazy src=http://dask.pydata.org/en/latest/_images/collections-schedulers.png alt="collections, schedulers"></p><ol><li>Various collections collections like <code>dask.dataframe</code> and <code>dask.array</code>
provide users familiar APIs for working with large datasets.</li><li>Computations are represented as a task graph. These graphs could be built by
hand, or more commonly built by one of the collections.</li><li>Dask schedulers run task graphs in parallel (potentially distributed across
a cluster), reusing libraries like NumPy and pandas to do the computations.</li></ol><p>Let&rsquo;s finish off this post by continuing to explore the FEC dataset with Dask. At this point, we&rsquo;ll use the distributed scheduler for it&rsquo;s nice diagnostics.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> dask.dataframe <span style=color:#66d9ef>as</span> dd
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dask <span style=color:#f92672>import</span> compute
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dask.distributed <span style=color:#f92672>import</span> Client
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> Client(processes<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><p>Calling <code>Client</code> without providing a scheduler address will make a local &ldquo;cluster&rdquo; of threads or processes on your machine. There are <a href=http://dask.pydata.org/en/latest/setup.html>many ways</a> to deploy a Dask cluster onto an actual cluster of machines, though we&rsquo;re particularly fond of <a href=http://dask.pydata.org/en/latest/setup/kubernetes.html>Kubernetes</a>. This highlights one of my favorite features of Dask: it scales down to use a handful of threads on a laptop <em>or</em> up to a cluster with thousands of nodes. Dask can comfortably handle medium-sized datasets (dozens of GBs, so larger than RAM) on a laptop. Or it can scale up to very large datasets with a cluster.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>individual_cols <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;cmte_id&#39;</span>, <span style=color:#e6db74>&#39;entity_tp&#39;</span>, <span style=color:#e6db74>&#39;employer&#39;</span>, <span style=color:#e6db74>&#39;occupation&#39;</span>,
</span></span><span style=display:flex><span>                   <span style=color:#e6db74>&#39;transaction_dt&#39;</span>, <span style=color:#e6db74>&#39;transaction_amt&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>indiv <span style=color:#f92672>=</span> dd<span style=color:#f92672>.</span>read_parquet(<span style=color:#e6db74>&#39;data/indiv-*.parq&#39;</span>,
</span></span><span style=display:flex><span>                        columns<span style=color:#f92672>=</span>individual_cols,
</span></span><span style=display:flex><span>                        engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pyarrow&#34;</span>)
</span></span><span style=display:flex><span>indiv
</span></span></code></pre></div><div class=output><div><strong>Dask DataFrame Structure:</strong></div><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table class=dataframe><thead><tr style=text-align:right><th></th><th>cmte_id</th><th>entity_tp</th><th>employer</th><th>occupation</th><th>transaction_dt</th><th>transaction_amt</th></tr><tr><th>npartitions=5</th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th></th><td>object</td><td>object</td><td>object</td><td>object</td><td>datetime64[ns]</td><td>int64</td></tr><tr><th></th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th></th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th></th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr></tbody></table></div><div>Dask Name: read-parquet, 5 tasks</div></div><p>We can compute summary statistics like the average mean and standard deviation of the transaction amount:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>avg_transaction <span style=color:#f92672>=</span> indiv<span style=color:#f92672>.</span>transaction_amt<span style=color:#f92672>.</span>mean()
</span></span></code></pre></div><p>We can answer questions like &ldquo;Which employer&rsquo;s employees donated the most?&rdquo;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>total_by_employee <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    indiv<span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#39;employer&#39;</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>transaction_amt<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>nlargest(<span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Or &ldquo;what is the average amount donated per occupation?&rdquo;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>avg_by_occupation <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    indiv<span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#34;occupation&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>transaction_amt<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>nlargest(<span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Since Dask is lazy, we haven&rsquo;t actually computed anything.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>total_by_employee
</span></span></code></pre></div><pre>
    Dask Series Structure:
    npartitions=1
        int64
          ...
    Name: transaction_amt, dtype: int64
    Dask Name: series-nlargest-agg, 13 tasks
</pre><p><code>avg_transaction</code>, <code>avg_by_occupation</code> and <code>total_by_employee</code> are three separate computations (they have different task graphs), but we know they share some structure: they&rsquo;re all reading in the same data, they might select the same subset of columns, and so on. Dask is able to avoid redundant computation when you use the top-level <code>dask.compute</code> function.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span>avg_transaction, by_employee, by_occupation <span style=color:#f92672>=</span> compute(
</span></span><span style=display:flex><span>    avg_transaction, total_by_employee, avg_by_occupation
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><pre>
    CPU times: user 57.5 s, sys: 14.4 s, total: 1min 11s
    Wall time: 54.9 s
</pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>avg_transaction
</span></span></code></pre></div><pre>
    566.0899206077507
</pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>by_employee
</span></span></code></pre></div><pre>
    employer
    RETIRED                1019973117
    SELF-EMPLOYED           834547641
    SELF                    537402882
    SELF EMPLOYED           447363032
    NONE                    418011322
    HOMEMAKER               355195126
    NOT EMPLOYED            345770418
    FAHR, LLC               166679844
    CANDIDATE                75186830
    ADELSON DRUG CLINIC      53358500
    Name: transaction_amt, dtype: int64
</pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>by_occupation
</span></span></code></pre></div><pre>
    occupation
    CHAIRMAN CEO & FOUNDER                   1,023,333.33
    PAULSON AND CO., INC.                    1,000,000.00
    CO-FOUNDING DIRECTOR                       875,000.00
    CHAIRMAN/CHIEF TECHNOLOGY OFFICER          750,350.00
    CO-FOUNDER, DIRECTOR, CHIEF INFORMATIO     675,000.00
    CO-FOUNDER, DIRECTOR                       550,933.33
    MOORE CAPITAL GROUP, LP                    500,000.00
    PERRY HOMES                                500,000.00
    OWNER, FOUNDER AND CEO                     500,000.00
    CHIEF EXECUTIVE OFFICER/PRODUCER           500,000.00
    Name: transaction_amt, dtype: float64
</pre><p>Things like filtering work well. Let&rsquo;s find the 10 most common occupations and filter the dataset down to just those.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>top_occupations <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    indiv<span style=color:#f92672>.</span>occupation<span style=color:#f92672>.</span>value_counts()
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>nlargest(<span style=color:#ae81ff>10</span>)<span style=color:#f92672>.</span>index
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>compute()
</span></span><span style=display:flex><span>top_occupations
</span></span></code></pre></div><pre>
    Index(['RETIRED', 'NOT EMPLOYED', 'ATTORNEY', 'PHYSICIAN', 'HOMEMAKER',
           'PRESIDENT', 'PROFESSOR', 'CONSULTANT', 'EXECUTIVE', 'ENGINEER'],
          dtype='object')
</pre><p>We&rsquo;ll filter the raw records down to just the ones from those occupations. Then we&rsquo;ll compute a few summary statistics on the transaction amounts for each group.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>donations <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    indiv[indiv<span style=color:#f92672>.</span>occupation<span style=color:#f92672>.</span>isin(top_occupations)]
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#34;occupation&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>transaction_amt
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>agg([<span style=color:#e6db74>&#39;count&#39;</span>, <span style=color:#e6db74>&#39;mean&#39;</span>, <span style=color:#e6db74>&#39;sum&#39;</span>, <span style=color:#e6db74>&#39;max&#39;</span>])
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>total_avg, occupation_avg <span style=color:#f92672>=</span> compute(indiv<span style=color:#f92672>.</span>transaction_amt<span style=color:#f92672>.</span>mean(),
</span></span><span style=display:flex><span>                                    donations[<span style=color:#e6db74>&#39;mean&#39;</span>])
</span></span></code></pre></div><p>These are small, concrete results so we can turn to familiar tools like matplotlib to visualize the result.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ax <span style=color:#f92672>=</span> occupation_avg<span style=color:#f92672>.</span>sort_values(ascending<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)<span style=color:#f92672>.</span>plot<span style=color:#f92672>.</span>barh(color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;k&#39;</span>, width<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>);
</span></span><span style=display:flex><span>lim <span style=color:#f92672>=</span> ax<span style=color:#f92672>.</span>get_ylim()
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>vlines(total_avg, <span style=color:#f92672>*</span>lim, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;C1&#39;</span>, linewidth<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>legend([<span style=color:#e6db74>&#39;Average donation&#39;</span>])
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>set(xlabel<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Donation Amount&#34;</span>, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Average Dontation by Occupation&#34;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span></code></pre></div><p><img loading=lazy src=/images/modern-pandas-08_49_0.png alt=png></p><p>Dask inherits all of pandas&rsquo; great time-series support. We can get the total amount donated per day using a <a href=https://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling><code>resample</code></a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>daily <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    indiv[[<span style=color:#e6db74>&#39;transaction_dt&#39;</span>, <span style=color:#e6db74>&#39;transaction_amt&#39;</span>]]<span style=color:#f92672>.</span>dropna()
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>set_index(<span style=color:#e6db74>&#39;transaction_dt&#39;</span>)[<span style=color:#e6db74>&#39;transaction_amt&#39;</span>]
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>resample(<span style=color:#e6db74>&#34;D&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>compute()
</span></span><span style=display:flex><span>daily
</span></span></code></pre></div><pre>
    1916-01-23    1000
    1916-01-24       0
    1916-01-25       0
    1916-01-26       0
    1916-01-27       0
                  ... 
    2201-05-29       0
    2201-05-30       0
    2201-05-31       0
    2201-06-01       0
    2201-06-02    2000
    Name: transaction_amt, Length: 104226, dtype: int64
</pre><p>It seems like we have some bad data. This should just be 2007-2016. We&rsquo;ll filter it down to the real subset before plotting.
Notice that the seamless transition from <code>dask.dataframe</code> operations above, to pandas operations below.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>subset <span style=color:#f92672>=</span> daily<span style=color:#f92672>.</span>loc[<span style=color:#e6db74>&#39;2011&#39;</span>:<span style=color:#e6db74>&#39;2016&#39;</span>]
</span></span><span style=display:flex><span>ax <span style=color:#f92672>=</span> subset<span style=color:#f92672>.</span>div(<span style=color:#ae81ff>1000</span>)<span style=color:#f92672>.</span>plot(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>set(ylim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Daily Donations&#34;</span>, ylabel<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;$ (thousands)&#34;</span>,)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine();
</span></span></code></pre></div><p><img loading=lazy src=/images/modern-pandas-08_54_0.png alt=png></p><h2 id=joining>Joining<a hidden class=anchor aria-hidden=true href=#joining>#</a></h2><p>Like pandas, Dask supports joining together multiple datasets.</p><p>Individual donations are made to <em>committees</em>. Committees are what make the actual expenditures (buying a TV ad).
Some committees are directly tied to a candidate (this are campaign committees). Other committees are tied to a group (like the Republican National Committee). Either may be tied to a party.</p><p>Let&rsquo;s read in the committees. The total number of committees is small, so we&rsquo;ll <code>.compute</code> immediately to get a pandas DataFrame (the reads still happen in parallel!).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>committee_cols <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;cmte_id&#39;</span>, <span style=color:#e6db74>&#39;cmte_nm&#39;</span>, <span style=color:#e6db74>&#39;cmte_tp&#39;</span>, <span style=color:#e6db74>&#39;cmte_pty_affiliation&#39;</span>]
</span></span><span style=display:flex><span>cm <span style=color:#f92672>=</span> dd<span style=color:#f92672>.</span>read_parquet(<span style=color:#e6db74>&#34;data/cm-*.parq&#34;</span>,
</span></span><span style=display:flex><span>                     columns<span style=color:#f92672>=</span>committee_cols)<span style=color:#f92672>.</span>compute()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Some committees change thier name, but the ID stays the same</span>
</span></span><span style=display:flex><span>cm <span style=color:#f92672>=</span> cm<span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#39;cmte_id&#39;</span>)<span style=color:#f92672>.</span>last()
</span></span><span style=display:flex><span>cm
</span></span></code></pre></div><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table class=dataframe><thead><tr style=text-align:right><th></th><th>cmte_nm</th><th>cmte_tp</th><th>cmte_pty_affiliation</th></tr><tr><th>cmte_id</th><th></th><th></th><th></th></tr></thead><tbody><tr><th>C00000042</th><td>ILLINOIS TOOL WORKS INC. FOR BETTER GOVERNMENT...</td><td>Q</td><td>NaN</td></tr><tr><th>C00000059</th><td>HALLMARK CARDS PAC</td><td>Q</td><td>UNK</td></tr><tr><th>C00000422</th><td>AMERICAN MEDICAL ASSOCIATION POLITICAL ACTION ...</td><td>Q</td><td>NaN</td></tr><tr><th>C00000489</th><td>D R I V E POLITICAL FUND CHAPTER 886</td><td>N</td><td>NaN</td></tr><tr><th>C00000547</th><td>KANSAS MEDICAL SOCIETY POLITICAL ACTION COMMITTEE</td><td>Q</td><td>UNK</td></tr><tr><th>...</th><td>...</td><td>...</td><td>...</td></tr><tr><th>C90017237</th><td>ORGANIZE NOW</td><td>I</td><td>NaN</td></tr><tr><th>C90017245</th><td>FRANCISCO AGUILAR</td><td>I</td><td>NaN</td></tr><tr><th>C90017336</th><td>LUDWIG, EUGENE</td><td>I</td><td>NaN</td></tr><tr><th>C99002396</th><td>AMERICAN POLITICAL ACTION COMMITTEE</td><td>Q</td><td>NaN</td></tr><tr><th>C99003428</th><td>THIRD DISTRICT REPUBLICAN PARTY</td><td>Y</td><td>REP</td></tr></tbody></table><p>28612 rows × 3 columns</p></div><p>We&rsquo;ll use <code>dd.merge</code>, which is analogous to <code>pd.merge</code> for joining a Dask <code>DataFrame</code> with a pandas or Dask <code>DataFrame</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>indiv <span style=color:#f92672>=</span> indiv[(indiv<span style=color:#f92672>.</span>transaction_dt <span style=color:#f92672>&gt;=</span> pd<span style=color:#f92672>.</span>Timestamp(<span style=color:#e6db74>&#34;2007-01-01&#34;</span>)) <span style=color:#f92672>&amp;</span>
</span></span><span style=display:flex><span>              (indiv<span style=color:#f92672>.</span>transaction_dt <span style=color:#f92672>&lt;=</span> pd<span style=color:#f92672>.</span>Timestamp(<span style=color:#e6db74>&#34;2018-01-01&#34;</span>))]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df2 <span style=color:#f92672>=</span> dd<span style=color:#f92672>.</span>merge(indiv, cm<span style=color:#f92672>.</span>reset_index(), on<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cmte_id&#39;</span>)
</span></span><span style=display:flex><span>df2
</span></span></code></pre></div><div><strong>Dask DataFrame Structure:</strong></div><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table class=dataframe><thead><tr style=text-align:right><th></th><th>cmte_id</th><th>entity_tp</th><th>employer</th><th>occupation</th><th>transaction_dt</th><th>transaction_amt</th><th>cmte_nm</th><th>cmte_tp</th><th>cmte_pty_affiliation</th></tr><tr><th>npartitions=20</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th></th><td>object</td><td>object</td><td>object</td><td>object</td><td>datetime64[ns]</td><td>int64</td><td>object</td><td>object</td><td>object</td></tr><tr><th></th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th></th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th></th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr></tbody></table></div><div>Dask Name: merge, 141 tasks</div><p>Now we can find which party raised more over the course of each election. We&rsquo;ll group by the day and party and sum the transaction amounts.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>indiv <span style=color:#f92672>=</span> indiv<span style=color:#f92672>.</span>repartition(npartitions<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>df2 <span style=color:#f92672>=</span> dd<span style=color:#f92672>.</span>merge(indiv, cm<span style=color:#f92672>.</span>reset_index(), on<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cmte_id&#39;</span>)
</span></span><span style=display:flex><span>df2
</span></span></code></pre></div><div><strong>Dask DataFrame Structure:</strong></div><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table class=dataframe><thead><tr style=text-align:right><th></th><th>cmte_id</th><th>entity_tp</th><th>employer</th><th>occupation</th><th>transaction_dt</th><th>transaction_amt</th><th>cmte_nm</th><th>cmte_tp</th><th>cmte_pty_affiliation</th></tr><tr><th>npartitions=10</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th></th><td>object</td><td>object</td><td>object</td><td>object</td><td>datetime64[ns]</td><td>int64</td><td>object</td><td>object</td><td>object</td></tr><tr><th></th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th></th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th></th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr></tbody></table></div><div>Dask Name: merge, 141 tasks</div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>party_donations <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    df2<span style=color:#f92672>.</span>groupby([df2<span style=color:#f92672>.</span>transaction_dt, <span style=color:#e6db74>&#39;cmte_pty_affiliation&#39;</span>])
</span></span><span style=display:flex><span>       <span style=color:#f92672>.</span>transaction_amt<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>compute()<span style=color:#f92672>.</span>sort_index()
</span></span></code></pre></div><p>We&rsquo;ll filter that down to just Republican and Democrats and plot.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ax <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    party_donations<span style=color:#f92672>.</span>loc[:, [<span style=color:#e6db74>&#39;REP&#39;</span>, <span style=color:#e6db74>&#39;DEM&#39;</span>]]
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>unstack(<span style=color:#e6db74>&#34;cmte_pty_affiliation&#34;</span>)<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>1</span>:<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>        <span style=color:#f92672>.</span>rolling(<span style=color:#e6db74>&#39;30D&#39;</span>)<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>plot(color<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;C0&#39;</span>, <span style=color:#e6db74>&#39;C3&#39;</span>], figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>6</span>),
</span></span><span style=display:flex><span>                                    linewidth<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>set(title<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Daily Donations (30-D Moving Average)&#34;</span>, xlabel<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Date&#34;</span>);
</span></span></code></pre></div><p><img loading=lazy src=/images/modern-pandas-08_64_0.png alt=png></p><h2 id=try-it-out>Try It Out!<a hidden class=anchor aria-hidden=true href=#try-it-out>#</a></h2><p>So that&rsquo;s a taste of Dask. Next time you hit a scaling problem with pandas (or NumPy, scikit-learn, or your custom code), feel free to</p><pre tabindex=0><code>pip install dask[complete]
</code></pre><p>or</p><pre tabindex=0><code>conda install dask
</code></pre><p>The <a href=http://dask.pydata.org/en/latest/>dask homepage</a> has links to all the relevant documentation, and <a href="https://mybinder.org/v2/gh/dask/dask-examples/master?filepath=dataframe.ipynb">binder notebooks</a> where you can try out Dask before installing.</p><p>As always, reach out to me on <a href=https://twitter.com/TomAugspurger>Twitter</a> or in the comments if you have anything to share.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tomaugspurger.net/tags/pandas/>Pandas</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://tomaugspurger.net/>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>