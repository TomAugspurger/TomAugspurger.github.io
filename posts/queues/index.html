<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Queues in the News | Tom's Blog</title>
<meta name=keywords content><meta name=description content="I came across a couple of new (to me) uses of queues recently. When I came up with the title to this article I knew I had to write them up together.
Queues in Dask
Over at the Coiled Blog, Gabe Joseph has a nice post summarizing a huge amount of effort addressing a problem that&rsquo;s been vexing demanding Dask users for years. The main symptom of the problem was unexpectedly high memory usage on workers, leading to crashing workers (which in turn caused even more network communication, and so more memory usage, and more crashing workers). This is actually a problem I worked on a bit back in 2019, and I made very little progress."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/queues/><link crossorigin=anonymous href=/assets/css/stylesheet.ced21e6d3497ee93fed8f8b357448095840179bd510b5ea0e6013078712e6dd1.css integrity="sha256-ztIebTSX7pP+2PizV0SAlYQBeb1RC16g5gEweHEubdE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tomaugspurger.net/posts/queues/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Queues in the News"><meta property="og:description" content="I came across a couple of new (to me) uses of queues recently. When I came up with the title to this article I knew I had to write them up together.
Queues in Dask
Over at the Coiled Blog, Gabe Joseph has a nice post summarizing a huge amount of effort addressing a problem that&rsquo;s been vexing demanding Dask users for years. The main symptom of the problem was unexpectedly high memory usage on workers, leading to crashing workers (which in turn caused even more network communication, and so more memory usage, and more crashing workers). This is actually a problem I worked on a bit back in 2019, and I made very little progress."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/queues/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-26T13:35:24-06:00"><meta property="article:modified_time" content="2022-12-26T13:35:24-06:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Queues in the News"><meta name=twitter:description content="I came across a couple of new (to me) uses of queues recently. When I came up with the title to this article I knew I had to write them up together.
Queues in Dask
Over at the Coiled Blog, Gabe Joseph has a nice post summarizing a huge amount of effort addressing a problem that&rsquo;s been vexing demanding Dask users for years. The main symptom of the problem was unexpectedly high memory usage on workers, leading to crashing workers (which in turn caused even more network communication, and so more memory usage, and more crashing workers). This is actually a problem I worked on a bit back in 2019, and I made very little progress."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":2,"name":"Queues in the News","item":"https://tomaugspurger.net/posts/queues/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Queues in the News","name":"Queues in the News","description":"I came across a couple of new (to me) uses of queues recently. When I came up with the title to this article I knew I had to write them up together.\nQueues in Dask Over at the Coiled Blog, Gabe Joseph has a nice post summarizing a huge amount of effort addressing a problem that\u0026rsquo;s been vexing demanding Dask users for years. The main symptom of the problem was unexpectedly high memory usage on workers, leading to crashing workers (which in turn caused even more network communication, and so more memory usage, and more crashing workers). This is actually a problem I worked on a bit back in 2019, and I made very little progress.\n","keywords":[],"articleBody":"I came across a couple of new (to me) uses of queues recently. When I came up with the title to this article I knew I had to write them up together.\nQueues in Dask Over at the Coiled Blog, Gabe Joseph has a nice post summarizing a huge amount of effort addressing a problem that’s been vexing demanding Dask users for years. The main symptom of the problem was unexpectedly high memory usage on workers, leading to crashing workers (which in turn caused even more network communication, and so more memory usage, and more crashing workers). This is actually a problem I worked on a bit back in 2019, and I made very little progress.\nA common source of this problem was having many (mostly) independent “chains” of computation. Dask would start on too many of the “root” tasks simultaneously, before finishing up some of the chains. The root tasks are typically memory increasing (e.g. load data from file system) while the later tasks are typically memory decreasing (take the mean of a large array).\nIn dask/distributed, Dask actually has two places where it determines which order to run things in. First, there’s a “static” ordering (implemented in dask/order.py, which has some truly great docstrings, check out the source.) Dask was actually doing really well here. Consider this task graph from the issue:\nThe “root” tasks are on the left (marked 0, 3, 11, 14). Dask’s typical depth-first algorithm works well here: we execute the first two root tasks (0 and 3) to finish up the first “chain” of computation (the box (0, 0) on the right) before moving onto the other two root nodes, 11 and 14.\nThe second time Dask (specifically, the distributed scheduler) considers what order to run things is at runtime. It gets this “static” ordering from dask.order which says what order you should run things in, but the distributed runtime has way more information available to it that it can use to influence its scheduling decisions. In this case, the distributed scheduler looked around and saw that it had some idle cores. It thought “hey, I have a bunch of these root tasks ready to run”, and scheduled those. Those tend to increase memory usage, leading to our memory problems.\nThe solution was a queue. From Gabe’s blog post:\nWe’re calling this mode of scheduling “queuing”, or “root task withholding”. The scheduler puts data-loading tasks in an internal queue, and only drips one out to a worker once it’s finished its current work and there’s nothing more useful to run instead that utilizes the work it just completed.\nQueue for Data Pipelines At work, we’re taking on more responsibility for the data pipeline responsible for getting various datasets to Azure Blob Storage. I’m dipping my toes into the whole “event-driven” architecture thing, and have become paranoid about dropping work. The Azure Architecture Center has a bunch of useful articles here. This article gives some names to some of the concepts I was bumbling through (e.g. “at least once processing”).\nIn our case, we’re using Azure Queue Storage as a simple way to reliably parallelize work across some machines. We somehow discover some assets to be copied (perhaps by querying an API on a schedule, or by listening to some events on a webhook), store those as messages on the queue.\nThen our workers can start processing those messages from the queue in parallel. The really neat thing about Azure’s Storage Queues (and, I gather, many queue systems) is the concept of “locking” a message. When the worker is ready, it receives a message from the queue and begins processing it. To prevent dropping messages (if, e.g. the worker dies mid-processing) the message isn’t actually deleted until the worker tells the queue service “OK, I’m doing processing this message”. If for whatever reason the worker doesn’t phone home saying it’s processed the message, the message reappears on the queue for some other worker to process.\nThe Azure SDK for Python actually does a really good job integrating language features into the clients for these services. In this case, we can just treat the Queue service as an iterator.\n\u003e\u003e\u003e queue_client = azure.storage.blob.QueueClient(\"https://queue-endpoint.queue.core.windows.net/queue-name\") \u003e\u003e\u003e for message in queue_client.receive_messages(): ... yield message ... # The caller finishes processing the message. ... queue_client.delete_message(message) I briefly went down a dead-end solution that added a “processing” state to our state database. Workers were responsible for updating the items state to “processing” as soon as they started, and “copied” or “failed” when they finished. But I quickly ran into issues where items were marked as “processing” but weren’t actually. Maybe the node was preempted; maybe (just maybe) there was a bug in my code. But for whatever reason I couldn’t trust the item’s state anymore. Queues were an elegant way to ensure that we processed these messages at least once, and now I can sleep comfortably at night knowing that we aren’t dropping messages on the floor.\n","wordCount":"826","inLanguage":"en","datePublished":"2022-12-26T13:35:24-06:00","dateModified":"2022-12-26T13:35:24-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/queues/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net/ accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Queues in the News</h1><div class=post-meta><span title='2022-12-26 13:35:24 -0600 -0600'>December 26, 2022</span></div></header><div class=post-content><p>I came across a couple of new (to me) uses of queues recently. When I came up with the title to this article I knew I had to write them up together.</p><h2 id=queues-in-dask>Queues in Dask<a hidden class=anchor aria-hidden=true href=#queues-in-dask>#</a></h2><p>Over at the <a href=https://www.coiled.io/blog/reducing-dask-memory-usage>Coiled Blog</a>, Gabe Joseph has a nice post summarizing a huge amount of effort addressing a problem that&rsquo;s been vexing demanding Dask users for years. The main symptom of the problem was unexpectedly high memory usage on workers, leading to crashing workers (which in turn caused even more network communication, and so more memory usage, and more crashing workers). This is actually a problem I worked on a bit back in 2019, and I made very little progress.</p><p>A common source of this problem was having many (mostly) independent &ldquo;chains&rdquo; of computation. Dask would start on too many of the &ldquo;root&rdquo; tasks simultaneously, before finishing up some of the chains. The root tasks are typically memory increasing (e.g. load data from file system) while the later tasks are typically memory decreasing (take the mean of a large array).</p><p>In <code>dask/distributed</code>, Dask actually has two places where it determines which order to run things in. First, there&rsquo;s a &ldquo;static&rdquo; ordering (implemented in <code>dask/order.py</code>, which has some truly great docstrings, check out <a href=https://github.com/dask/dask/blob/main/dask/order.py>the source</a>.) Dask was actually doing really well here. Consider this task graph from <a href=https://github.com/dask/distributed/issues/2602#issuecomment-496634172>the issue</a>:</p><p><img loading=lazy src=https://user-images.githubusercontent.com/1312546/58502338-f2599c00-814b-11e9-989a-5bfd2c3785a8.png alt></p><p>The &ldquo;root&rdquo; tasks are on the left (marked 0, 3, 11, 14). Dask&rsquo;s typical depth-first algorithm works well here: we execute the first two root tasks (0 and 3) to finish up the first &ldquo;chain&rdquo; of computation (the box <code>(0, 0)</code> on the right) before moving onto the other two root nodes, 11 and 14.</p><p>The second time Dask (specifically, the distributed scheduler) considers what order to run things is at runtime. It gets this &ldquo;static&rdquo; ordering from <code>dask.order</code> which says what order you <em>should</em> run things in, but the distributed runtime has <em>way</em> more information available to it that it can use to influence its scheduling decisions. In this case, the distributed scheduler looked around and saw that it had some idle cores. It thought &ldquo;hey, I have a bunch of these root tasks ready to run&rdquo;, and scheduled those. Those tend to increase memory usage, leading to our memory problems.</p><p>The solution was a queue. From <a href=https://www.coiled.io/blog/reducing-dask-memory-usage>Gabe&rsquo;s blog post</a>:</p><blockquote><p>We&rsquo;re calling this mode of scheduling <a href=https://distributed.dask.org/en/stable/scheduling-policies.html#queuing>&ldquo;queuing&rdquo;</a>, or &ldquo;root task withholding&rdquo;. The scheduler puts data-loading tasks in an internal queue, and only drips one out to a worker once it&rsquo;s finished its current work and there&rsquo;s nothing more useful to run instead that utilizes the work it just completed.</p></blockquote><h2 id=queue-for-data-pipelines>Queue for Data Pipelines<a hidden class=anchor aria-hidden=true href=#queue-for-data-pipelines>#</a></h2><p><a href=http://planetarycomputer.microsoft.com/>At work</a>, we&rsquo;re taking on more responsibility for the data pipeline responsible for getting various datasets to Azure Blob Storage. I&rsquo;m dipping my toes into the whole &ldquo;event-driven&rdquo; architecture thing, and have become <em>paranoid</em> about dropping work. The <a href=https://learn.microsoft.com/en-us/azure/architecture/>Azure Architecture Center</a> has a bunch of useful articles here. <a href=https://learn.microsoft.com/en-us/azure/architecture/patterns/competing-consumers>This article</a> gives some names to some of the concepts I was bumbling through (e.g. &ldquo;at least once processing&rdquo;).</p><p>In our case, we&rsquo;re using <a href=https://learn.microsoft.com/en-us/azure/storage/queues/storage-queues-introduction>Azure Queue Storage</a> as a simple way to <em>reliably</em> parallelize work across some machines. We somehow discover some assets to be copied (perhaps by querying an API on a schedule, or by listening to some events on a webhook), store those as messages on the queue.</p><p>Then our workers can start processing those messages from the queue in parallel. The really neat thing about Azure&rsquo;s Storage Queues (and, I gather, many queue systems) is the concept of &ldquo;locking&rdquo; a message. When the worker is ready, it receives a message from the queue and begins processing it. To prevent dropping messages (if, e.g. the worker dies mid-processing) the message isn&rsquo;t actually deleted until the worker tells the queue service &ldquo;OK, I&rsquo;m doing processing this message&rdquo;. If for whatever reason the worker doesn&rsquo;t phone home saying it&rsquo;s processed the message, the message reappears on the queue for some other worker to process.</p><p>The <a href=https://learn.microsoft.com/en-us/azure/developer/python/sdk/azure-sdk-overview>Azure SDK for Python</a> actually does a really good job integrating language features into the clients for these services. In this case, we can just treat the Queue service as an iterator.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> queue_client <span style=color:#f92672>=</span> azure<span style=color:#f92672>.</span>storage<span style=color:#f92672>.</span>blob<span style=color:#f92672>.</span>QueueClient(<span style=color:#e6db74>&#34;https://queue-endpoint.queue.core.windows.net/queue-name&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#66d9ef>for</span> message <span style=color:#f92672>in</span> queue_client<span style=color:#f92672>.</span>receive_messages():
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>    <span style=color:#66d9ef>yield</span> message
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>    <span style=color:#75715e># The caller finishes processing the message.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>    queue_client<span style=color:#f92672>.</span>delete_message(message)
</span></span></code></pre></div><p>I briefly went down a dead-end solution that added a &ldquo;processing&rdquo; state to our state database. Workers were responsible for updating the items state to &ldquo;processing&rdquo; as soon as they started, and &ldquo;copied&rdquo; or &ldquo;failed&rdquo; when they finished. But I quickly ran into issues where items were marked as &ldquo;processing&rdquo; but weren&rsquo;t actually. Maybe the node was preempted; maybe (just maybe) there was a bug in my code. But for whatever reason I couldn&rsquo;t trust the item&rsquo;s state anymore. Queues were an elegant way to ensure that we processed these messages at least once, and now I can sleep comfortably at night knowing that we aren&rsquo;t dropping messages on the floor.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://tomaugspurger.net/>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>