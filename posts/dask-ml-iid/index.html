<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Rewriting scikit-learn for big data, in under 9 hours. | Tom's Blog</title><meta name=keywords content><meta name=description content="This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I&rsquo;m thankful to them for hosting me and Anaconda for sending me there.
Towards the end of our week, Gael threw out the observation that for many applications, you don&rsquo;t need to train on the entire dataset, a sample is often sufficient. But it&rsquo;d be nice if the trained estimator would be able to transform and predict for dask arrays, getting all the nice distributed parallelism and memory management dask brings."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/dask-ml-iid/><link crossorigin=anonymous href=/assets/css/stylesheet.3690c96d8a707265a16abd3b389bb33e4e3916869c3142cba43a3cfaaed4b5f9.css integrity="sha256-NpDJbYpwcmWhar07OJuzPk45FoacMULLpDo8+q7Utfk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Rewriting scikit-learn for big data, in under 9 hours."><meta property="og:description" content="This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I&rsquo;m thankful to them for hosting me and Anaconda for sending me there.
Towards the end of our week, Gael threw out the observation that for many applications, you don&rsquo;t need to train on the entire dataset, a sample is often sufficient. But it&rsquo;d be nice if the trained estimator would be able to transform and predict for dask arrays, getting all the nice distributed parallelism and memory management dask brings."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/dask-ml-iid/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-01-28T00:00:00+00:00"><meta property="article:modified_time" content="2018-01-28T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Rewriting scikit-learn for big data, in under 9 hours."><meta name=twitter:description content="This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I&rsquo;m thankful to them for hosting me and Anaconda for sending me there.
Towards the end of our week, Gael threw out the observation that for many applications, you don&rsquo;t need to train on the entire dataset, a sample is often sufficient. But it&rsquo;d be nice if the trained estimator would be able to transform and predict for dask arrays, getting all the nice distributed parallelism and memory management dask brings."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":3,"name":"Rewriting scikit-learn for big data, in under 9 hours.","item":"https://tomaugspurger.net/posts/dask-ml-iid/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Rewriting scikit-learn for big data, in under 9 hours.","name":"Rewriting scikit-learn for big data, in under 9 hours.","description":"This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I\u0026rsquo;m thankful to them for hosting me and Anaconda for sending me there.\nTowards the end of our week, Gael threw out the observation that for many applications, you don\u0026rsquo;t need to train on the entire dataset, a sample is often sufficient. But it\u0026rsquo;d be nice if the trained estimator would be able to transform and predict for dask arrays, getting all the nice distributed parallelism and memory management dask brings.","keywords":[],"articleBody":"This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I’m thankful to them for hosting me and Anaconda for sending me there.\nTowards the end of our week, Gael threw out the observation that for many applications, you don’t need to train on the entire dataset, a sample is often sufficient. But it’d be nice if the trained estimator would be able to transform and predict for dask arrays, getting all the nice distributed parallelism and memory management dask brings.\nThis intrigued me, and I had a 9 hour plane ride, so…\ndask_ml.iid I put together the dask_ml.iid sub-package. The estimators contained within are appropriate for data that are independently and identically distributed (IID). Roughly speaking, your data is IID if there aren’t any “patterns” in the data as you move top to bottom. For example, time-series data is often not IID, there’s often an underlying time trend to the data. Or the data may be autocorrelated (if y was above average yesterday, it’ll probably be above average today too). If your data is sorted, say by customer ID, then it likely isn’t IID. You might be able to shuffle it in this case.\nIf your data are IID, it may be OK to just fit the model on the first block. In principal, it should be a representative sample of your entire dataset.\nHere’s a quick example. We’ll fit a GradientBoostingClassifier. The dataset will be 1,000,000 x 20, in chunks of 10,000. This would take way too long to fit regularly. But, with IID data, we may be OK fitting the model on just the the first 10,000 observations.\n\u003e\u003e\u003e from dask_ml.datasets import make_classification \u003e\u003e\u003e from dask_ml.iid.ensemble import GradientBoostingClassifier \u003e\u003e\u003e X, y = make_classification(n_samples=1_000_000, chunks=10_100) \u003e\u003e\u003e clf = GradientBoostingClassifier() \u003e\u003e\u003e clf.fit(X, y) At this point, we have a scikit-learn estimator that can be used to transform or predict for dask arrays (in parallel, out of core or distributed across your cluster).\n\u003e\u003e\u003e prob_a dask.array\u003cpredict_proba, shape=(1000000, 2), dtype=float64, chunksize=(10000, 2)\u003e \u003e\u003e\u003e prob_a[:10].compute() array([[0.98268198, 0.01731802], [0.41509521, 0.58490479], [0.97702961, 0.02297039], [0.91652623, 0.08347377], [0.96530773, 0.03469227], [0.94015097, 0.05984903], [0.98167384, 0.01832616], [0.97621963, 0.02378037], [0.95951444, 0.04048556], [0.98654415, 0.01345585]]) An alternative to dask_ml.iid is to sample your data and use a regular scikit-learn estimator. But the dask_ml.iid approach is slightly preferable, since post-fit tasks like prediction can be done on dask arrays in parallel (and potentially distributed). Scikit-Learn’s estimators are not dask-aware, so they’d just convert it to a NumPy array, possibly blowing up your memory.\nIf dask and dask_ml.iid had existed a few years ago, it would have solved all the “big data” needs of my old job. Personally, I never hit a problem where, if my dataset was already large, training on an even larger dataset was the answer. I’d always hit the level part of the learning curve, or was already dealing with highly imbalanced classes. But, I would often have to make predictions for a much larger dataset. For example, I might have trained a model on “all the customers for this store” and predicted for “All the people in Iowa”.\n","wordCount":"525","inLanguage":"en","datePublished":"2018-01-28T00:00:00Z","dateModified":"2018-01-28T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/dask-ml-iid/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Rewriting scikit-learn for big data, in under 9 hours.</h1><div class=post-meta><span title='2018-01-28 00:00:00 +0000 UTC'>January 28, 2018</span></div></header><div class=post-content><p>This past week, I had a chance to visit some of the scikit-learn developers at
Inria in Paris. It was a fun and productive week, and I&rsquo;m thankful to them for
hosting me and Anaconda for sending me there.</p><p>Towards the end of our week, Gael threw out the observation that for many
applications, you don&rsquo;t need to <em>train</em> on the entire dataset, a sample is often
sufficient. But it&rsquo;d be nice if the trained estimator would be able to
<em>transform</em> and <em>predict</em> for dask arrays, getting all the nice distributed
parallelism and memory management dask brings.</p><p>This intrigued me, and I had a 9 hour plane ride, so&mldr;</p><h2 id=dask_mliid><code>dask_ml.iid</code><a hidden class=anchor aria-hidden=true href=#dask_mliid>#</a></h2><p>I put together the <code>dask_ml.iid</code> sub-package. The estimators contained within
are appropriate for data that are independently and identically distributed
(IID). Roughly speaking, your data is IID if there aren&rsquo;t any &ldquo;patterns&rdquo; in the
data as you move top to bottom. For example, time-series data is often <em>not</em>
IID, there&rsquo;s often an underlying time trend to the data. Or the data may be
autocorrelated (if <code>y</code> was above average yesterday, it&rsquo;ll probably be above
average today too). If your data is sorted, say by customer ID, then it likely
isn&rsquo;t IID. You might be able to shuffle it in this case.</p><p>If your data are IID, it <em>may</em> be OK to just fit the model on the first block.
In principal, it should be a representative sample of your entire dataset.</p><p>Here&rsquo;s a quick example. We&rsquo;ll fit a <code>GradientBoostingClassifier</code>. The dataset
will be 1,000,000 x 20, in chunks of 10,000. This would take <em>way</em> too long to
fit regularly. But, with IID data, we may be OK fitting the model on just the
the first 10,000 observations.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>from</span> dask_ml.datasets <span style=color:#f92672>import</span> make_classification
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>from</span> dask_ml.iid.ensemble <span style=color:#f92672>import</span> GradientBoostingClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> X, y <span style=color:#f92672>=</span> make_classification(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>1_000_000</span>, chunks<span style=color:#f92672>=</span><span style=color:#ae81ff>10_100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> clf <span style=color:#f92672>=</span> GradientBoostingClassifier()
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> clf<span style=color:#f92672>.</span>fit(X, y)
</span></span></code></pre></div><p>At this point, we have a scikit-learn estimator that can be used to transform or
predict for dask arrays (in parallel, out of core or distributed across your
cluster).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> prob_a
</span></span><span style=display:flex><span>dask<span style=color:#f92672>.</span>array<span style=color:#f92672>&lt;</span>predict_proba, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1000000</span>, <span style=color:#ae81ff>2</span>), dtype<span style=color:#f92672>=</span>float64, chunksize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10000</span>, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> prob_a[:<span style=color:#ae81ff>10</span>]<span style=color:#f92672>.</span>compute()
</span></span><span style=display:flex><span>array([[<span style=color:#ae81ff>0.98268198</span>, <span style=color:#ae81ff>0.01731802</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.41509521</span>, <span style=color:#ae81ff>0.58490479</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.97702961</span>, <span style=color:#ae81ff>0.02297039</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.91652623</span>, <span style=color:#ae81ff>0.08347377</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.96530773</span>, <span style=color:#ae81ff>0.03469227</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.94015097</span>, <span style=color:#ae81ff>0.05984903</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.98167384</span>, <span style=color:#ae81ff>0.01832616</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.97621963</span>, <span style=color:#ae81ff>0.02378037</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.95951444</span>, <span style=color:#ae81ff>0.04048556</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.98654415</span>, <span style=color:#ae81ff>0.01345585</span>]])
</span></span></code></pre></div><p>An alternative to <code>dask_ml.iid</code> is to sample your data and use a regular
scikit-learn estimator. But the <code>dask_ml.iid</code> approach is <em>slightly</em> preferable,
since post-fit tasks like prediction can be done on dask arrays in parallel (and
potentially distributed). Scikit-Learn&rsquo;s estimators are not dask-aware, so
they&rsquo;d just convert it to a NumPy array, possibly blowing up your memory.</p><p>If dask and <code>dask_ml.iid</code> had existed a few years ago, it would have solved all
the &ldquo;big data&rdquo; needs of my old job. Personally, I never hit a problem where, if
my dataset was already large, training on an even larger dataset was the answer.
I&rsquo;d always hit the level part of the learning curve, or was already dealing with
highly imbalanced classes. But, I would often have to make <em>predictions</em> for a
much larger dataset. For example, I might have trained a model on &ldquo;all the
customers for this store&rdquo; and predicted for &ldquo;All the people in Iowa&rdquo;.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://tomaugspurger.net>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>