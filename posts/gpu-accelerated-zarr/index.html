<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GPU-Accelerated Zarr | Tom's Blog</title>
<meta name=keywords content><meta name=description content='This post gives detailed background to my PyData Global talk, &ldquo;GPU-Accelerated
Zarr&rdquo; (slides, video). It
deliberately gets into the weeds, but I will try to provide some background for
people who are new to Zarr, GPUs, or both.
The first takeaway is that zarr-python natively supports
NVIDIA GPUs. With a one-line zarr.config.enable_gpu() you can configure zarr
to return CuPy arrays, which reside on your GPU:
>>> import zarr
>>> zarr.config.enable_gpu()
>>> z = zarr.open_array("path/to/store.zarr", mode="r")
>>> type(z[:])
cupy.ndarray
The second takeaway, and the main focus of this post, is that that simple
one-liner leaves performance on the table. It depends a bit on your workload,
but I&rsquo;d claim that  Zarr&rsquo;s data loading pipeline shouldn&rsquo;t ever be the
bottleneck. Achieving maximum throughput today requires some care to ensure that
the system&rsquo;s resources are used efficiently. I&rsquo;m hopeful that we can improve the
libraries to do the right thing in more situations.'><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/gpu-accelerated-zarr/><link crossorigin=anonymous href=/assets/css/stylesheet.ced21e6d3497ee93fed8f8b357448095840179bd510b5ea0e6013078712e6dd1.css integrity="sha256-ztIebTSX7pP+2PizV0SAlYQBeb1RC16g5gEweHEubdE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tomaugspurger.net/posts/gpu-accelerated-zarr/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="GPU-Accelerated Zarr"><meta property="og:description" content='This post gives detailed background to my PyData Global talk, &ldquo;GPU-Accelerated
Zarr&rdquo; (slides, video). It
deliberately gets into the weeds, but I will try to provide some background for
people who are new to Zarr, GPUs, or both.
The first takeaway is that zarr-python natively supports
NVIDIA GPUs. With a one-line zarr.config.enable_gpu() you can configure zarr
to return CuPy arrays, which reside on your GPU:
>>> import zarr
>>> zarr.config.enable_gpu()
>>> z = zarr.open_array("path/to/store.zarr", mode="r")
>>> type(z[:])
cupy.ndarray
The second takeaway, and the main focus of this post, is that that simple
one-liner leaves performance on the table. It depends a bit on your workload,
but I&rsquo;d claim that  Zarr&rsquo;s data loading pipeline shouldn&rsquo;t ever be the
bottleneck. Achieving maximum throughput today requires some care to ensure that
the system&rsquo;s resources are used efficiently. I&rsquo;m hopeful that we can improve the
libraries to do the right thing in more situations.'><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/gpu-accelerated-zarr/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-11T08:00:00-06:00"><meta property="article:modified_time" content="2025-12-11T08:00:00-06:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GPU-Accelerated Zarr"><meta name=twitter:description content='This post gives detailed background to my PyData Global talk, &ldquo;GPU-Accelerated
Zarr&rdquo; (slides, video). It
deliberately gets into the weeds, but I will try to provide some background for
people who are new to Zarr, GPUs, or both.
The first takeaway is that zarr-python natively supports
NVIDIA GPUs. With a one-line zarr.config.enable_gpu() you can configure zarr
to return CuPy arrays, which reside on your GPU:
>>> import zarr
>>> zarr.config.enable_gpu()
>>> z = zarr.open_array("path/to/store.zarr", mode="r")
>>> type(z[:])
cupy.ndarray
The second takeaway, and the main focus of this post, is that that simple
one-liner leaves performance on the table. It depends a bit on your workload,
but I&rsquo;d claim that  Zarr&rsquo;s data loading pipeline shouldn&rsquo;t ever be the
bottleneck. Achieving maximum throughput today requires some care to ensure that
the system&rsquo;s resources are used efficiently. I&rsquo;m hopeful that we can improve the
libraries to do the right thing in more situations.'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":2,"name":"GPU-Accelerated Zarr","item":"https://tomaugspurger.net/posts/gpu-accelerated-zarr/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GPU-Accelerated Zarr","name":"GPU-Accelerated Zarr","description":"This post gives detailed background to my PyData Global talk, \u0026ldquo;GPU-Accelerated Zarr\u0026rdquo; (slides, video). It deliberately gets into the weeds, but I will try to provide some background for people who are new to Zarr, GPUs, or both.\nThe first takeaway is that zarr-python natively supports NVIDIA GPUs. With a one-line zarr.config.enable_gpu() you can configure zarr to return CuPy arrays, which reside on your GPU:\n\u0026gt;\u0026gt;\u0026gt; import zarr \u0026gt;\u0026gt;\u0026gt; zarr.config.enable_gpu() \u0026gt;\u0026gt;\u0026gt; z = zarr.open_array(\u0026#34;path/to/store.zarr\u0026#34;, mode=\u0026#34;r\u0026#34;) \u0026gt;\u0026gt;\u0026gt; type(z[:]) cupy.ndarray The second takeaway, and the main focus of this post, is that that simple one-liner leaves performance on the table. It depends a bit on your workload, but I\u0026rsquo;d claim that Zarr\u0026rsquo;s data loading pipeline shouldn\u0026rsquo;t ever be the bottleneck. Achieving maximum throughput today requires some care to ensure that the system\u0026rsquo;s resources are used efficiently. I\u0026rsquo;m hopeful that we can improve the libraries to do the right thing in more situations.\n","keywords":[],"articleBody":"This post gives detailed background to my PyData Global talk, “GPU-Accelerated Zarr” (slides, video). It deliberately gets into the weeds, but I will try to provide some background for people who are new to Zarr, GPUs, or both.\nThe first takeaway is that zarr-python natively supports NVIDIA GPUs. With a one-line zarr.config.enable_gpu() you can configure zarr to return CuPy arrays, which reside on your GPU:\n\u003e\u003e\u003e import zarr \u003e\u003e\u003e zarr.config.enable_gpu() \u003e\u003e\u003e z = zarr.open_array(\"path/to/store.zarr\", mode=\"r\") \u003e\u003e\u003e type(z[:]) cupy.ndarray The second takeaway, and the main focus of this post, is that that simple one-liner leaves performance on the table. It depends a bit on your workload, but I’d claim that Zarr’s data loading pipeline shouldn’t ever be the bottleneck. Achieving maximum throughput today requires some care to ensure that the system’s resources are used efficiently. I’m hopeful that we can improve the libraries to do the right thing in more situations.\nThis post pairs nicely with Earthmover’s I/O-Maxing Tensors in the Cloud post, which showed that network and object storage service (e.g. S3) also shouldn’t be a bottleneck in most workloads. Ideally, your actual computation is where the majority of time is spent, and the I/O pipeline just gets out of your way.\nSome background I imagine that some people reading this have experience with Zarr but not GPUs, or vice versa. Feel free to skip the sections you’re familiar with, and meet up with us at the Speed of Light section.\nZarr Background for GPU People Zarr is many things, but today we’ll focus on Zarr as the storage format for n-dimensional arrays. Instead of tabular data, which you might store in a columnar format like Apache Parquet, you’re working with data that fits things like xarray’s data model: everything is an n-dimensional array with metadata. For example, 3-d array measuring forecasts for a temperature field with dimensions (x, y, time).\nZarr is commonly used in many domains including microscopy, genomics, remote sensing, and climate / weather modeling. It works well with both local file systems and remote cloud object storage. High-level libraries like xarray can use zarr as a storage format:\n# https://tutorial.xarray.dev/intermediate/remote_data/cmip6-cloud.html \u003e\u003e\u003e ds = xr.open_zarr( ... \"gs://cmip6/CMIP6/ScenarioMIP/NOAA-GFDL/...\", ... consolidated=True, ... ) \u003e\u003e\u003e zos_2015jan = ds.zos.sel(time=\"2015-01-16\") \u003e\u003e\u003e zos_2100dec = ds.zos.sel(time=\"2100-12-16\") \u003e\u003e\u003e sealevelchange = zos_2100dec - zos_2015jan \u003e\u003e\u003e sealevelchange.plot.imshow() xarray knows how to translate the high-level slicing like time=\"2015-01-16\" to the lower level slicing of a Zarr array, and Zarr knows how to translate positional slices in the large n-dimensional array to files / objects in storage. This diagram shows the structure of a Zarr store:\nThe large logical array is split into one or more chunks along one or more dimensions. The chunks are then compressed and stored to disk, which lowers storage costs and can improve read and write performance (it might be faster to read fewer bytes, even if you have to spend time decompressing them).\nZarr’s sharding codec is especially important for GPUs. This makes it possible to store many chunks in the same file (a file on disk, or an object in object storage). We call the collection of chunks a shard, and the shard is what’s actually written to disk.\nMultiple chunks are (independently) compressed, concatenated, and stored into the same file / object. We’ll discuss this more when we talk about performance, but the key thing sharding provides is amortizing some constant costs (opening a file, checking its length, etc.) over many chunks, which can be operated on in parallel (which is great news for GPUs).\nFor now, just note that we’ll be dealing with various levels of Zarr’s hierarchy:\nArrays: the logical n-dimensional array Shards: the file on disk / object in object storage, which contains many chunks concatenated together Chunks: the smallest unit we can read (since it must be decompressed to interpret the bytes correctly) GPU Background for Zarr People GPUs are massively parallel processors: they excel when you can apply the same problem to a big batch of data. This works well for video games, ML / AI workloads, and data science / data analysis applications.\n(NVIDIA) GPUs execute “kernels”, which are essentially functions that run on GPU data. Today, we won’t be discussing how to author a compute kernel. We’ll be using existing kernels (from libraries like nvcomp, CuPy, and CCCL). Instead, we’ll be worried about higher-level things like memory allocations, data movement, and concurrency\nMany (though not all) GPU architectures have dedicated GPU memory. This is separate from the regular main memory of your machine (you’ll hear the term “device” to refer to GPUs, and “host” to refer to the host operating system / machine, where your program is running).\nWhile device memory tends to be relatively fast compared to host memory (for example, it might have \u003e3.3 TB/s from the GPU’s memory to its compute cores), it’s moving data between host and device memory is relatively slow (perhaps just 128 GB/s over PCIe). It also tends to be relatively small (an NVIDIA H100 has 80-94GB of GPU memory; newer generations have more, but still GPU memory is precious when processing large datasets). All this means we need to be careful with memory, both how we allocate and deallocate memory and how we move data between the host and device.\nIn GPU programming, keeping the GPU busy is necessary (but not sufficient!) to achieve good performance. We’ll use GPU utilization, the percent of time (over some window) when the GPU was busy executing some kernel, as a rough measure of how well we’re doing.\nOne way to achieve high GPU utilization is to queue up work for the GPU to do. The GPU is a device, a coprocessor, onto which your host program offloads work. As much as possible, we’ll have our Python program just do orchestration, leaving the heavy computation to the GPU. Doing this well requires your host program to not slow down the (very fast) GPU.\nIn some sense, you want your Python program to be “ahead” of the GPU. If you wait to submit your next computation until some data is ready on the GPU, or some previous computation is completed, you’ll inevitably have some time gap when your GPU is idle. Sometimes this is inevitable, but with a bit of care we’ll be able to make our Zarr example perform well.\nMy Cloud Native Geospatial Conference post touched on this under Pipelining. This program waits to schedule the computation until the CPU is done reading the data, and so doesn’t achieve high throughput:\nThis second program queues up plenty of work to do, and so achieves higher throughput:\nFor this example, we’ll use a single threaded program with multiple CUDA Streams to achieve good pipelining. CUDA streams are a way to express a sequence (a stream, if you will) of computations that must happen in order. But, crucially, you can have multiple streams active at the same time. This is nice because it frees you from having to worry too much about exactly how to schedule work on the GPU. For example, one stream of computation might heavily use the memory subsystem (to transfer data from the host to device, for example) while another stream might be using the compute cores. But you don’t have to worry about timing things so that the memory-intensive operation runs at the same time as the compute-intensive operation.\nIn pseudocode:\na0 = read_chunk(\"path/to/a\", stream=stream_a) b0 = read_chunk(\"path/to/b\", stream=stream_b) a1 = transform(a0, stream=stream_a) b1 = transform(b0, stream=stream_b) read_chunk might exercise the memory system to transfer data from the host to the device, while transform might really hammer the compute cores.\nAll you need to do is “just” correctly express the relationships between the different parts of your computation (not always easy!). The GPU will take care of running things concurrently where possible.\nOne subtle point here: these APIs are typically non-blocking in your host Python (or C/C++/whatever) program. read_chunk makes some CUDA API calls internally to kick off the host to device transfer, but it doesn’t wait for that transfer to complete. This is good, since we want our host program to be well ahead of the GPU; we want to go to the next line and feed the GPU more work to do.\nIf we actually poked the memory address where the data’s supposed to be it might be junk. We just don’t know. If we really need to wait for some data / computation to be completed, we can call stream.synchronize(), which forces the host program to wait until all the computations on that stream are done. But ideally, you don’t need that. For the typical case of launching some CUDA kernel some some data, synchronization is unnecessary. You only need to ensure that the computation happens on the same CUDA stream as the data loading (like in our pseudocode example, launching each transform on the appropriate stream), and you’re good to go.\nCUDA streams do take some getting used to. You can make analogies to thread programming and to async / await, but that only gets you so far. At the end of the day they’re an extremely useful tool to have in your toolkit.\nSpeed of Light When analyzing performance, it can be helpful to perform a simple “speed-of-light” analysis: given the constraints of my system, what performance (throughput, latency, whatever metric you care about) should I expect to achieve? This can combine abstract things (like a performance model for how your system operates) with practical things (what’s the sequential read throughput of my disk? What’s the clock cycle of my CPU?).\nMany Zarr workloads involve (at least) three stages:\nReading bytes from storage (local disk or remote object storage). Your disk (for local storage) or NIC / remote storage service (for remote storage) has some throughput, which you should aim to saturate. Which bytes you need to read will be dictated in part by your application. Zarr supports reading subsets of data (with the chunk being the smallest decompressable unit). Ideally, your chunking should align with your access pattern.\nDecompressing bytes with the Codec Pipeline. Different codecs have different throughput targets, and these can depend heavily on the data, chunk size, and hardware. We’re using the default Zstd codec in this example.\nYour actual computation. This should ideally be the bottleneck: it’s the whole reason you’re loading all this data after all.\nAnd if you are using a GPU, at some point you need to get the bytes from host to device memory1.\nFinally, you might need to store your result. If your computation reduces the data this might be negligible. But if you’re outputting large n-dimensional arrays this can be as or more expensive than the reading.\nIn this case, we don’t really care about what the computation is; just something that uses the data and takes a bit of time. We’ll do a bunch of matrix multiplications because they’re pretty computationally expensive and they’re well suited to GPUs.\nNotably, we won’t do any kind computation that involves data from multiple shards. They’re completely independent in this example, which makes parallelizing at the shard level much simpler.\nExample Workload This workload operates on a 1-D float32 array with the following properties:\nLevel Shape Size (MB) Count per parent Chunk (256_000,) 1.024 400 chunks / shard Shard (102_400_000,) 409.6 8 shards / array Array (819_200_000,) 3,276.8 - Each chunk is Zstd compressed, and the shards take about 77.5 MB on disk giving a compression ratio of about 5.3.\nThe fact that the array is 1-D isn’t too relevant here: zarr supports n-dimension arrays with chunking along any dimension. It does ensure that one optimization is always available when decoding bytes, because the chunks are always contiguous subsets of the shards. We’ll talk about this in detail in the Decode bytes section.\nOur workload will read the data, transfer it to the GPU (if using the GPU) and perform a bunch of matrix multiplications.\nPerformance Summary This example workload has been fine-tuned to make the GPU look good, and I’ve done zero tuning / optimization of the CPU implementation. Any comparisons with CPU libraries are essentially bunk, but it’s a natural question so I’ll report them anyway.\nThe top level summary will compare three implementations:\nzarr-python: Uses vanilla zarr-python for I/O and decoding, and NumPy for the computation. zarr-python GPU: Uses zarr-python’s built-in GPU support to return CuPy arrays, so the GPU is used for computation. At the moment, this still uses Numcodecs to decompress the data, which runs on the CPU. After decompression, the data is moved to the GPU for the matrix multiplication. Custom GPU: My custom implementation of I/O and decoding with CuPy for the computation. Implementation Duration (ms) Zarr / NumPy 19,892 Zarr / CuPy 3,407 Custom / CuPy 478 You can find the code for these in my CUDA Stream Samples repository.\nPlease don’t take the absolute numbers, or even the relative numbers too seriously. I’ve spent zero time optimizing the Zarr/NumPy and Zarr/CuPy implementations. The important thing to take away here is that we have plenty of room for improvement. My Custom I/O pipeline just gradually removed bottlenecks as they came up, some of which apply to zarr-python’s CPU implementation as well. Follow https://github.com/zarr-developers/zarr-python/issues/2904 if you’re interested in developments.\nThe remainder of the post will describe, in some detail, what makes the custom implementation so fast.\nPerformance optimizations Once you have the basics down (using the right data structures / algorithm, removing the most egregious overheads), speeding up a problem often involves parallelization. And you very often have multiple levels of parallelization available. Picking the right level is absolutely a skill that requires some general knowledge about performance and specific details for your problem.\nIn this case, we’ll operate at the shard level. This will be the maximum amount of data we need to hold in memory at any point in time (though the problem is small enough that we can operate on all the shards at the same time).\nWe’ll use a few techniques to get good performance in our pipeline:\nNo (large) memory allocations on the critical path. This applies to both host and device memory allocations. We’ll achieve this by preallocating all the arrays we need to process the shard. Whether or not this should be considered cheating or not is a bit debatable and a bit workload dependent. I’d argue that the most advanced, performance-sensitive workloads will process large amounts of data and so can preallocate a pool of buffers and reuse them across their unit of parallelization (shards in our case).\nRegardless, if we’re doing large memory allocations after we’ve started processing a shard (either host or device allocations for the final array or for intermediates) then these allocations can quickly become the bottleneck. Pre-allocation (and reuse across shards) is an important optimization if it’s available.\nUse pinned (page-locked) memory for host buffers Using pinned memory makes the host to device transfers much faster. More on that later.\nUse CUDA streams to overlap I/O and Computation Our workload has a regular pattern of “read, transfer, decode, compute” on each shard. Because these exercise different parts of the GPU (transfer uses the memory subsystem, decode and compute launch kernels that run on the GPU’s cores), we can run them concurrently.\nWe’ll assign a CUDA stream per shard. We’ll be very careful to avoid stream / device synchronizations so that our host program schedules all the work to be done.\nThroughout this, we’ll use nvtx to annotate certain ranges of code. This will make reading the Nsight Systems report easier.\nHere’s a screenshot of an nsys profile, with a few important bits highlighted (open the file for a full-sized screenshot):\nUnder Processes \u003e Threads \u003e python, you see the traces for our host program, in this case a Python program. This will include our nvtx annotations (read::disk, read::transfer, read::decode, etc.) and calls to the CUDA API (e.g. cudaMemcpyAsync). These calls measure the time spent by the CPU / host program, not the GPU. Under Processes \u003e CUDA HW, you’ll see the corresponding traces for GPU operations. This shows CUDA kernels (functions that run on the GPU) in light blue and memory operations (like host to device transfers) in teal. You can download the full nsight report here and open it locally with NVIDIA Nsight Systems.\nThis table summarizes roughly where we spend our time on the GPU per shard (very rough, and there’s some variation across shards, especially as we start overlapping operations with CUDA streams).\nStage Duration (ms) Raw Throughput (GB/s) Effective Throughput (GB/s) Read 13.6 5.7 30.1 Transfer 1.5 51.7 273 Decode 45 1.7 9.1 Compute 150 2.7 2.7 Raw throughput measures the actual number of bytes processed per time unit, which is the compressed size for reading, transferring, and decoding. “Effective Throughput” uses the uncompressed number of bytes for each stage. After decompression the actual number of bytes processed equals the uncompressed bytes, so Compute’s raw throughput is equal to its effective throughput.\nRead bytes First, we need to load the data. In my example, I’m just using files on a local disk, though you could use remote object storage and still perform well. We’ll parallelize things at the shard level (i.e. we’re assuming that the entirety of the shard fits in GPU memory).\npath = array.store_path.store.root / array.store_path.path / key with open(path, \"rb\") as f, nvtx.annotate(\"read::disk\"): f.readinto(host_buffer) On my system, it takes about 13.6 ms to read the 77.5 MB, for a throughput of about 5.7 GB/s from disk (the OS probably had at least some of the pages cached). The effective throughput (uncompressed size over duration) is about 30.1 GB/s. I’ll note that I haven’t spent much effort optimizing this section.\nNote that we use readinto to read the data from disk directly into the pre-allocated host buffer: we don’t want any (large) memory allocations on the critical path. Also, we’re using pinned memory (AKA page-locked memory) for the host buffers. This prevents the operating system from paging the buffers, which lets the GPU directly access that memory when copying it, no intermediate buffers required.\nAnd it’s worth emphasizing: this I/O is happening on the host Python program, and it is blocking. As we’ll see later, time spent doing stuff in Python is time not spent scheduling work on the GPU. We’ll need to ensure that the GPU is fed sufficient work, so let’s keep our eye on this section.\nThe profile report for this section is pretty boring:\nNote what the GPU is doing right now: nothing! There aren’t any CUDA HW annotations visible above the initial read::disk. At least for the very first shard we read, the GPU is necessarily idle. But as we’ll discuss shortly, subsequent shards are able to overlap disk I/O with CUDA operations.\nThis screenshot shows the profile for the second shard:\nNow the GPU is busy with some other operations (decoding the chunks from the first shard in this case, which are directly above the read::decode happening on the host at that time). This is partly why I didn’t bother with parallelizing the disk I/O: only one thing can be the bottleneck, and right now we’re able to load data from disk quickly enough.\nTransfer bytes After we’ve read the bytes into memory, we schedule the host to device transfer:\nwith nvtx.annotate(\"read::transfer\"), stream: # device_buffer is a pre-allocated cupy.ndarray device_buffer.set( host_buffer[:-index_offset].view(device_buffer.dtype), stream=stream ) This is where our earlier discussion on blocking vs. non-blocking APIs comes in handy. The device_buffer.set call is not blocking, which is why it takes only ~60 μs on the host. It only makes the CUDA API call to set up the transfer and then immediately returns back to the Python program (to close our context managers and then continue to the next line in our program).\nThe actual memory copy (which is running on the device) takes about 1.5 ms for a throughput of about 52 GB/s (this is still compressed data, so the effective throughput is even higher). Here’s the same profile I showed earlier, but now you’ll understand the context around what happens on the host (the CUDA API call to do something) and device.\nI’ve added the orange lines connecting the fast cudaMemcpyAsync on the host to the (not quite as fast) Memcpy HtoD (host to device) running on the device.\nAnd if you look closely, you’ll see that just above that Memcpy HtoD in teal, we’re executing a compute kernel (in light-blue). We’ll get to that in a bit, but this show that we’re overlapping Host-to-Device transfers with compute kernels.\nDecode bytes At this point we have (or will have, eventually) the Zstd compressed bytes in GPU memory. You might think that “decompressing a stream of bytes” doesn’t mesh well with “GPUs as massively parallel processors”. And you’d be (partially) right! We can’t really parallelize decoding within a single chunk, but we can decode all the chunks in a shard in parallel. My colleague Akshay has a nice overview of how the GPU can be used to decode many buffers in parallel.\nI have no idea how to implement a Zstd decompressor, but fortunately we don’t have to. The nvCOMP library implements a bunch of GPU-accelerated compression and decompression routines, including Zstd. It provides C, C++, and Python APIs. A quick note: this example is using a custom wrapper around nvcomp’s C API. This works around a couple issues with nvcomp’s Python bindings.\nAt the moment, accessing an attribute on the decompressed array returned by nvcomp causes a “stream synchronization”. This forces essentially blocks the host program from progressing until the GPU has caught up, which we’d like to avoid. We need to issue compute instructions still, and we’d ideally move on to the next shard! We’d like full control over all the memory allocations, including the ability to preallocate the output buffers that the arrays should be decompressed into. This is possible with the C API, but not (yet) the Python API. My custom wrapper is not at all robust, well designed, etc. It’s just enough to work for this demo. Don’t use it! Use the official Python bindings, and reach out to me or the nvcomp team if you run into any issues. But here’s the basic idea in code:\nzstd_codec = ZstdCodec(stream=stream) # get a list of arrays, each of which is a view into the original device buffer # `device_buffer` is stream-ordered on `stream`, # so `device_arrays` are all stream-ordered on `stream` device_arrays = [ device_buffer[offset : offset + size] for offset, size in index ] with nvtx.annotate(\"read::decode\"): zstd_codec.decode_batch(device_arrays, out=out_chunks) # and now `out` is stream-ordered on `stream` The zstd_codec.decode_batch call takes about 2.4 ms on my machine. Again this just schedules the decompression call.\nThe actual decompression takes about 25-45 ms, for a throughput of about roughly 1.7 GB/s.\nAgain, we’ve pre-allocated the out ndarray, however this is not always possible. Zarr allows chunking over arbitrary dimensions, but we’ve assumed that the chunks are contiguous slices of the output array2. If your chunks aren’t contiguous slices of the output array, you’ll need to decode into an intermediate buffer and then perform some memory copies into the output buffer.\nAnyway, all this is to say that decompression isn’t our bottleneck. And this is despite decompression competing for GPU cores with the computation. The newer NVIDIA Blackwell Architecture includes a dedicated Decompression Engine which improves the decompression throughput even more.\nAnd for those curious, a brief experiment without compression is about twice as slow on the GPU as the version with compression, though I didn’t investigate it deeply.\nComputation This example is primarily focused on the data loading portion of a Zarr workload, so the computation is secondary. I just threw in a bunch of matrix multiplications / reductions (which GPUs tend to do quickly).\nBut while the specific computation is unimportant, there are some characteristics to consider about your computation, it should take some non-negligible amount of time, such it’s worthwhile moving the data from the host to the device for the computation (and moving the result back to the host).\nThe key thing we care about here is overlapping host to device copies with compute, so that the GPU isn’t sitting around waiting for data. Note how the teal Host to Device Copy is running at the same time as the matrix multiplication from the previous shard:\nAnd at this point, you can start analyzing GPU metrics if you still need to squeeze additional performance out of your pipeline.\nBut I think that’s enough for now.\nSummary One takeaway here is that GPUs are fast, which, sure. A slightly more interesting takeaway is that GPUs can be extremely fast, but achieving that takes some care.\nIn this workload my custom pipeline achieved high throughput by\nBeing very careful with memory allocations and data movement. Using pinned host memory to speed up the one host to device transfer per shard Use nvcomp and Zarr shards to parallelize decoding many chunks on the GPU Use CUDA streams to express our workloads’ shard-level parallelism, so that we can overlap host I/O, host-to-device copies, kernel launches and kernel execution. I’m hopeful that we can optimize the codec pipeline and memory handling in zarr-python to close the gap between what it provides and my custom, hand-optimized implementation (0.5s). But doing that in a general purpose library will require even more thought and care than my hacky implementation.\nIf you’ve made it this far, congrats. Reach out if you have any feedback, either directly or on the Zarr discussions board.\nNVIDIA does have GPU Direct Storage which offers a way to read directly from storage to the device, bypassing the host (OS and memory system) entirely. I haven’t tried using that yet. ↩︎\nExplaining that optimization in more detail. We need the chunks to be contiguous in the shard. Consider this shard, with the letters indicating the chunks:\n| a a a a | | b b b b | | c c c c | | d d d d | In C-contiguous order, that can be stored as:\n| a a a a b b b b c c c c d d d d| i.e. all of the a’s are together in a contiguous chunk. That means we can tell nvcomp to write its output at this memory address and it’ll work out fine. Likewise for b, just offset by some amount, and so on for the other chunks.\nHowever, this chunking is not amenable to this optimization because the chunks aren’t contiguous in the shard:\n| a a b b | | a a b b | | c c d d | | c c d d | Maybe someone smarter than me could pull off something with stride tricks. But for now, note that the ability to preallocate the output array might not always be an option.\nThat’s not necessarily a deal-killer: you’ll just need a temporary buffer for the decompressed output and an extra memcpy per chunk into the output shard. ↩︎\n","wordCount":"4493","inLanguage":"en","datePublished":"2025-12-11T08:00:00-06:00","dateModified":"2025-12-11T08:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/gpu-accelerated-zarr/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net/ accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">GPU-Accelerated Zarr</h1><div class=post-meta><span title='2025-12-11 08:00:00 -0600 -0600'>December 11, 2025</span></div></header><div class=post-content><p>This post gives detailed background to my PyData Global talk, &ldquo;GPU-Accelerated
Zarr&rdquo; (<a href=https://assets.tomaugspurger.net/tomaugspurger/posts/gpu-accelerated-zarr/GPU%20Acceleterated%20Zarr.pdf>slides</a>, <a href=https://youtu.be/dYt_fabaK60>video</a>). It
deliberately gets into the weeds, but I will try to provide some background for
people who are new to Zarr, GPUs, or both.</p><p>The first takeaway is that zarr-python <a href=https://zarr.readthedocs.io/en/stable/user-guide/gpu/>natively supports</a>
NVIDIA GPUs. With a one-line <code>zarr.config.enable_gpu()</code> you can configure zarr
to return CuPy arrays, which reside on your GPU:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>import</span> zarr
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> zarr<span style=color:#f92672>.</span>config<span style=color:#f92672>.</span>enable_gpu()
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> z <span style=color:#f92672>=</span> zarr<span style=color:#f92672>.</span>open_array(<span style=color:#e6db74>&#34;path/to/store.zarr&#34;</span>, mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;r&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> type(z[:])
</span></span><span style=display:flex><span>cupy<span style=color:#f92672>.</span>ndarray
</span></span></code></pre></div><p>The second takeaway, and the main focus of this post, is that that simple
one-liner leaves performance on the table. It depends a bit on your workload,
but I&rsquo;d claim that Zarr&rsquo;s data loading pipeline <em>shouldn&rsquo;t</em> ever be the
bottleneck. Achieving maximum throughput today requires some care to ensure that
the system&rsquo;s resources are used efficiently. I&rsquo;m hopeful that we can improve the
libraries to do the right thing in more situations.</p><p>This post pairs nicely with Earthmover&rsquo;s <a href=https://earthmover.io/blog/i-o-maxing-tensors-in-the-cloud>I/O-Maxing Tensors in the
Cloud</a> post, which showed that network and object storage service
(e.g. S3) also shouldn&rsquo;t be a bottleneck in most workloads. Ideally, your actual
computation is where the majority of time is spent, and the I/O pipeline
just gets out of your way.</p><h1 id=some-background>Some background<a hidden class=anchor aria-hidden=true href=#some-background>#</a></h1><p>I imagine that some people reading this have experience with Zarr but not GPUs,
or <em>vice versa</em>. Feel free to skip the sections you&rsquo;re familiar with, and meet
up with us at the <a href=#speed-of-light>Speed of Light</a> section.</p><h2 id=zarr-background-for-gpu-people>Zarr Background for GPU People<a hidden class=anchor aria-hidden=true href=#zarr-background-for-gpu-people>#</a></h2><p><a href=https://zarr.dev/>Zarr</a> is many things, but today we&rsquo;ll focus on Zarr as the <em>storage
format for n-dimensional arrays</em>. Instead of tabular data, which you might store
in a columnar format like Apache Parquet, you&rsquo;re working with data that fits
things like <a href=https://xarray.dev/>xarray</a>&rsquo;s data model: everything is an n-dimensional array with
metadata. For example, 3-d array measuring forecasts for a temperature field
with dimensions <code>(x, y, time)</code>.</p><p><img loading=lazy src=https://docs.xarray.dev/en/latest/_images/dataset-diagram.png alt="xarray dataset diagram"></p><p>Zarr is commonly used in many domains including microscopy, genomics, remote
sensing, and climate / weather modeling. It works well with both local file
systems and remote cloud object storage. High-level libraries like <a href=https://xarray.dev/>xarray</a>
can use zarr as a storage format:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># https://tutorial.xarray.dev/intermediate/remote_data/cmip6-cloud.html</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> ds <span style=color:#f92672>=</span> xr<span style=color:#f92672>.</span>open_zarr(
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>     <span style=color:#e6db74>&#34;gs://cmip6/CMIP6/ScenarioMIP/NOAA-GFDL/...&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>     consolidated<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>...</span> )
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> zos_2015jan <span style=color:#f92672>=</span> ds<span style=color:#f92672>.</span>zos<span style=color:#f92672>.</span>sel(time<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;2015-01-16&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> zos_2100dec <span style=color:#f92672>=</span> ds<span style=color:#f92672>.</span>zos<span style=color:#f92672>.</span>sel(time<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;2100-12-16&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> sealevelchange <span style=color:#f92672>=</span> zos_2100dec <span style=color:#f92672>-</span> zos_2015jan
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> sealevelchange<span style=color:#f92672>.</span>plot<span style=color:#f92672>.</span>imshow()
</span></span></code></pre></div><p><img loading=lazy src=https://tutorial.xarray.dev/_images/de44ed3784cdeb9939192b1251435c6642fe29e96039bcb6194aa92ebab2f13a.png alt="Plot showing the sea level change between the two dates."></p><p>xarray knows how to translate the high-level slicing like <code>time="2015-01-16"</code> to
the lower level slicing of a Zarr array, and Zarr knows how to translate
positional slices in the large n-dimensional array to files / objects in
storage. This diagram shows the structure of a Zarr store:</p><p><img loading=lazy src=https://zarr-specs.readthedocs.io/en/latest/_images/terminology-hierarchy.excalidraw.png alt="Zarr store hierarchy."></p><p>The large logical array is split into one or more <em>chunks</em> along one or more
dimensions. The chunks are then compressed and stored to disk, which lowers
storage costs and can improve read and write performance (it might be faster to
read fewer bytes, even if you have to spend time decompressing them).</p><p>Zarr&rsquo;s <a href=https://zarr-specs.readthedocs.io/en/latest/v3/codecs/sharding-indexed/index.html>sharding codec</a> is especially important for GPUs. This makes
it possible to store many <em>chunks</em> in the same file (a file on disk, or an
object in object storage). We call the collection of chunks a shard, and the
shard is what&rsquo;s actually written to disk.</p><p><img loading=lazy src=https://zarr-specs.readthedocs.io/en/latest/_images/sharding.png alt></p><p>Multiple chunks are (independently) compressed, concatenated, and stored into
the same file / object. We&rsquo;ll discuss this more when we talk about performance,
but the key thing sharding provides is amortizing some constant costs (opening a
file, checking its length, etc.) over many chunks, which can be operated on in
parallel (which is great news for GPUs).</p><p>For now, just note that we&rsquo;ll be dealing with various levels of Zarr&rsquo;s hierarchy:</p><ul><li>Arrays: the logical n-dimensional array</li><li>Shards: the file on disk / object in object storage, which contains many chunks concatenated together</li><li>Chunks: the smallest unit we can read (since it must be decompressed to interpret the bytes correctly)</li></ul><h2 id=gpu-background-for-zarr-people>GPU Background for Zarr People<a hidden class=anchor aria-hidden=true href=#gpu-background-for-zarr-people>#</a></h2><p>GPUs are massively parallel processors: they excel when you can apply the same
problem to a big batch of data. This works well for video games, ML / AI
workloads, and data science / data analysis applications.</p><p>(NVIDIA) GPUs execute &ldquo;kernels&rdquo;, which are essentially functions that run on GPU
data. Today, we won&rsquo;t be discussing how to author a compute kernel. We&rsquo;ll be
using existing kernels (from libraries like <a href=https://docs.nvidia.com/cuda/nvcomp/py_api.html>nvcomp</a>, <a href=https://cupy.dev/>CuPy</a>, and
<a href=https://nvidia.github.io/cccl/>CCCL</a>). Instead, we&rsquo;ll be worried about higher-level things like memory
allocations, data movement, and concurrency</p><p>Many (though <a href=https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/>not all</a>) GPU architectures have dedicated GPU
memory. This is separate from the regular main memory of your machine (you&rsquo;ll
hear the term &ldquo;device&rdquo; to refer to GPUs, and &ldquo;host&rdquo; to refer to the host
operating system / machine, where your program is running).</p><p>While device memory tends to be relatively fast compared to host memory (for
example, it might have >3.3 TB/s from the GPU&rsquo;s memory to its compute cores),
it&rsquo;s moving data between host and device memory is relatively slow (perhaps just
128 GB/s over PCIe). It also tends to be relatively small (an <a href=https://www.nvidia.com/en-us/data-center/h100/>NVIDIA
H100</a> has 80-94GB of GPU memory; newer generations have more, but still
GPU memory is precious when processing large datasets). All this means we need
to be careful with memory, both how we allocate and deallocate memory and how we
move data between the host and device.</p><p>In GPU programming, keeping the GPU busy is necessary (but not sufficient!) to
achieve good performance. We&rsquo;ll use GPU utilization, the percent of time (over
some window) when the GPU was busy executing some kernel, as a rough measure of
how well we&rsquo;re doing.</p><p>One way to achieve high GPU utilization is to queue up work for the GPU
to do. The GPU is a <em>device</em>, a <em>coprocessor</em>, onto which your host program
offloads work. As much as possible, we&rsquo;ll have our Python program just do
orchestration, leaving the heavy computation to the GPU. Doing this well
requires your host program to not slow down the (very fast) GPU.</p><p>In some sense, you want your Python program to be &ldquo;ahead&rdquo; of the GPU. If you
wait to submit your next computation until some data is ready on the GPU, or
some previous computation is completed, you&rsquo;ll inevitably have some time gap
when your GPU is idle. Sometimes this is inevitable, but with a bit of care
we&rsquo;ll be able to make our Zarr example perform well.</p><p>My <a href=https://tomaugspurger.net/posts/cng-forum-2025/>Cloud Native Geospatial Conference</a> post touched on this under
<a href=https://tomaugspurger.net/posts/cng-forum-2025/#pipelining>Pipelining</a>. This program waits to schedule the computation until
the CPU is done reading the data, and so doesn&rsquo;t achieve high throughput:</p><img src=/assets/cng-forum-2025-pipeline-bad.svg><p>This second program queues up plenty of work to do, and so achieves higher throughput:</p><img src=/assets/cng-forum-2025-pipeline-good.svg><p>For this example, we&rsquo;ll use a single threaded program with multiple <a href=https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/>CUDA
Streams</a> to achieve good pipelining. CUDA streams are a way to express
a sequence (a stream, if you will) of computations that must happen in order.
But, crucially, you can have <em>multiple</em> streams active at the same time. This is
nice because it frees you from having to worry too much about exactly how to
schedule work on the GPU. For example, one stream of computation might heavily
use the memory subsystem (to transfer data from the host to device, for example)
while another stream might be using the compute cores. But you don&rsquo;t have to
worry about timing things so that the memory-intensive operation runs at the
same time as the compute-intensive operation.</p><p>In pseudocode:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>a0 <span style=color:#f92672>=</span> read_chunk(<span style=color:#e6db74>&#34;path/to/a&#34;</span>, stream<span style=color:#f92672>=</span>stream_a)
</span></span><span style=display:flex><span>b0 <span style=color:#f92672>=</span> read_chunk(<span style=color:#e6db74>&#34;path/to/b&#34;</span>, stream<span style=color:#f92672>=</span>stream_b)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>a1 <span style=color:#f92672>=</span> transform(a0, stream<span style=color:#f92672>=</span>stream_a)
</span></span><span style=display:flex><span>b1 <span style=color:#f92672>=</span> transform(b0, stream<span style=color:#f92672>=</span>stream_b)
</span></span></code></pre></div><p><code>read_chunk</code> might exercise the memory system to transfer data from the host to
the device, while <code>transform</code> might really hammer the compute cores.</p><p>All you need to do is &ldquo;just&rdquo; correctly express the relationships between the
different parts of your computation (not always easy!). The GPU will take care
of running things concurrently where possible.</p><p>One subtle point here: these APIs are typically <em>non-blocking</em> in your host
Python (or C/C++/whatever) program. <code>read_chunk</code> makes some CUDA API calls
internally to kick off the host to device transfer, but it doesn&rsquo;t wait for that
transfer to complete. This is good, since we want our host program to be well
ahead of the GPU; we want to go to the next line and feed the GPU more work to
do.</p><p>If we actually poked the memory address where the data&rsquo;s supposed to be it might
be junk. We just don&rsquo;t know. If we <em>really</em> need to wait for some data /
computation to be completed, we can call <code>stream.synchronize()</code>, which forces
the host program to wait until all the computations on that stream are done.
But ideally, you don&rsquo;t need that. For the typical case of launching some
CUDA kernel some some data, synchronization is unnecessary. You only need
to ensure that the computation happens on the same CUDA stream as the data
loading (like in our pseudocode example, launching each <code>transform</code> on
the appropriate stream), and you&rsquo;re good to go.</p><p>CUDA streams do take some getting used to. You can make analogies to thread
programming and to async / await, but that only gets you so far. At the end of
the day they&rsquo;re an extremely useful tool to have in your toolkit.</p><h1 id=speed-of-light>Speed of Light<a hidden class=anchor aria-hidden=true href=#speed-of-light>#</a></h1><p>When analyzing performance, it can be helpful to perform a simple
&ldquo;speed-of-light&rdquo; analysis: given the constraints of my system, what performance
(throughput, latency, whatever metric you care about) should I expect to
achieve? This can combine abstract things (like a performance model for how your
system operates) with practical things (what&rsquo;s the sequential read throughput
of my disk? What&rsquo;s the clock cycle of my CPU?).</p><p>Many Zarr workloads involve (at least) three stages:</p><ol><li><p>Reading bytes from storage (local disk or remote object storage). Your disk
(for local storage) or <a href=https://en.wikipedia.org/wiki/Network_interface_controller>NIC</a> / remote storage service (for remote storage)
has some throughput, which you should aim to saturate. <em>Which</em> bytes you need
to read will be dictated in part by your application. Zarr supports reading
subsets of data (with the <strong>chunk</strong> being the smallest decompressable unit).
Ideally, your chunking should align with your access pattern.</p></li><li><p>Decompressing bytes with the Codec Pipeline.
Different codecs have different throughput targets, and these can depend
heavily on the data, chunk size, and hardware. We&rsquo;re using the default
Zstd codec in this example.</p></li><li><p>Your actual computation. This should ideally be the bottleneck: it&rsquo;s the
whole reason you&rsquo;re loading all this data after all.</p></li></ol><p>And if you are using a GPU, at some point you need to get the bytes from host to
device memory<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>Finally, you might need to store your result. If your computation reduces the
data this might be negligible. But if you&rsquo;re outputting large n-dimensional
arrays this can be as or more expensive than the reading.</p><p>In this case, we don&rsquo;t really care about what the computation is; just something
that uses the data and takes a bit of time. We&rsquo;ll do a bunch of matrix
multiplications because they&rsquo;re pretty computationally expensive and they&rsquo;re
well suited to GPUs.</p><p>Notably, we <em>won&rsquo;t</em> do any kind computation that involves data from multiple
shards. They&rsquo;re completely independent in this example, which makes
parallelizing at the shard level much simpler.</p><h2 id=example-workload>Example Workload<a hidden class=anchor aria-hidden=true href=#example-workload>#</a></h2><p>This workload operates on a 1-D float32 array with the following properties:</p><table><thead><tr><th style=text-align:left>Level</th><th style=text-align:left>Shape</th><th style=text-align:left>Size (MB)</th><th style=text-align:left>Count per parent</th></tr></thead><tbody><tr><td style=text-align:left>Chunk</td><td style=text-align:left><code>(256_000,)</code></td><td style=text-align:left>1.024</td><td style=text-align:left>400 chunks / shard</td></tr><tr><td style=text-align:left>Shard</td><td style=text-align:left><code>(102_400_000,)</code></td><td style=text-align:left>409.6</td><td style=text-align:left>8 shards / array</td></tr><tr><td style=text-align:left>Array</td><td style=text-align:left><code>(819_200_000,)</code></td><td style=text-align:left>3,276.8</td><td style=text-align:left>-</td></tr></tbody></table><p>Each chunk is Zstd compressed, and the shards take about 77.5 MB on disk
giving a compression ratio of about 5.3.</p><p>The fact that the array is 1-D isn&rsquo;t too relevant here: zarr supports
n-dimension arrays with chunking along any dimension. It <em>does</em> ensure that one
optimization is always available when decoding bytes, because the chunks are
always contiguous subsets of the shards. We&rsquo;ll talk about this in detail in the
<a href=#decode-bytes>Decode bytes</a> section.</p><p>Our workload will read the data, transfer it to the GPU (if using the GPU) and
perform a bunch of matrix multiplications.</p><h3 id=performance-summary>Performance Summary<a hidden class=anchor aria-hidden=true href=#performance-summary>#</a></h3><p><em>This example workload has been fine-tuned to make the GPU look good, and I&rsquo;ve
done zero tuning / optimization of the CPU implementation. Any comparisons with CPU
libraries are essentially bunk, but it&rsquo;s a natural question so I&rsquo;ll report them
anyway.</em></p><p>The top level summary will compare three implementations:</p><ol><li>zarr-python: Uses vanilla zarr-python for I/O and decoding, and NumPy for the computation.</li><li>zarr-python GPU: Uses zarr-python&rsquo;s <a href=https://zarr.readthedocs.io/en/stable/user-guide/gpu/>built-in GPU support</a> to return CuPy arrays, so the GPU is used for computation. At the moment, this still uses Numcodecs to decompress the data, which runs on the CPU. After decompression, the data is moved to the GPU for the matrix multiplication.</li><li>Custom GPU: My custom implementation of I/O and decoding with CuPy for the computation.</li></ol><table><thead><tr><th style=text-align:left>Implementation</th><th style=text-align:left>Duration (ms)</th></tr></thead><tbody><tr><td style=text-align:left>Zarr / NumPy</td><td style=text-align:left>19,892</td></tr><tr><td style=text-align:left>Zarr / CuPy</td><td style=text-align:left>3,407</td></tr><tr><td style=text-align:left>Custom / CuPy</td><td style=text-align:left>478</td></tr></tbody></table><p>You can find the code for these in my <a href=https://github.com/TomAugspurger/cuda-streams-sample/blob/8898c89f5ee2e38d0617a31f61f23b146253c842/zarr_shards.py>CUDA Stream Samples</a> repository.</p><p>Please don&rsquo;t take the absolute numbers, or even the relative numbers too
seriously. I&rsquo;ve spent <em>zero</em> time optimizing the Zarr/NumPy and Zarr/CuPy
implementations. The important thing to take away here is that we have plenty of
room for improvement. My Custom I/O pipeline just gradually removed bottlenecks
as they came up, some of which apply to zarr-python&rsquo;s CPU implementation as
well. Follow <a href=https://github.com/zarr-developers/zarr-python/issues/2904>https://github.com/zarr-developers/zarr-python/issues/2904</a> if
you&rsquo;re interested in developments.</p><p>The remainder of the post will describe, in some detail, what makes the custom
implementation so fast.</p><h3 id=performance-optimizations>Performance optimizations<a hidden class=anchor aria-hidden=true href=#performance-optimizations>#</a></h3><p>Once you have the basics down (using the right data structures / algorithm,
removing the most egregious overheads), speeding up a problem often involves
parallelization. And you very often have multiple levels of parallelization
available. Picking the right level is absolutely a skill that requires some
general knowledge about performance and specific details for your problem.</p><p>In this case, we&rsquo;ll operate at the <em>shard</em> level. This will be the maximum
amount of data we need to hold in memory at any point in time (though the
problem is small enough that we can operate on all the shards at the same time).</p><p>We&rsquo;ll use a few techniques to get good performance in our pipeline:</p><ol><li>No (large) memory allocations on the critical path.</li></ol><p>This applies to both host and device memory allocations. We&rsquo;ll achieve this by
preallocating all the arrays we need to process the shard. Whether or not this
should be considered cheating or not is a bit debatable and a bit workload
dependent. I&rsquo;d argue that the most advanced, performance-sensitive workloads
will process large amounts of data and so can preallocate a pool of buffers and
reuse them across their unit of parallelization (shards in our case).</p><p>Regardless, if we&rsquo;re doing large memory allocations after we&rsquo;ve started
processing a shard (either host or device allocations for the final array or for
intermediates) then these allocations can quickly become the bottleneck.
Pre-allocation (and reuse across shards) is an important optimization if it&rsquo;s
available.</p><ol start=2><li>Use pinned (page-locked) memory for host buffers</li></ol><p>Using <a href=https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/#pinned_host_memory>pinned memory</a> makes the host to device transfers much faster. More on that later.</p><ol start=3><li>Use CUDA streams to overlap I/O and Computation</li></ol><p>Our workload has a regular pattern of &ldquo;read, transfer, decode, compute&rdquo; on each
shard. Because these exercise different parts of the GPU (transfer uses the
memory subsystem, decode and compute launch kernels that run on the GPU&rsquo;s cores),
we can run them concurrently.</p><p>We&rsquo;ll assign a CUDA stream per shard. We&rsquo;ll be very careful to avoid stream /
device synchronizations so that our host program schedules all the work to be
done.</p><p>Throughout this, we&rsquo;ll use <a href=https://nvtx.readthedocs.io/en/latest/>nvtx</a> to annotate certain ranges of code. This will
make reading the <a href=https://developer.nvidia.com/nsight-systems>Nsight Systems</a> report easier.</p><p>Here&rsquo;s a screenshot of an nsys profile, with a few important bits highlighted
(open the file for a full-sized screenshot):</p><p><img loading=lazy src=https://assets.tomaugspurger.net/tomaugspurger/posts/gpu-accelerated-zarr/nsys-overview.png alt></p><ul><li>Under Processes > Threads > python, you see the traces for our <em>host</em> program,
in this case a Python program. This will include our <a href=https://nvtx.readthedocs.io/en/latest/>nvtx</a> annotations
(<code>read::disk</code>, <code>read::transfer</code>, <code>read::decode</code>, etc.) and calls to the CUDA API (e.g. <code>cudaMemcpyAsync</code>). These calls measure the time spent by the CPU / host program, not the GPU.</li></ul><p><img loading=lazy src=https://assets.tomaugspurger.net/tomaugspurger/posts/gpu-accelerated-zarr/nsys-gpu-detail-python.png alt></p><ul><li>Under Processes > CUDA HW, you&rsquo;ll see the corresponding traces for GPU operations. This shows
CUDA kernels (functions that run on the GPU) in light blue and memory operations (like host
to device transfers) in teal.</li></ul><p><img loading=lazy src=https://assets.tomaugspurger.net/tomaugspurger/posts/gpu-accelerated-zarr/nsys-gpu-detail-cuda.png alt></p><p>You can download the full nsight report <a href=https://assets.tomaugspurger.net/zarr-shards.nsys-rep>here</a> and open it locally with NVIDIA
Nsight Systems.</p><p>This table summarizes roughly where we spend our time on the GPU per shard (very
rough, and there&rsquo;s some variation across shards, especially as we start
overlapping operations with CUDA streams).</p><table><thead><tr><th style=text-align:left>Stage</th><th style=text-align:left>Duration (ms)</th><th style=text-align:left>Raw Throughput (GB/s)</th><th style=text-align:left>Effective Throughput (GB/s)</th></tr></thead><tbody><tr><td style=text-align:left>Read</td><td style=text-align:left>13.6</td><td style=text-align:left>5.7</td><td style=text-align:left>30.1</td></tr><tr><td style=text-align:left>Transfer</td><td style=text-align:left>1.5</td><td style=text-align:left>51.7</td><td style=text-align:left>273</td></tr><tr><td style=text-align:left>Decode</td><td style=text-align:left>45</td><td style=text-align:left>1.7</td><td style=text-align:left>9.1</td></tr><tr><td style=text-align:left>Compute</td><td style=text-align:left>150</td><td style=text-align:left>2.7</td><td style=text-align:left>2.7</td></tr></tbody></table><p>Raw throughput measures the actual number of bytes processed per time unit,
which is the compressed size for reading, transferring, and decoding.
&ldquo;Effective Throughput&rdquo; uses the uncompressed number of bytes for each stage.
After decompression the actual number of bytes processed equals the uncompressed
bytes, so <code>Compute</code>&rsquo;s raw throughput is equal to its effective throughput.</p><h3 id=read-bytes>Read bytes<a hidden class=anchor aria-hidden=true href=#read-bytes>#</a></h3><p>First, we need to load the data. In my example, I&rsquo;m just using files on a local
disk, though you could use remote object storage and <a href=https://earthmover.io/blog/i-o-maxing-tensors-in-the-cloud>still perform
well</a>. We&rsquo;ll parallelize things at the shard level (i.e. we&rsquo;re assuming
that the entirety of the shard fits in GPU memory).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>path <span style=color:#f92672>=</span> array<span style=color:#f92672>.</span>store_path<span style=color:#f92672>.</span>store<span style=color:#f92672>.</span>root <span style=color:#f92672>/</span> array<span style=color:#f92672>.</span>store_path<span style=color:#f92672>.</span>path <span style=color:#f92672>/</span> key
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(path, <span style=color:#e6db74>&#34;rb&#34;</span>) <span style=color:#66d9ef>as</span> f, nvtx<span style=color:#f92672>.</span>annotate(<span style=color:#e6db74>&#34;read::disk&#34;</span>):
</span></span><span style=display:flex><span>    f<span style=color:#f92672>.</span>readinto(host_buffer)
</span></span></code></pre></div><p>On my system, it takes about 13.6 ms to read the 77.5 MB, for a throughput of
about 5.7 GB/s from disk (the OS probably had at least some of the pages
cached). The effective throughput (uncompressed size over duration) is about
30.1 GB/s. I&rsquo;ll note that I haven&rsquo;t spent much effort optimizing this section.</p><p>Note that we use <code>readinto</code> to read the data from disk directly into the
pre-allocated host buffer: we don&rsquo;t want any (large) memory allocations on the
critical path. Also, we&rsquo;re using <a href=https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/#pinned_host_memory>pinned memory</a> (AKA page-locked
memory) for the host buffers. This prevents the operating system from paging the
buffers, which lets the GPU directly access that memory when copying it, no
intermediate buffers required.</p><p>And it&rsquo;s worth emphasizing: this I/O is happening on the host Python program,
and it is blocking. As we&rsquo;ll see later, time spent doing stuff in Python is
time <em>not</em> spent scheduling work on the GPU. We&rsquo;ll need to ensure that the GPU
is fed sufficient work, so let&rsquo;s keep our eye on this section.</p><p>The profile report for this section is pretty boring:</p><p><img loading=lazy src=https://assets.tomaugspurger.net/tomaugspurger/posts/gpu-accelerated-zarr/read-disk-initial.png alt></p><p>Note what the GPU is doing right now: nothing! There aren&rsquo;t any CUDA HW
annotations visible above the initial <code>read::disk</code>. At least for the very first
shard we read, the GPU is necessarily idle. But as we&rsquo;ll discuss shortly,
subsequent shards are able to overlap disk I/O with CUDA operations.</p><p>This screenshot shows the profile for the second shard:</p><p><img loading=lazy src=https://assets.tomaugspurger.net/tomaugspurger/posts/gpu-accelerated-zarr/read-disk-subsequent.png alt></p><p>Now the GPU is busy with some other operations (decoding the chunks from the
first shard in this case, which are directly above the <code>read::decode</code> happening
on the host at that time). This is partly why I didn&rsquo;t bother with parallelizing
the disk I/O: only one thing can be the bottleneck, and right now we&rsquo;re able to
load data from disk quickly enough.</p><h3 id=transfer-bytes>Transfer bytes<a hidden class=anchor aria-hidden=true href=#transfer-bytes>#</a></h3><p>After we&rsquo;ve read the bytes into memory, we <em>schedule</em> the host to device
transfer:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> nvtx<span style=color:#f92672>.</span>annotate(<span style=color:#e6db74>&#34;read::transfer&#34;</span>), stream:
</span></span><span style=display:flex><span>    <span style=color:#75715e># device_buffer is a pre-allocated cupy.ndarray</span>
</span></span><span style=display:flex><span>    device_buffer<span style=color:#f92672>.</span>set(
</span></span><span style=display:flex><span>        host_buffer[:<span style=color:#f92672>-</span>index_offset]<span style=color:#f92672>.</span>view(device_buffer<span style=color:#f92672>.</span>dtype), stream<span style=color:#f92672>=</span>stream
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>This is where our earlier discussion on blocking vs. non-blocking APIs comes
in handy. The <code>device_buffer.set</code> call is <em>not</em> blocking, which is why it
takes only ~60 μs on the host. It only makes the CUDA API call to set up
the transfer and then immediately returns back to the Python program (to
close our context managers and then continue to the next line in our program).</p><p>The actual memory copy (which is running on the device) takes about 1.5 ms for a
throughput of about 52 GB/s (this is still compressed data, so the effective
throughput is even higher). Here&rsquo;s the same profile I showed earlier, but now
you&rsquo;ll understand the context around what happens on the host (the CUDA API call
to do something) and device.</p><p><img loading=lazy src=https://assets.tomaugspurger.net/tomaugspurger/posts/gpu-accelerated-zarr/read-transfer-host-and-device.png alt></p><p>I&rsquo;ve added the orange lines connecting the fast <code>cudaMemcpyAsync</code> on the host to
the (not quite as fast) <code>Memcpy HtoD</code> (host to device) running on the device.</p><p>And if you look closely, you&rsquo;ll see that just above that <code>Memcpy HtoD</code> in teal,
we&rsquo;re executing a compute kernel (in light-blue). We&rsquo;ll get to that in a bit,
but this show that we&rsquo;re overlapping Host-to-Device transfers with compute
kernels.</p><h3 id=decode-bytes>Decode bytes<a hidden class=anchor aria-hidden=true href=#decode-bytes>#</a></h3><p>At this point we have (or will have, eventually) the Zstd compressed bytes in
GPU memory. You might think that &ldquo;decompressing a stream of bytes&rdquo; doesn&rsquo;t mesh
well with &ldquo;GPUs as massively parallel processors&rdquo;. And you&rsquo;d be (partially)
right! We can&rsquo;t really parallelize decoding <em>within</em> a single chunk, but we can
decode all the chunks in a shard in parallel. My colleague Akshay has a
<a href=https://github.com/zarr-developers/zarr-python/issues/1398#issuecomment-1563275350>nice overview</a>
of how the GPU can be used to decode <em>many</em> buffers in parallel.</p><p><em>I</em> have no idea how to implement a Zstd decompressor, but fortunately we don&rsquo;t have to.
The <a href=https://docs.nvidia.com/cuda/nvcomp/index.html>nvCOMP</a> library implements a bunch of GPU-accelerated compression
and decompression routines, including Zstd. It provides C, C++, and Python APIs.
A quick note: this example is using a <a href=https://github.com/TomAugspurger/cuda-streams-sample/blob/28be70d1bc9b6ba31058d5e8b96c8186753f3f54/nvcomp_minimal/nvcomp_minimal/zstd.pyx>custom wrapper</a> around
nvcomp&rsquo;s C API. This works around a couple issues with nvcomp&rsquo;s <a href=https://docs.nvidia.com/cuda/nvcomp/py_api.html>Python
bindings</a>.</p><ol><li>At the moment, accessing an attribute on the decompressed array returned by
nvcomp causes a &ldquo;stream synchronization&rdquo;. This forces essentially blocks
the host program from progressing until the GPU has caught up, which we&rsquo;d
like to avoid. We need to issue compute instructions still, and we&rsquo;d ideally
move on to the next shard!</li><li>We&rsquo;d like full control over all the memory allocations, including the ability
to preallocate the output buffers that the arrays should be decompressed into.
This is possible with the C API, but not (yet) the Python API.</li></ol><p>My custom wrapper is not at all robust, well designed, etc. It&rsquo;s just enough to
work for this demo. Don&rsquo;t use it! Use the official Python bindings, and reach
out to me or the nvcomp team if you run into any issues. But here&rsquo;s the basic
idea in code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>zstd_codec <span style=color:#f92672>=</span> ZstdCodec(stream<span style=color:#f92672>=</span>stream)
</span></span><span style=display:flex><span><span style=color:#75715e># get a list of arrays, each of which is a view into the original device buffer</span>
</span></span><span style=display:flex><span><span style=color:#75715e># `device_buffer` is stream-ordered on `stream`,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># so `device_arrays` are all stream-ordered on `stream`</span>
</span></span><span style=display:flex><span>device_arrays <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    device_buffer[offset : offset <span style=color:#f92672>+</span> size] <span style=color:#66d9ef>for</span> offset, size <span style=color:#f92672>in</span> index
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> nvtx<span style=color:#f92672>.</span>annotate(<span style=color:#e6db74>&#34;read::decode&#34;</span>):
</span></span><span style=display:flex><span>    zstd_codec<span style=color:#f92672>.</span>decode_batch(device_arrays, out<span style=color:#f92672>=</span>out_chunks)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># and now `out` is stream-ordered on `stream`</span>
</span></span></code></pre></div><p>The <code>zstd_codec.decode_batch</code> call takes about 2.4 ms on my machine. Again
this just <em>schedules</em> the decompression call.</p><p>The actual decompression takes about 25-45 ms, for a throughput of about roughly
1.7 GB/s.</p><p>Again, we&rsquo;ve pre-allocated the <code>out</code> ndarray, however <em>this is not always
possible</em>. Zarr allows chunking over arbitrary dimensions, but we&rsquo;ve assumed
that the chunks are contiguous slices of the output array<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. If
your chunks aren&rsquo;t contiguous slices of the output array, you&rsquo;ll need to
decode into an intermediate buffer and then perform some memory copies
into the output buffer.</p><p>Anyway, all this is to say that decompression isn&rsquo;t our bottleneck. And this is
despite decompression competing for GPU cores with the computation. The newer
NVIDIA Blackwell Architecture includes a dedicated <a href=https://developer.nvidia.com/blog/speeding-up-data-decompression-with-nvcomp-and-the-nvidia-blackwell-decompression-engine/>Decompression Engine</a>
which improves the decompression throughput even more.</p><p>And for those curious, a brief experiment without compression is about twice as
slow on the GPU as the version with compression, though I didn&rsquo;t investigate it
deeply.</p><h3 id=computation>Computation<a hidden class=anchor aria-hidden=true href=#computation>#</a></h3><p>This example is primarily focused on the data loading portion of a Zarr
workload, so the computation is secondary. I just threw in a bunch of matrix
multiplications / reductions (which GPUs tend to do quickly).</p><p>But while the specific computation is unimportant, there are some
characteristics to consider about <em>your</em> computation, it should take some
non-negligible amount of time, such it&rsquo;s worthwhile moving the data from the
host to the device for the computation (and moving the result back to the host).</p><p>The key thing we care about here is overlapping host to device copies with
compute, so that the GPU isn&rsquo;t sitting around waiting for data. Note how the
teal Host to Device Copy is running at the same time as the matrix
multiplication from the previous shard:</p><p><img loading=lazy src=https://assets.tomaugspurger.net/tomaugspurger/posts/gpu-accelerated-zarr/overlapping-hto2-and-compute.png alt></p><p>And at this point, you can start analyzing GPU metrics if you still need to squeeze additional performance out of your pipeline.</p><p><img loading=lazy src=https://assets.tomaugspurger.net/tomaugspurger/posts/gpu-accelerated-zarr/GPU-metrics.png alt></p><p>But I think that&rsquo;s enough for now.</p><h1 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h1><p>One takeaway here is that GPUs are fast, which, sure. A slightly more
interesting takeaway is that GPUs <em>can</em> be <em>extremely</em> fast, but achieving that
takes some care.</p><p>In this workload my <a href=https://github.com/TomAugspurger/cuda-streams-sample/blob/8898c89f5ee2e38d0617a31f61f23b146253c842/zarr_shards.py>custom pipeline</a> achieved high throughput by</p><ol><li>Being <em>very</em> careful with memory allocations and data movement.</li><li>Using pinned host memory to speed up the one host to device transfer per shard</li><li>Use nvcomp and Zarr shards to parallelize decoding many chunks on the GPU</li><li>Use CUDA streams to express our workloads&rsquo; shard-level parallelism, so that
we can overlap host I/O, host-to-device copies, kernel launches and kernel
execution.</li></ol><p>I&rsquo;m hopeful that we can optimize the codec pipeline and memory
handling in zarr-python to close the gap between what it provides and my
custom, hand-optimized implementation (0.5s). But doing that in a general
purpose library will require even more thought and care than my hacky
implementation.</p><p>If you&rsquo;ve made it this far, congrats. Reach out if you have any feedback, either
<a href=mailto:tom.w.augspurger@gmail.com>directly</a> or on the <a href=https://github.com/zarr-developers/zarr-python/discussions>Zarr
discussions</a> board.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>NVIDIA does have <a href=https://docs.nvidia.com/gpudirect-storage/index.html>GPU Direct Storage</a>
which offers a way to read directly from storage to the device, bypassing the host
(OS and memory system) entirely. I haven&rsquo;t tried using that yet.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Explaining that optimization in more detail. We need the chunks to be contiguous
in the shard. Consider this shard, with the letters indicating the chunks:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>| a a a a |
</span></span><span style=display:flex><span>| b b b b |
</span></span><span style=display:flex><span>| c c c c |
</span></span><span style=display:flex><span>| d d d d |
</span></span></code></pre></div><p>In C-contiguous order, that can be stored as:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>| a a a a b b b b c c c c d d d d|
</span></span></code></pre></div><p>i.e. all of the <code>a</code>&rsquo;s are together in a contiguous chunk. That
means we can tell nvcomp to write its output at this memory
address and it&rsquo;ll work out fine. Likewise for <code>b</code>, just offset
by some amount, and so on for the other chunks.</p><p>However, this chunking is not amenable to this optimization
because the chunks aren&rsquo;t contiguous in the shard:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain><span style=display:flex><span>| a a b b |
</span></span><span style=display:flex><span>| a a b b |
</span></span><span style=display:flex><span>| c c d d |
</span></span><span style=display:flex><span>| c c d d |
</span></span></code></pre></div><p>Maybe someone smarter than me could pull off something with stride tricks. But
for now, note that the ability to preallocate the output array might not always
be an option.</p><p>That&rsquo;s not necessarily a deal-killer: you&rsquo;ll just need a temporary buffer for the
decompressed output and an extra memcpy per chunk into the output shard.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://tomaugspurger.net/>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>