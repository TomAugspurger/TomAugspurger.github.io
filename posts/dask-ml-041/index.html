<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>dask-ml 0.4.1 Released | Tom's Blog</title>
<meta name=keywords content><meta name=description content="This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.
dask-ml 0.4.1 was released today with a few enhancements. See the changelog for all the changes from 0.4.0.
Conda packages are available on conda-forge
$ conda install -c conda-forge dask-ml and wheels and the source are available on PyPI
$ pip install dask-ml I wanted to highlight one change, that touches on a topic I mentioned in my first post on scalable Machine Learning."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/dask-ml-041/><link crossorigin=anonymous href=/assets/css/stylesheet.ced21e6d3497ee93fed8f8b357448095840179bd510b5ea0e6013078712e6dd1.css integrity="sha256-ztIebTSX7pP+2PizV0SAlYQBeb1RC16g5gEweHEubdE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tomaugspurger.net/posts/dask-ml-041/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="dask-ml 0.4.1 Released"><meta property="og:description" content="This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.
dask-ml 0.4.1 was released today with a few enhancements. See the changelog for all the changes from 0.4.0.
Conda packages are available on conda-forge
$ conda install -c conda-forge dask-ml and wheels and the source are available on PyPI
$ pip install dask-ml I wanted to highlight one change, that touches on a topic I mentioned in my first post on scalable Machine Learning."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/dask-ml-041/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-02-13T00:00:00+00:00"><meta property="article:modified_time" content="2018-02-13T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="dask-ml 0.4.1 Released"><meta name=twitter:description content="This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.
dask-ml 0.4.1 was released today with a few enhancements. See the changelog for all the changes from 0.4.0.
Conda packages are available on conda-forge
$ conda install -c conda-forge dask-ml and wheels and the source are available on PyPI
$ pip install dask-ml I wanted to highlight one change, that touches on a topic I mentioned in my first post on scalable Machine Learning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":2,"name":"dask-ml 0.4.1 Released","item":"https://tomaugspurger.net/posts/dask-ml-041/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"dask-ml 0.4.1 Released","name":"dask-ml 0.4.1 Released","description":"This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.\ndask-ml 0.4.1 was released today with a few enhancements. See the changelog for all the changes from 0.4.0.\nConda packages are available on conda-forge\n$ conda install -c conda-forge dask-ml and wheels and the source are available on PyPI\n$ pip install dask-ml I wanted to highlight one change, that touches on a topic I mentioned in my first post on scalable Machine Learning.","keywords":[],"articleBody":"This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.\ndask-ml 0.4.1 was released today with a few enhancements. See the changelog for all the changes from 0.4.0.\nConda packages are available on conda-forge\n$ conda install -c conda-forge dask-ml and wheels and the source are available on PyPI\n$ pip install dask-ml I wanted to highlight one change, that touches on a topic I mentioned in my first post on scalable Machine Learning. I discussed how, in my limited experience, a common workflow was to train on a small batch of data and predict for a much larger set of data. The training data easily fits in memory on a single machine, but the full dataset does not.\nA new meta-estimator, ParallelPostFit helps with this common case. It’s a meta-estimator that wraps a regular scikit-learn estimator, similar to how GridSearchCV wraps an estimator. The .fit method is very simple; it just calls the underlying estimator’s .fit method and copies over the learned attributes. This means ParalellPostFit is not suitable for training on large datasets. It is, however, perfect for post-fit tasks like .predict, or .transform.\nAs an example, we’ll fit a scikit-learn GradientBoostingClassifier on a small in-memory dataset.\n\u003e\u003e\u003e from sklearn.ensemble import GradientBoostingClassifier \u003e\u003e\u003e import sklearn.datasets \u003e\u003e\u003e import dask_ml.datasets \u003e\u003e\u003e X, y = sklearn.datasets.make_classification(n_samples=1000, ... random_state=0) \u003e\u003e\u003e clf = ParallelPostFit(estimator=GradientBoostingClassifier()) \u003e\u003e\u003e clf.fit(X, y) ParallelPostFit(estimator=GradientBoostingClassifier(...)) Nothing special so far. But now, let’s suppose we had a “large” dataset for prediction. We’ll use dask_ml.datasets.make_classification, but in practice you would read this from a file system or database.\n\u003e\u003e\u003e X_big, y_big = dask_ml.datasets.make_classification(n_samples=100000, chunks=1000, random_state=0) In this case we have a dataset with 100,000 samples split into blocks of 1,000. We can now predict for this large dataset.\n\u003e\u003e\u003e clf.predict(X) dask.array\u003cpredict, shape=(10000,), dtype=int64, chunksize=(1000,)\u003e Now things are different. ParallelPostFit.predict, .predict_proba, and .transform, all return dask arrays instead of immediately computing the result. We’ve built up task graph of computations to be performed, which allows dask to step in and compute things in parallel. When you’re ready for the answer, call compute:\n\u003e\u003e\u003e clf.predict_proba(X).compute() array([[0.99141094, 0.00858906], [0.93178389, 0.06821611], [0.99129105, 0.00870895], ..., [0.97996652, 0.02003348], [0.98087444, 0.01912556], [0.99407016, 0.00592984]]) At that point the dask scheduler comes in and executes your compute in parallel, using all the cores of your laptop or workstation, or all the machines on your cluster.\nParallelPostFit “fixes” a couple of issues in scikit-learn outside of scikit-learn itself\nparallel predict parallel transform If you’re able to depend on dask and dask-ml, consider giving ParallelPostFit a shot and let me know how it turns out. For estimators whose predict is relatively expensive and not already parallelized, ParallelPostFit can give a nice performance boost.\nEven if the underlying estimator’s predict or tranform method is cheap or parallelized, ParallelPostFit does still help with distributed the work on all the machines in your cluster, or doing the operation out-of-core.\nThanks to all the contributors who worked on this release.\n","wordCount":"490","inLanguage":"en","datePublished":"2018-02-13T00:00:00Z","dateModified":"2018-02-13T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/dask-ml-041/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net/ accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">dask-ml 0.4.1 Released</h1><div class=post-meta><span title='2018-02-13 00:00:00 +0000 UTC'>February 13, 2018</span></div></header><div class=post-content><p><em>This work is supported by <a href=http://anaconda.com/>Anaconda Inc</a> and the Data
Driven Discovery Initiative from the <a href=https://www.moore.org/>Moore
Foundation</a>.</em></p><p><a href=http://dask-ml.readthedocs.io/en/latest/>dask-ml</a> 0.4.1 was released today with a few enhancements. See the
<a href=http://dask-ml.readthedocs.io/en/latest/changelog.html>changelog</a> for all the changes from 0.4.0.</p><p>Conda packages are available on conda-forge</p><pre><code>$ conda install -c conda-forge dask-ml
</code></pre><p>and wheels and the source are available on PyPI</p><pre><code>$ pip install dask-ml
</code></pre><p>I wanted to highlight one change, that touches on a topic I mentioned in my
first post on <a href=scalable-ml-01>scalable Machine Learning</a>. I discussed how, in
my limited experience, a common workflow was to train on a small batch of data
and predict for a much larger set of data. The training data easily fits in
memory on a single machine, but the full dataset does not.</p><p>A new meta-estimator, <a href=http://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.wrappers.ParallelPostFit.html#dask_ml.wrappers.ParallelPostFit><code>ParallelPostFit</code></a> helps with this
common case. It&rsquo;s a meta-estimator that wraps a regular scikit-learn estimator,
similar to how <code>GridSearchCV</code> wraps an estimator. The <code>.fit</code> method is very
simple; it just calls the underlying estimator&rsquo;s <code>.fit</code> method and copies over
the learned attributes. This means <code>ParalellPostFit</code> is not suitable for
<em>training</em> on large datasets. It is, however, perfect for post-fit tasks like
<code>.predict</code>, or <code>.transform</code>.</p><p>As an example, we&rsquo;ll fit a scikit-learn <code>GradientBoostingClassifier</code> on a small
in-memory dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> GradientBoostingClassifier
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>import</span> sklearn.datasets
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>import</span> dask_ml.datasets
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> X, y <span style=color:#f92672>=</span> sklearn<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>make_classification(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>                                             random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> clf <span style=color:#f92672>=</span> ParallelPostFit(estimator<span style=color:#f92672>=</span>GradientBoostingClassifier())
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> clf<span style=color:#f92672>.</span>fit(X, y)
</span></span><span style=display:flex><span>ParallelPostFit(estimator<span style=color:#f92672>=</span>GradientBoostingClassifier(<span style=color:#f92672>...</span>))
</span></span></code></pre></div><p>Nothing special so far. But now, let&rsquo;s suppose we had a &ldquo;large&rdquo; dataset for
prediction. We&rsquo;ll use <code>dask_ml.datasets.make_classification</code>, but in practice
you would read this from a file system or database.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> X_big, y_big <span style=color:#f92672>=</span> dask_ml<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>make_classification(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>100000</span>,
</span></span><span style=display:flex><span>                                                        chunks<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>,
</span></span><span style=display:flex><span>                                                        random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p>In this case we have a dataset with 100,000 samples split into blocks of 1,000.
We can now predict for this large dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> clf<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>dask<span style=color:#f92672>.</span>array<span style=color:#f92672>&lt;</span>predict, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10000</span>,), dtype<span style=color:#f92672>=</span>int64, chunksize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1000</span>,)<span style=color:#f92672>&gt;</span>
</span></span></code></pre></div><p>Now things are different. <code>ParallelPostFit.predict</code>, <code>.predict_proba</code>, and
<code>.transform</code>, all return dask arrays instead of immediately computing the
result. We&rsquo;ve built up task graph of computations to be performed, which allows
dask to step in and compute things in parallel. When you&rsquo;re ready for the
answer, call <code>compute</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> clf<span style=color:#f92672>.</span>predict_proba(X)<span style=color:#f92672>.</span>compute()
</span></span><span style=display:flex><span>array([[<span style=color:#ae81ff>0.99141094</span>, <span style=color:#ae81ff>0.00858906</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.93178389</span>, <span style=color:#ae81ff>0.06821611</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.99129105</span>, <span style=color:#ae81ff>0.00870895</span>],
</span></span><span style=display:flex><span>       <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.97996652</span>, <span style=color:#ae81ff>0.02003348</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.98087444</span>, <span style=color:#ae81ff>0.01912556</span>],
</span></span><span style=display:flex><span>       [<span style=color:#ae81ff>0.99407016</span>, <span style=color:#ae81ff>0.00592984</span>]])
</span></span></code></pre></div><p>At that point the dask scheduler comes in and executes your compute in parallel,
using all the cores of your laptop or workstation, or all the machines on your
cluster.</p><p><code>ParallelPostFit</code> &ldquo;fixes&rdquo; a couple of issues in scikit-learn outside of
scikit-learn itself</p><ul><li><a href=https://github.com/scikit-learn/scikit-learn/issues/7448>parallel predict</a></li><li><a href=https://github.com/scikit-learn/scikit-learn/issues/7635>parallel transform</a></li></ul><p>If you&rsquo;re able to depend on dask and dask-ml, consider giving <code>ParallelPostFit</code>
a shot and let me know how it turns out. For estimators whose predict is
relatively expensive and not already parallelized, <code>ParallelPostFit</code> can give
a nice <a href=http://dask-ml.readthedocs.io/en/latest/auto_examples/plot_parallel_postfit.html#sphx-glr-auto-examples-plot-parallel-postfit-py>performance boost</a>.</p><p><img loading=lazy src=/images/sphx_glr_plot_parallel_postfit_001.png alt=parallel-post-fit></p><p>Even if the underlying estimator&rsquo;s <code>predict</code> or <code>tranform</code> method is cheap or
parallelized, <code>ParallelPostFit</code> does still help with distributed the work on all
the machines in your cluster, or doing the operation out-of-core.</p><p>Thanks to all the contributors who worked on this release.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://tomaugspurger.net/>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>