<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scalable Machine Learning (Part 1) | Tom's Blog</title><meta name=keywords content><meta name=description content="This work is supported by Anaconda Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
Anaconda is interested in scaling the scientific python ecosystem. My current focus is on out-of-core, parallel, and distributed machine learning. This series of posts will introduce those concepts, explore what we have available today, and track the community&rsquo;s efforts to push the boundaries.
You can download a Jupyter notebook demonstrating the analysis here."><meta name=author content><link rel=canonical href=https://tomaugspurger.github.io/posts/scalable-ml-01/><link crossorigin=anonymous href=/assets/css/stylesheet.67b4a5a78be69ca812d40e5543fbd83efd0b0cc5025c2505fd7758fd5d32ec2e.css integrity="sha256-Z7Slp4vmnKgS1A5VQ/vYPv0LDMUCXCUF/XdY/V0y7C4=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tomaugspurger.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Scalable Machine Learning (Part 1)"><meta property="og:description" content="This work is supported by Anaconda Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
Anaconda is interested in scaling the scientific python ecosystem. My current focus is on out-of-core, parallel, and distributed machine learning. This series of posts will introduce those concepts, explore what we have available today, and track the community&rsquo;s efforts to push the boundaries.
You can download a Jupyter notebook demonstrating the analysis here."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.github.io/posts/scalable-ml-01/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-09-11T00:00:00+00:00"><meta property="article:modified_time" content="2017-09-11T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scalable Machine Learning (Part 1)"><meta name=twitter:description content="This work is supported by Anaconda Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
Anaconda is interested in scaling the scientific python ecosystem. My current focus is on out-of-core, parallel, and distributed machine learning. This series of posts will introduce those concepts, explore what we have available today, and track the community&rsquo;s efforts to push the boundaries.
You can download a Jupyter notebook demonstrating the analysis here."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Scalable Machine Learning (Part 1)","item":"https://tomaugspurger.github.io/posts/scalable-ml-01/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scalable Machine Learning (Part 1)","name":"Scalable Machine Learning (Part 1)","description":"This work is supported by Anaconda Inc. and the Data Driven Discovery Initiative from the Moore Foundation.\nAnaconda is interested in scaling the scientific python ecosystem. My current focus is on out-of-core, parallel, and distributed machine learning. This series of posts will introduce those concepts, explore what we have available today, and track the community\u0026rsquo;s efforts to push the boundaries.\nYou can download a Jupyter notebook demonstrating the analysis here.","keywords":[],"articleBody":"This work is supported by Anaconda Inc. and the Data Driven Discovery Initiative from the Moore Foundation.\nAnaconda is interested in scaling the scientific python ecosystem. My current focus is on out-of-core, parallel, and distributed machine learning. This series of posts will introduce those concepts, explore what we have available today, and track the community’s efforts to push the boundaries.\nYou can download a Jupyter notebook demonstrating the analysis here.\nConstraints I am (or was, anyway) an economist, and economists like to think in terms of constraints. How are we constrained by scale? The two main ones I can think of are\nI’m constrained by size: My training dataset fits in RAM, but I have to predict for a much larger dataset. Or, my training dataset doesn’t even fit in RAM. I’d like to scale out by adopting algorithms that work in batches locally, or on a distributed cluster. I’m constrained by time: I’d like to fit more models (think hyper-parameter optimization or ensemble learning) on my dataset in a given amount of time. I’d like to scale out by fitting more models in parallel, either on my laptop by using more cores, or on a cluster. These aren’t mutually exclusive or exhaustive, but they should serve as a nice framework for our discussion. I’ll be showing where the usual pandas + scikit-learn for in-memory analytics workflow breaks down, and offer some solutions for scaling out to larger problems.\nThis post will focus on cases where your training dataset fits in memory, but you must predict on a dataset that’s larger than memory. Later posts will explore into parallel, out-of-core, and distributed training of machine learning models.\nDon’t forget your Statistics Statistics is a thing1. Statisticians have thought a lot about things like sampling and the variance of estimators. So it’s worth stating up front that you may be able to just\nSELECT * FROM dataset ORDER BY random() LIMIT 10000; and fit your model on a (representative) subset of your data. You may not need distributed machine learning. The tricky thing is selecting how large your sample should be. The “correct” value depends on the complexity of your learning task, the complexity of your model, and the nature of your data. The best you can do here is think carefully about your problem and to plot the learning curve.\nsource As usual, the scikit-learn developers do a great job explaining the concept in addition to providing a great library. I encourage you to follow that link. This gist is that—for some models on some datasets—training the model on more observations doesn’t improve performance. At some point the learning curve levels off and you’re just wasting time and money training on those extra observations.\nFor today, we’ll assume that we’re on the flat part of the learning curve. Later in the series we’ll explore cases where we run out of RAM before the learning curve levels off.\nFit, Predict In my experience, the first place I bump into RAM constraints is when my training dataset fits in memory, but I have to make predictions for a dataset that’s orders of magnitude larger. In these cases, I fit my model like normal, and do my predictions out-of-core (without reading the full dataset into memory at once).\nWe’ll see that the training side is completely normal (since everything fits in RAM). We’ll see that dask let’s us write normal-looking pandas and NumPy code, so we don’t have to worry about writing the batching code ourself.\nTo make this concrete, we’ll use the (tried and true) New York City taxi dataset. The goal will be to predict if the passenger leaves a tip. We’ll train the model on a single month’s worth of data (which fits in my laptop’s RAM), and predict on the full dataset2.\nLet’s load in the first month of data from disk:\ndtype = { 'vendor_name': 'category', 'Payment_Type': 'category', } df = pd.read_csv(\"data/yellow_tripdata_2009-01.csv\", dtype=dtype, parse_dates=['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime'],) df.head() vendor_name Trip_Pickup_DateTime Trip_Dropoff_DateTime Passenger_Count Trip_Distance Start_Lon Start_Lat Rate_Code store_and_forward End_Lon End_Lat Payment_Type Fare_Amt surcharge mta_tax Tip_Amt Tolls_Amt Total_Amt 0 VTS 2009-01-04 02:52:00 2009-01-04 03:02:00 1 2.63 -73.991957 40.721567 NaN NaN -73.993803 40.695922 CASH 8.9 0.5 NaN 0.00 0.0 9.40 1 VTS 2009-01-04 03:31:00 2009-01-04 03:38:00 3 4.55 -73.982102 40.736290 NaN NaN -73.955850 40.768030 Credit 12.1 0.5 NaN 2.00 0.0 14.60 2 VTS 2009-01-03 15:43:00 2009-01-03 15:57:00 5 10.35 -74.002587 40.739748 NaN NaN -73.869983 40.770225 Credit 23.7 0.0 NaN 4.74 0.0 28.44 3 DDS 2009-01-01 20:52:58 2009-01-01 21:14:00 1 5.00 -73.974267 40.790955 NaN NaN -73.996558 40.731849 CREDIT 14.9 0.5 NaN 3.05 0.0 18.45 4 DDS 2009-01-24 16:18:23 2009-01-24 16:24:56 1 0.40 -74.001580 40.719382 NaN NaN -74.008378 40.720350 CASH 3.7 0.0 NaN 0.00 0.0 3.70 The January 2009 file has about 14M rows, and pandas takes about a minute to read the CSV into memory. We’ll do the usual train-test split:\nX = df.drop(\"Tip_Amt\", axis=1) y = df['Tip_Amt'] \u003e 0 X_train, X_test, y_train, y_test = train_test_split(X, y) print(\"Train:\", len(X_train)) print(\"Test: \", len(X_test)) Train: 10569309 Test: 3523104 Aside on Pipelines The first time you’re introduced to scikit-learn, you’ll typically be shown how you pass two NumPy arrays X and y straight into an estimator’s .fit method.\nfrom sklearn.linear_model import LinearRegression est = LinearRegression() est.fit(X, y) Eventually, you might want to use some of scikit-learn’s pre-processing methods. For example, we might impute missing values with the median and normalize the data before handing it off to LinearRegression. You could do this “by hand”:\nfrom sklearn.preprocessing import Imputer, StandardScaler imputer = Imputer(strategy='median') X_filled = imputer.fit_transform(X, y) scaler = StandardScaler() X_scaled = X_scaler.fit_transform(X_filled, y) est = LinearRegression() est.fit(X_scaled, y) We set up each step, and manually pass the data through: X -\u003e X_filled -\u003e X_scaled.\nThe downside of this approach is that we now have to remember which pre-processing steps we did, and in what order. The pipeline from raw data to fit model is spread across multiple python objects. A better approach is to use scikit-learn’s Pipeline object.\nfrom sklearn.pipeline import make_pipeline pipe = make_pipeline( Imputer(strategy='median'), StandardScaler(), LinearRegression() ) pipe.fit(X, y) Each step in the pipeline implements the fit, transform, and fit_transform methods. Scikit-learn takes care of shepherding the data through the various transforms, and finally to the estimator at the end. Pipelines have many benefits but the main one for our purpose today is that it packages our entire task into a single python object. Later on, our predict step will be a single function call, which makes scaling out to the entire dataset extremely convenient.\nIf you want more information on Pipelines, check out the scikit-learn docs, this blog post, and my talk from PyData Chicago 2016. We’ll be implementing some custom ones, which is not the point of this post. Don’t get lost in the weeds here, I only include this section for completeness.\nOur Pipeline This isn’t a perfectly clean dataset, which is nice because it gives us a chance to demonstrate some pandas’ pre-processing prowess, before we hand the data of to scikit-learn to fit the model.\nfrom sklearn.pipeline import make_pipeline # We'll use FunctionTransformer for simple transforms from sklearn.preprocessing import FunctionTransformer # TransformerMixin gives us fit_transform for free from sklearn.base import TransformerMixin There are some minor differences in the spelling on “Payment Type”:\ndf.Payment_Type.cat.categories Index(['CASH', 'CREDIT', 'Cash', 'Credit', 'Dispute', 'No Charge'], dtype='object') We’ll reconcile that by lower-casing everything with a .str.lower(). But resist the temptation to just do that imperatively inplace! We’ll package it up into a function that will later be wrapped up in a FunctionTransformer.\ndef payment_lowerer(X): return X.assign(Payment_Type=X.Payment_Type.str.lower()) I should note here that I’m using .assign to update the variables since it implicitly copies the data. We don’t want to be modifying the caller’s data without their consent.\nNot all the columns look useful. We could have easily solved this by only reading in the data that we’re actually going to use, but let’s solve it now with another simple transformer:\nclass ColumnSelector(TransformerMixin): \"Select `columns` from `X`\" def __init__(self, columns): self.columns = columns def fit(self, X, y=None): return self def transform(self, X, y=None): return X[self.columns] Internally, pandas stores datetimes like Trip_Pickup_DateTime as a 64-bit integer representing the nanoseconds since some time in the 1600s. If we left this untransformed, scikit-learn would happily transform that column to its integer representation, which may not be the most meaningful item to stick in a linear model for predicting tips. A better feature might the hour of the day:\nclass HourExtractor(TransformerMixin): \"Transform each datetime in `columns` to integer hour of the day\" def __init__(self, columns): self.columns = columns def fit(self, X, y=None): return self def transform(self, X, y=None): return X.assign(**{col: lambda x: x[col].dt.hour for col in self.columns}) Likewise, we’ll need to ensure the categorical variables (in a statistical sense) are categorical dtype (in a pandas sense). We want categoricals so that we can call get_dummies later on without worrying about missing or extra categories in a subset of the data throwing off our linear algebra (See my talk for more details).\nclass CategoricalEncoder(TransformerMixin): \"\"\" Convert to Categorical with specific `categories`. Examples -------- \u003e\u003e\u003e CategoricalEncoder({\"A\": ['a', 'b', 'c']}).fit_transform( ... pd.DataFrame({\"A\": ['a', 'b', 'a', 'a']}) ... )['A'] 0 a 1 b 2 a 3 a Name: A, dtype: category Categories (2, object): [a, b, c] \"\"\" def __init__(self, categories): self.categories = categories def fit(self, X, y=None): return self def transform(self, X, y=None): X = X.copy() for col, categories in self.categories.items(): X[col] = X[col].astype('category').cat.set_categories(categories) return X Finally, we’d like to normalize the quantitative subset of the data. Scikit-learn has a StandardScaler, which we’ll mimic here, to just operate on a subset of the columns.\nclass StandardScaler(TransformerMixin): \"Scale a subset of the columns in a DataFrame\" def __init__(self, columns): self.columns = columns def fit(self, X, y=None): # Yes, non-ASCII symbols can be a valid identfiers in python 3 self.μs = X[self.columns].mean() self.σs = X[self.columns].std() return self def transform(self, X, y=None): X = X.copy() X[self.columns] = X[self.columns].sub(self.μs).div(self.σs) return X Side-note: I’d like to repeat my desire for a library of Transformers that work well on NumPy arrays, dask arrays, pandas DataFrames and dask dataframes. I think that’d be a popular library. Essentially everything we’ve written could go in there and be imported.\nNow we can build up the pipeline:\n# The columns at the start of the pipeline columns = ['vendor_name', 'Trip_Pickup_DateTime', 'Passenger_Count', 'Trip_Distance', 'Payment_Type', 'Fare_Amt', 'surcharge'] # The mapping of {column: set of categories} categories = { 'vendor_name': ['CMT', 'DDS', 'VTS'], 'Payment_Type': ['cash', 'credit', 'dispute', 'no charge'], } scale = ['Trip_Distance', 'Fare_Amt', 'surcharge'] pipe = make_pipeline( ColumnSelector(columns), HourExtractor(['Trip_Pickup_DateTime']), FunctionTransformer(payment_lowerer, validate=False), CategoricalEncoder(categories), FunctionTransformer(pd.get_dummies, validate=False), StandardScaler(scale), LogisticRegression(), ) pipe [('columnselector', \u003c__main__.ColumnSelector at 0x1a2c726d8\u003e), ('hourextractor', \u003c__main__.HourExtractor at 0x10dc72a90\u003e), ('functiontransformer-1', FunctionTransformer(accept_sparse=False, func=, inv_kw_args=None, inverse_func=None, kw_args=None, pass_y='deprecated', validate=False)), ('categoricalencoder', \u003c__main__.CategoricalEncoder at 0x11dd72f98\u003e), ('functiontransformer-2', FunctionTransformer(accept_sparse=False, func=, inv_kw_args=None, inverse_func=None, kw_args=None, pass_y='deprecated', validate=False)), ('standardscaler', \u003c__main__.StandardScaler at 0x162580a90\u003e), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False))] We can fit the pipeline as normal:\n%time pipe.fit(X_train, y_train) This take about a minute on my laptop. We can check the accuracy (but again, this isn’t the point)\n\u003e\u003e\u003e pipe.score(X_train, y_train) 0.9931 \u003e\u003e\u003e pipe.score(X_test, y_test) 0.9931 It turns out people essentially tip if and only if they’re paying with a card, so this isn’t a particularly difficult task. Or perhaps more accurately, tips are only recorded when someone pays with a card.\nScaling Out with Dask OK, so we’ve fit our model and it’s been basically normal. Maybe we’ve been overly-dogmatic about doing everything in a pipeline, but that’s just good model hygiene anyway.\nNow, to scale out to the rest of the dataset. We’ll predict the probability of tipping for every cab ride in the dataset (bearing in mind that the full dataset doesn’t fit in my laptop’s RAM, so we’ll do it out-of-core).\nTo make things a bit easier we’ll use dask, though it isn’t strictly necessary for this section. It saves us from writing a for loop (big whoop). Later on well see that we can, reuse this code when we go to scale out to a cluster (that part is pretty cool, actually). Dask can scale down to a single laptop, and up to thousands of cores.\nimport dask.dataframe as dd df = dd.read_csv(\"data/*.csv\", dtype=dtype, parse_dates=['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime'],) X = df.drop(\"Tip_Amt\", axis=1) X is a dask.dataframe, which can be mostly be treated like a pandas dataframe (internally, operations are done on many smaller dataframes). X has about 170M rows (compared with the 14M for the training dataset).\nSince scikit-learn isn’t dask-aware, we can’t simply call pipe.predict_proba(X). At some point, our dask.dataframe would be cast to a numpy.ndarray, and our memory would blow up. Fortunately, dask has some nice little escape hatches for dealing with functions that know how to operate on NumPy arrays, but not dask objects. In this case, we’ll use map_partitions.\nyhat = X.map_partitions(lambda x: pd.Series(pipe.predict_proba(x)[:, 1], name='yhat'), meta=('yhat', 'f8')) map_partitions will go through each partition in your dataframe (one per file), calling the function on each partition. Dask worries about stitching together the result (though we provide a hint with the meta keyword, to say that it’s a Series with name yhat and dtype f8).\nNow we can write it out to disk (using parquet rather than CSV, because CSVs are evil).\nyhat.to_frame().to_parquet(\"data/predictions.parq\") This takes about 9 minutes to finish on my laptop.\nScaling Out (even further) If 9 minutes is too long, and you happen to have a cluster sitting around, you can repurpose that dask code to run on the distributed scheduler. I’ll use dask-kubernetes, to start up a cluster on Google Cloud Platform, but you could also use dask-ec2 for AWS, or dask-drmaa or dask-yarn if already have access to a cluster from your business or institution.\ndask-kubernetes create scalable-ml This sets up a cluster with 8 workers and 54 GB of memory.\nThe next part of this post is a bit fuzzy, since your teams will probably have different procedures and infrastructure around persisting models. At my old job, I wrote a small utility for serializing a scikit-learn model along with some metadata about what it was trained on, before dumping it in S3. If you want to be fancy, you should watch this talk by Rob Story on how Stripe handles these things (it’s a bit more sophisticated than my “dump it on S3” script).\nFor this blog post, “shipping it to prod” consists of a joblib.dump(pipe, \"taxi-model.pkl\") on our laptop, and copying it to somewhere the cluster can load the file. Then on the cluster, we’ll load it up, and create a Client to communicate with our cluster’s workers.\nfrom distributed import Client from sklearn.externals import joblib pipe = joblib.load(\"taxi-model.pkl\") c = Client('dask-scheduler:8786') Depending on how your cluster is set up, specifically with respect to having a shared-file-system or not, the rest of the code is more-or-less identical. If we’re using S3 or Google Cloud Storage as our shared file system, we’d modify the loading code to read from S3 or GCS, rather than our local hard drive:\ndf = dd.read_csv(\"s3://bucket/yellow_tripdata_2009*.csv\", dtype=dtype, parse_dates=['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime'], storage_options={'anon': True}) df = c.persist(df) # persist the dataset in distributed memory # across all the workers in the Dataset X = df.drop(\"Tip_Amt\", axis=1) y = df['Tip_Amt'] \u003e 0 Computing the predictions is identical to our out-of-core-on-my-laptop code:\nyhat = X.map_partitions(lambda x: pd.Series(pipe.predict_proba(x)[:, 1], name='yhat'), meta=('yhat', 'f8')) And saving the data (say to S3) might look like\nyhat.to_parquet(\"s3://bucket/predictions.parq\") The loading took about 4 minutes on the cluster, the predict about 10 seconds, and the writing about 1 minute. Not bad overall.\nWrapup Today, we went into detail on what’s potentially the first scaling problem you’ll hit with scikit-learn: you can train your dataset in-memory (on a laptop, or a large workstation), but you have to predict on a much larger dataset.\nWe saw that the existing tools handle this case quite well. For training, we followed best-practices and did everything inside a Pipeline object. For predicting, we used dask to write regular pandas code that worked out-of-core on my laptop or on a distributed cluster.\nIf this topic interests you, you should watch this talk by Stephen Hoover on how Civis is scaling scikit-learn.\nIn future posts we’ll dig into\nhow dask can speed up your existing pipelines by executing them in parallel scikit-learn’s out of core API for when your training dataset doesn’t fit in memory using dask to implement distributed machine learning algorithms Until then I would really appreciate your feedback. My personal experience using scikit-learn and pandas can’t cover the diversity of use-cases they’re being thrown into. You can reach me on Twitter @TomAugspurger or by email at mailto:tom.w.augspurger@gmail.com. Thanks for reading!\np \u003c .05 ↩︎\nThis is a bad example, since there could be a time-trend or seasonality to the dataset. But our focus isn’t on building a good model, I hope you’ll forgive me. ↩︎\n","wordCount":"2789","inLanguage":"en","datePublished":"2017-09-11T00:00:00Z","dateModified":"2017-09-11T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.github.io/posts/scalable-ml-01/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.github.io accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.github.io/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.github.io/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Scalable Machine Learning (Part 1)</h1><div class=post-meta><span title='2017-09-11 00:00:00 +0000 UTC'>September 11, 2017</span></div></header><div class=post-content><p><em>This work is supported by <a href=https://www.anaconda.com/>Anaconda Inc.</a> and the Data Driven Discovery
Initiative from the <a href=https://www.moore.org/>Moore Foundation</a>.</em></p><p>Anaconda is interested in scaling the scientific python ecosystem. My current
focus is on out-of-core, parallel, and distributed machine learning. This series
of posts will introduce those concepts, explore what we have available today,
and track the community&rsquo;s efforts to push the boundaries.</p><p><em>You can download a Jupyter notebook demonstrating the analysis <a href=https://nbviewer.jupyter.org/gist/TomAugspurger/94ee62127bbc8e20223f97ebd7d29191>here</a>.</em></p><h2 id=constraints>Constraints<a hidden class=anchor aria-hidden=true href=#constraints>#</a></h2><p>I am (or was, anyway) an economist, and economists like to think in terms of
constraints. How are we constrained by scale? The two main ones I can think of
are</p><ol><li><strong>I&rsquo;m constrained by size</strong>: My training dataset fits in RAM, but I have to
predict for a much larger dataset. Or, my training dataset doesn&rsquo;t even fit
in RAM. <em>I&rsquo;d like to scale out by adopting algorithms that work in batches
locally, or on a distributed cluster.</em></li><li><strong>I&rsquo;m constrained by time</strong>: I&rsquo;d like to fit more models (think
hyper-parameter optimization or ensemble learning) on my dataset in a given
amount of time. <em>I&rsquo;d like to scale out by fitting more models in parallel,
either on my laptop by using more cores, or on a cluster.</em></li></ol><p>These aren&rsquo;t mutually exclusive or exhaustive, but they should serve as a nice
framework for our discussion. I&rsquo;ll be showing where the usual pandas +
scikit-learn for in-memory analytics workflow breaks down, and offer some
solutions for scaling out to larger problems.</p><p>This post will focus on cases where your <em>training</em> dataset fits in memory, but
you must predict on a dataset that&rsquo;s larger than memory. Later posts will
explore into parallel, out-of-core, and distributed training of machine learning
models.</p><h2 id=dont-forget-your-statistics>Don&rsquo;t forget your Statistics<a hidden class=anchor aria-hidden=true href=#dont-forget-your-statistics>#</a></h2><p>Statistics is a thing<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Statisticians have thought a lot about things like
sampling and the variance of estimators. So it&rsquo;s worth stating up front that
you may be able to just</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>SELECT</span> <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>FROM</span> dataset
</span></span><span style=display:flex><span><span style=color:#66d9ef>ORDER</span> <span style=color:#66d9ef>BY</span> random()
</span></span><span style=display:flex><span><span style=color:#66d9ef>LIMIT</span> <span style=color:#ae81ff>10000</span>;
</span></span></code></pre></div><p>and fit your model on a (representative) subset of your data. <em>You may not need
distributed machine learning</em>. The tricky thing is selecting how large your
sample should be. The &ldquo;correct&rdquo; value depends on the complexity of your learning
task, the complexity of your model, and the nature of your data. The best you
can do here is think carefully about your problem and to plot the <a href=http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html>learning
curve</a>.</p><p><img loading=lazy src=http://scikit-learn.org/stable/_images/sphx_glr_plot_learning_curve_001.png alt=scikit-learn></p><div style=text-align:center><a href=http://scikit-learn.org/stable/_images/sphx_glr_plot_learning_curve_001.png><i>source</i></a></div><p>As usual, the scikit-learn developers do a great job explaining the concept in
addition to providing a great library. I encourage you to follow <a href=http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html>that
link</a>. This gist is that—for some models on some
datasets—training the model on more observations doesn&rsquo;t improve performance. At
some point the learning curve levels off and you&rsquo;re just wasting time and money
training on those extra observations.</p><p>For today, we&rsquo;ll assume that we&rsquo;re on the flat part of the learning curve. Later
in the series we&rsquo;ll explore cases where we run out of RAM before the learning
curve levels off.</p><h2 id=fit-predict>Fit, Predict<a hidden class=anchor aria-hidden=true href=#fit-predict>#</a></h2><p>In my experience, the first place I bump into RAM constraints is when my
training dataset fits in memory, but I have to make predictions for a dataset
that&rsquo;s orders of magnitude larger. In these cases, I fit my model like normal,
and do my predictions out-of-core (without reading the full dataset into memory
at once).</p><p>We&rsquo;ll see that the training side is completely normal (since everything fits in
RAM). We&rsquo;ll see that <a href=https://dask.pydata.org>dask</a> let&rsquo;s us write normal-looking pandas and NumPy code,
so we don&rsquo;t have to worry about writing the batching code ourself.</p><p>To make this concrete, we&rsquo;ll use the (tried and true) New York City taxi
dataset. The goal will be to predict if the passenger leaves a tip. We&rsquo;ll train
the model on a single month&rsquo;s worth of data (which fits in my laptop&rsquo;s RAM), and
predict on the full dataset<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>Let&rsquo;s load in the first month of data from disk:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dtype <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;vendor_name&#39;</span>: <span style=color:#e6db74>&#39;category&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;Payment_Type&#39;</span>: <span style=color:#e6db74>&#39;category&#39;</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;data/yellow_tripdata_2009-01.csv&#34;</span>, dtype<span style=color:#f92672>=</span>dtype,
</span></span><span style=display:flex><span>                 parse_dates<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;Trip_Pickup_DateTime&#39;</span>, <span style=color:#e6db74>&#39;Trip_Dropoff_DateTime&#39;</span>],)
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><table><thead><tr style=text-align:right><th></th><th>vendor_name</th><th>Trip_Pickup_DateTime</th><th>Trip_Dropoff_DateTime</th><th>Passenger_Count</th><th>Trip_Distance</th><th>Start_Lon</th><th>Start_Lat</th><th>Rate_Code</th><th>store_and_forward</th><th>End_Lon</th><th>End_Lat</th><th>Payment_Type</th><th>Fare_Amt</th><th>surcharge</th><th>mta_tax</th><th>Tip_Amt</th><th>Tolls_Amt</th><th>Total_Amt</th></tr></thead><tbody><tr><th>0</th><td>VTS</td><td>2009-01-04 02:52:00</td><td>2009-01-04 03:02:00</td><td>1</td><td>2.63</td><td>-73.991957</td><td>40.721567</td><td>NaN</td><td>NaN</td><td>-73.993803</td><td>40.695922</td><td>CASH</td><td>8.9</td><td>0.5</td><td>NaN</td><td>0.00</td><td>0.0</td><td>9.40</td></tr><tr><th>1</th><td>VTS</td><td>2009-01-04 03:31:00</td><td>2009-01-04 03:38:00</td><td>3</td><td>4.55</td><td>-73.982102</td><td>40.736290</td><td>NaN</td><td>NaN</td><td>-73.955850</td><td>40.768030</td><td>Credit</td><td>12.1</td><td>0.5</td><td>NaN</td><td>2.00</td><td>0.0</td><td>14.60</td></tr><tr><th>2</th><td>VTS</td><td>2009-01-03 15:43:00</td><td>2009-01-03 15:57:00</td><td>5</td><td>10.35</td><td>-74.002587</td><td>40.739748</td><td>NaN</td><td>NaN</td><td>-73.869983</td><td>40.770225</td><td>Credit</td><td>23.7</td><td>0.0</td><td>NaN</td><td>4.74</td><td>0.0</td><td>28.44</td></tr><tr><th>3</th><td>DDS</td><td>2009-01-01 20:52:58</td><td>2009-01-01 21:14:00</td><td>1</td><td>5.00</td><td>-73.974267</td><td>40.790955</td><td>NaN</td><td>NaN</td><td>-73.996558</td><td>40.731849</td><td>CREDIT</td><td>14.9</td><td>0.5</td><td>NaN</td><td>3.05</td><td>0.0</td><td>18.45</td></tr><tr><th>4</th><td>DDS</td><td>2009-01-24 16:18:23</td><td>2009-01-24
16:24:56</td><td>1</td><td>0.40</td><td>-74.001580</td><td>40.719382</td><td>NaN</td><td>NaN</td><td>-74.008378</td><td>40.720350</td><td>CASH</td><td>3.7</td><td>0.0</td><td>NaN</td><td>0.00</td><td>0.0</td><td>3.70</td></tr></tbody></table><p>The January 2009 file has about 14M rows, and pandas takes about a minute to
read the CSV into memory. We&rsquo;ll do the usual train-test split:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>X <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>drop(<span style=color:#e6db74>&#34;Tip_Amt&#34;</span>, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;Tip_Amt&#39;</span>] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(X, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Train:&#34;</span>, len(X_train))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Test: &#34;</span>, len(X_test))
</span></span></code></pre></div><pre><code>Train: 10569309
Test:  3523104
</code></pre><h2 id=aside-on-pipelines>Aside on Pipelines<a hidden class=anchor aria-hidden=true href=#aside-on-pipelines>#</a></h2><p>The first time you&rsquo;re introduced to scikit-learn, you&rsquo;ll typically be shown how
you pass two NumPy arrays <code>X</code> and <code>y</code> straight into an estimator&rsquo;s <code>.fit</code>
method.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LinearRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>est <span style=color:#f92672>=</span> LinearRegression()
</span></span><span style=display:flex><span>est<span style=color:#f92672>.</span>fit(X, y)
</span></span></code></pre></div><p>Eventually, you might want to use some of scikit-learn&rsquo;s pre-processing methods.
For example, we might impute missing values with the median and normalize the
data before handing it off to <code>LinearRegression</code>. You could do this &ldquo;by hand&rdquo;:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> Imputer, StandardScaler
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>imputer <span style=color:#f92672>=</span> Imputer(strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;median&#39;</span>)
</span></span><span style=display:flex><span>X_filled <span style=color:#f92672>=</span> imputer<span style=color:#f92672>.</span>fit_transform(X, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>scaler <span style=color:#f92672>=</span> StandardScaler()
</span></span><span style=display:flex><span>X_scaled <span style=color:#f92672>=</span> X_scaler<span style=color:#f92672>.</span>fit_transform(X_filled, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>est <span style=color:#f92672>=</span> LinearRegression()
</span></span><span style=display:flex><span>est<span style=color:#f92672>.</span>fit(X_scaled, y)
</span></span></code></pre></div><p>We set up each step, and manually pass the data through: <code>X -> X_filled -> X_scaled</code>.</p><p>The downside of this approach is that we now have to remember which
pre-processing steps we did, and in what order. The pipeline from raw data to
fit model is spread across multiple python objects. A better approach is to use
scikit-learn&rsquo;s <code>Pipeline</code> object.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.pipeline <span style=color:#f92672>import</span> make_pipeline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> make_pipeline(
</span></span><span style=display:flex><span>    Imputer(strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;median&#39;</span>),
</span></span><span style=display:flex><span>    StandardScaler(),
</span></span><span style=display:flex><span>    LinearRegression()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>pipe<span style=color:#f92672>.</span>fit(X, y)
</span></span></code></pre></div><p>Each step in the pipeline implements the <code>fit</code>, <code>transform</code>, and <code>fit_transform</code>
methods. Scikit-learn takes care of shepherding the data through the various
transforms, and finally to the estimator at the end. Pipelines have many
benefits but the main one for our purpose today is that it packages our entire
task into a single python object. Later on, our <code>predict</code> step will be a single
function call, which makes scaling out to the entire dataset extremely
convenient.</p><p>If you want more information on <code>Pipeline</code>s, check out the <a href=http://scikit-learn.org/stable/modules/pipeline.html#pipeline>scikit-learn
docs</a>, <a href=http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html>this blog</a> post, and my talk from
<a href="https://www.youtube.com/watch?v=KLPtEBokqQ0">PyData Chicago 2016</a>. We&rsquo;ll be implementing some custom ones,
which is <em>not</em> the point of this post. Don&rsquo;t get lost in the weeds here, I only
include this section for completeness.</p><h2 id=our-pipeline>Our Pipeline<a hidden class=anchor aria-hidden=true href=#our-pipeline>#</a></h2><p>This isn&rsquo;t a perfectly clean dataset, which is nice because it gives us a chance
to demonstrate some pandas&rsquo; pre-processing prowess, before we hand the data
of to scikit-learn to fit the model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.pipeline <span style=color:#f92672>import</span> make_pipeline
</span></span><span style=display:flex><span><span style=color:#75715e># We&#39;ll use FunctionTransformer for simple transforms</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> FunctionTransformer
</span></span><span style=display:flex><span><span style=color:#75715e># TransformerMixin gives us fit_transform for free</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.base <span style=color:#f92672>import</span> TransformerMixin
</span></span></code></pre></div><p>There are some minor differences in the spelling on &ldquo;Payment Type&rdquo;:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df<span style=color:#f92672>.</span>Payment_Type<span style=color:#f92672>.</span>cat<span style=color:#f92672>.</span>categories
</span></span></code></pre></div><pre><code>Index(['CASH', 'CREDIT', 'Cash', 'Credit', 'Dispute', 'No Charge'], dtype='object')
</code></pre><p>We&rsquo;ll reconcile that by lower-casing everything with a <code>.str.lower()</code>. But
resist the temptation to just do that imperatively inplace! We&rsquo;ll package it up
into a function that will later be wrapped up in a <a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html>FunctionTransformer</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>payment_lowerer</span>(X):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> X<span style=color:#f92672>.</span>assign(Payment_Type<span style=color:#f92672>=</span>X<span style=color:#f92672>.</span>Payment_Type<span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>lower())
</span></span></code></pre></div><p>I should note here that I&rsquo;m using
<a href=https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html><code>.assign</code></a>
to update the variables since it implicitly copies the data. We don&rsquo;t want to
be modifying the caller&rsquo;s data without their consent.</p><p>Not all the columns look useful. We could have easily solved this by only
reading in the data that we&rsquo;re actually going to use, but let&rsquo;s solve it now
with another simple transformer:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ColumnSelector</span>(TransformerMixin):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Select `columns` from `X`&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, columns):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>columns <span style=color:#f92672>=</span> columns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> X[self<span style=color:#f92672>.</span>columns]
</span></span></code></pre></div><p>Internally, pandas stores <code>datetimes</code> like <code>Trip_Pickup_DateTime</code> as a 64-bit
integer representing the nanoseconds since some time in the 1600s. If we left
this untransformed, scikit-learn would happily transform that column to its
integer representation, which may not be the most meaningful item to stick in
a linear model for predicting tips. A better feature might the hour of the day:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>HourExtractor</span>(TransformerMixin):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Transform each datetime in `columns` to integer hour of the day&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, columns):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>columns <span style=color:#f92672>=</span> columns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> X<span style=color:#f92672>.</span>assign(<span style=color:#f92672>**</span>{col: <span style=color:#66d9ef>lambda</span> x: x[col]<span style=color:#f92672>.</span>dt<span style=color:#f92672>.</span>hour
</span></span><span style=display:flex><span>                           <span style=color:#66d9ef>for</span> col <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>columns})
</span></span></code></pre></div><p>Likewise, we&rsquo;ll need to ensure the categorical variables (in a statistical
sense) are categorical dtype (in a pandas sense). We want categoricals so that
we can call <code>get_dummies</code> later on without worrying about missing or extra
categories in a subset of the data throwing off our linear algebra (See my
<a href="https://www.youtube.com/watch?v=KLPtEBokqQ0">talk</a> for more details).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CategoricalEncoder</span>(TransformerMixin):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Convert to Categorical with specific `categories`.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Examples
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    --------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &gt;&gt;&gt; CategoricalEncoder({&#34;A&#34;: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]}).fit_transform(
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ...     pd.DataFrame({&#34;A&#34;: [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;]})
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ... )[&#39;A&#39;]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    0    a
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    1    b
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    2    a
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    3    a
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Name: A, dtype: category
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Categories (2, object): [a, b, c]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, categories):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>categories <span style=color:#f92672>=</span> categories
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        X <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> col, categories <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>categories<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>            X[col] <span style=color:#f92672>=</span> X[col]<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#39;category&#39;</span>)<span style=color:#f92672>.</span>cat<span style=color:#f92672>.</span>set_categories(categories)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> X
</span></span></code></pre></div><p>Finally, we&rsquo;d like to normalize the quantitative subset of the data.
Scikit-learn has a <a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html>StandardScaler</a>, which we&rsquo;ll mimic here, to just operate on
a subset of the columns.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>StandardScaler</span>(TransformerMixin):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Scale a subset of the columns in a DataFrame&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, columns):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>columns <span style=color:#f92672>=</span> columns
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Yes, non-ASCII symbols can be a valid identfiers in python 3</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>μs <span style=color:#f92672>=</span> X[self<span style=color:#f92672>.</span>columns]<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>σs <span style=color:#f92672>=</span> X[self<span style=color:#f92672>.</span>columns]<span style=color:#f92672>.</span>std()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, X, y<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        X <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>        X[self<span style=color:#f92672>.</span>columns] <span style=color:#f92672>=</span> X[self<span style=color:#f92672>.</span>columns]<span style=color:#f92672>.</span>sub(self<span style=color:#f92672>.</span>μs)<span style=color:#f92672>.</span>div(self<span style=color:#f92672>.</span>σs)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> X
</span></span></code></pre></div><p>Side-note: I&rsquo;d like to repeat my desire for a library of <code>Transformers</code> that
work well on NumPy arrays, dask arrays, pandas <code>DataFrame</code>s and dask dataframes.
I think that&rsquo;d be a popular library. Essentially everything we&rsquo;ve written could
go in there and be imported.</p><p>Now we can build up the pipeline:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># The columns at the start of the pipeline</span>
</span></span><span style=display:flex><span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;vendor_name&#39;</span>, <span style=color:#e6db74>&#39;Trip_Pickup_DateTime&#39;</span>,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#39;Passenger_Count&#39;</span>, <span style=color:#e6db74>&#39;Trip_Distance&#39;</span>,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#39;Payment_Type&#39;</span>, <span style=color:#e6db74>&#39;Fare_Amt&#39;</span>, <span style=color:#e6db74>&#39;surcharge&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># The mapping of {column: set of categories}</span>
</span></span><span style=display:flex><span>categories <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;vendor_name&#39;</span>: [<span style=color:#e6db74>&#39;CMT&#39;</span>, <span style=color:#e6db74>&#39;DDS&#39;</span>, <span style=color:#e6db74>&#39;VTS&#39;</span>],
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;Payment_Type&#39;</span>: [<span style=color:#e6db74>&#39;cash&#39;</span>, <span style=color:#e6db74>&#39;credit&#39;</span>, <span style=color:#e6db74>&#39;dispute&#39;</span>, <span style=color:#e6db74>&#39;no charge&#39;</span>],
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>scale <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;Trip_Distance&#39;</span>, <span style=color:#e6db74>&#39;Fare_Amt&#39;</span>, <span style=color:#e6db74>&#39;surcharge&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> make_pipeline(
</span></span><span style=display:flex><span>    ColumnSelector(columns),
</span></span><span style=display:flex><span>    HourExtractor([<span style=color:#e6db74>&#39;Trip_Pickup_DateTime&#39;</span>]),
</span></span><span style=display:flex><span>    FunctionTransformer(payment_lowerer, validate<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),
</span></span><span style=display:flex><span>    CategoricalEncoder(categories),
</span></span><span style=display:flex><span>    FunctionTransformer(pd<span style=color:#f92672>.</span>get_dummies, validate<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),
</span></span><span style=display:flex><span>    StandardScaler(scale),
</span></span><span style=display:flex><span>    LogisticRegression(),
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>pipe
</span></span></code></pre></div><pre><code>[('columnselector', &lt;__main__.ColumnSelector at 0x1a2c726d8&gt;),
 ('hourextractor', &lt;__main__.HourExtractor at 0x10dc72a90&gt;),
 ('functiontransformer-1', FunctionTransformer(accept_sparse=False,
           func=&lt;function payment_lowerer at 0x17e0d5510&gt;, inv_kw_args=None,
           inverse_func=None, kw_args=None, pass_y='deprecated',
           validate=False)),
 ('categoricalencoder', &lt;__main__.CategoricalEncoder at 0x11dd72f98&gt;),
 ('functiontransformer-2', FunctionTransformer(accept_sparse=False,
           func=&lt;function get_dummies at 0x10f43b0d0&gt;, inv_kw_args=None,
           inverse_func=None, kw_args=None, pass_y='deprecated',
           validate=False)),
 ('standardscaler', &lt;__main__.StandardScaler at 0x162580a90&gt;),
 ('logisticregression',
  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
           verbose=0, warm_start=False))]
</code></pre><p>We can fit the pipeline as normal:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>time pipe<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span></code></pre></div><p>This take about a minute on my laptop. We can check the accuracy (but again,
this isn&rsquo;t the point)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> pipe<span style=color:#f92672>.</span>score(X_train, y_train)
</span></span><span style=display:flex><span><span style=color:#ae81ff>0.9931</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> pipe<span style=color:#f92672>.</span>score(X_test, y_test)
</span></span><span style=display:flex><span><span style=color:#ae81ff>0.9931</span>
</span></span></code></pre></div><p>It turns out people essentially tip if and only if they&rsquo;re paying with a card,
so this isn&rsquo;t a particularly difficult task. Or perhaps more accurately, tips
are only <em>recorded</em> when someone pays with a card.</p><h2 id=scaling-out-with-dask>Scaling Out with Dask<a hidden class=anchor aria-hidden=true href=#scaling-out-with-dask>#</a></h2><p>OK, so we&rsquo;ve fit our model and it&rsquo;s been basically normal. Maybe we&rsquo;ve been
overly-dogmatic about doing <em>everything</em> in a pipeline, but that&rsquo;s just good
model hygiene anyway.</p><p>Now, to scale out to the rest of the dataset. We&rsquo;ll predict the probability of
tipping for every cab ride in the dataset (bearing in mind that the full dataset
doesn&rsquo;t fit in my laptop&rsquo;s RAM, so we&rsquo;ll do it out-of-core).</p><p>To make things a bit easier we&rsquo;ll use dask, though it isn&rsquo;t strictly necessary
for this section. It saves us from writing a for loop (big whoop). Later on well
see that we can, reuse this code when we go to scale out to a cluster (that part
is pretty cool, actually). Dask can scale down to a single laptop, and up to
thousands of cores.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> dask.dataframe <span style=color:#66d9ef>as</span> dd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> dd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;data/*.csv&#34;</span>, dtype<span style=color:#f92672>=</span>dtype,
</span></span><span style=display:flex><span>                 parse_dates<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;Trip_Pickup_DateTime&#39;</span>, <span style=color:#e6db74>&#39;Trip_Dropoff_DateTime&#39;</span>],)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>drop(<span style=color:#e6db74>&#34;Tip_Amt&#34;</span>, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p><code>X</code> is a <code>dask.dataframe</code>, which can be mostly be treated like a pandas
dataframe (internally, operations are done on many smaller dataframes). <code>X</code> has
about 170M rows (compared with the 14M for the training dataset).</p><p>Since scikit-learn isn&rsquo;t dask-aware, we can&rsquo;t simply call
<code>pipe.predict_proba(X)</code>. At some point, our <code>dask.dataframe</code> would be cast to a
<code>numpy.ndarray</code>, and our memory would blow up. Fortunately, dask has some nice
little escape hatches for dealing with functions that know how to operate on
NumPy arrays, but not dask objects. In this case, we&rsquo;ll use <code>map_partitions</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>yhat <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>map_partitions(<span style=color:#66d9ef>lambda</span> x: pd<span style=color:#f92672>.</span>Series(pipe<span style=color:#f92672>.</span>predict_proba(x)[:, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>                                            name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;yhat&#39;</span>),
</span></span><span style=display:flex><span>                        meta<span style=color:#f92672>=</span>(<span style=color:#e6db74>&#39;yhat&#39;</span>, <span style=color:#e6db74>&#39;f8&#39;</span>))
</span></span></code></pre></div><p><code>map_partitions</code> will go through each partition in your dataframe (one per
file), calling the function on each partition. Dask worries about stitching
together the result (though we provide a hint with the <code>meta</code> keyword, to say
that it&rsquo;s a <code>Series</code> with name <code>yhat</code> and dtype <code>f8</code>).</p><p>Now we can write it out to disk (using parquet rather than CSV, because CSVs are
evil).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>yhat<span style=color:#f92672>.</span>to_frame()<span style=color:#f92672>.</span>to_parquet(<span style=color:#e6db74>&#34;data/predictions.parq&#34;</span>)
</span></span></code></pre></div><p>This takes about 9 minutes to finish on my laptop.</p><h2 id=scaling-out-even-further>Scaling Out (even further)<a hidden class=anchor aria-hidden=true href=#scaling-out-even-further>#</a></h2><p>If 9 minutes is too long, and you happen to have a cluster sitting around, you
can repurpose that dask code to run on the <a href=http://distributed.readthedocs.io/en/latest/>distributed scheduler</a>. I&rsquo;ll use
<a href=https://github.com/dask/dask-kubernetes>dask-kubernetes</a>, to start up a cluster on Google Cloud Platform, but you could
also use <a href=https://github.com/dask/dask-ec2>dask-ec2</a> for AWS, or <a href=https://github.com/dask/dask-drmaa>dask-drmaa</a> or <a href=https://github.com/dask/dask-yarn>dask-yarn</a> if already have
access to a cluster from your business or institution.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dask<span style=color:#f92672>-</span>kubernetes create scalable<span style=color:#f92672>-</span>ml
</span></span></code></pre></div><p>This sets up a cluster with 8 workers and 54 GB of memory.</p><p>The next part of this post is a bit fuzzy, since your teams will probably have
different procedures and infrastructure around persisting models. At my old job,
I wrote a small utility for serializing a scikit-learn model along with some
metadata about what it was trained on, before dumping it in S3. If you want to
be fancy, you should watch <a href="https://www.youtube.com/watch?v=vKU8MWORHP8">this talk</a>
by <a href=https://twitter.com/oceankidbilly>Rob Story</a> on how Stripe handles these things
(it&rsquo;s a bit more sophisticated than my &ldquo;dump it on S3&rdquo; script).</p><p>For this blog post, &ldquo;shipping it to prod&rdquo; consists of a <code>joblib.dump(pipe, "taxi-model.pkl")</code> on our laptop, and copying it to somewhere the cluster can
load the file. Then on the cluster, we&rsquo;ll load it up, and create a <code>Client</code> to
communicate with our cluster&rsquo;s workers.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> distributed <span style=color:#f92672>import</span> Client
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.externals <span style=color:#f92672>import</span> joblib
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> joblib<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;taxi-model.pkl&#34;</span>)
</span></span><span style=display:flex><span>c <span style=color:#f92672>=</span> Client(<span style=color:#e6db74>&#39;dask-scheduler:8786&#39;</span>)
</span></span></code></pre></div><p>Depending on how your cluster is set up, specifically with respect to having a
shared-file-system or not, the rest of the code is more-or-less identical. If
we&rsquo;re using S3 or Google Cloud Storage as our shared file system, we&rsquo;d modify
the loading code to read from S3 or GCS, rather than our local hard drive:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df <span style=color:#f92672>=</span> dd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#34;s3://bucket/yellow_tripdata_2009*.csv&#34;</span>,
</span></span><span style=display:flex><span>                 dtype<span style=color:#f92672>=</span>dtype,
</span></span><span style=display:flex><span>                 parse_dates<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;Trip_Pickup_DateTime&#39;</span>, <span style=color:#e6db74>&#39;Trip_Dropoff_DateTime&#39;</span>],
</span></span><span style=display:flex><span>                 storage_options<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;anon&#39;</span>: <span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> c<span style=color:#f92672>.</span>persist(df)  <span style=color:#75715e># persist the dataset in distributed memory</span>
</span></span><span style=display:flex><span>                    <span style=color:#75715e># across all the workers in the Dataset</span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>drop(<span style=color:#e6db74>&#34;Tip_Amt&#34;</span>, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;Tip_Amt&#39;</span>] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>
</span></span></code></pre></div><p>Computing the predictions is identical to our out-of-core-on-my-laptop code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>yhat <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>map_partitions(<span style=color:#66d9ef>lambda</span> x: pd<span style=color:#f92672>.</span>Series(pipe<span style=color:#f92672>.</span>predict_proba(x)[:, <span style=color:#ae81ff>1</span>], name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;yhat&#39;</span>),
</span></span><span style=display:flex><span>                        meta<span style=color:#f92672>=</span>(<span style=color:#e6db74>&#39;yhat&#39;</span>, <span style=color:#e6db74>&#39;f8&#39;</span>))
</span></span></code></pre></div><p>And saving the data (say to S3) might look like</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>yhat<span style=color:#f92672>.</span>to_parquet(<span style=color:#e6db74>&#34;s3://bucket/predictions.parq&#34;</span>)
</span></span></code></pre></div><p>The loading took about 4 minutes on the cluster, the predict about 10 seconds,
and the writing about 1 minute. Not bad overall.</p><h2 id=wrapup>Wrapup<a hidden class=anchor aria-hidden=true href=#wrapup>#</a></h2><p>Today, we went into detail on what&rsquo;s potentially the first scaling problem
you&rsquo;ll hit with scikit-learn: you can train your dataset in-memory (on a laptop,
or a large workstation), but you have to predict on a much larger dataset.</p><p>We saw that the existing tools handle this case quite well. For training, we
followed best-practices and did everything inside a <code>Pipeline</code> object. For
predicting, we used <code>dask</code> to write regular pandas code that worked out-of-core
on my laptop or on a distributed cluster.</p><p>If this topic interests you, you should watch <a href="https://www.youtube.com/watch?v=KqKEttfQ_hE">this talk</a>
by <a href=https://twitter.com/stephenactual>Stephen Hoover</a> on how Civis is scaling scikit-learn.</p><p>In future posts we&rsquo;ll dig into</p><ul><li>how dask can speed up your existing pipelines by executing them in parallel</li><li>scikit-learn&rsquo;s out of core API for when your training dataset doesn&rsquo;t fit in
memory</li><li>using dask to implement distributed machine learning algorithms</li></ul><p>Until then I would <em>really</em> appreciate your feedback. My personal experience
using scikit-learn and pandas can&rsquo;t cover the diversity of use-cases they&rsquo;re
being thrown into. You can reach me on Twitter
<a href=https://twitter.com/TomAugspurger>@TomAugspurger</a> or by email at
<a href=mailto:tom.w.augspurger@gmail.com>mailto:tom.w.augspurger@gmail.com</a>. Thanks for reading!</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>p &lt; .05&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>This is a bad example, since there could be a time-trend or seasonality to
the dataset. But our focus isn&rsquo;t on building a good model, I hope you&rsquo;ll
forgive me.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://tomaugspurger.github.io>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>