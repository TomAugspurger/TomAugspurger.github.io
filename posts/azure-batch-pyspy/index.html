<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>py-spy in Azure Batch | Tom's Blog</title>
<meta name=keywords content><meta name=description content="Today, I was debugging a hanging task in Azure Batch. This short post records how I used py-spy to investigate the problem.
Background Azure Batch is a compute service that we use to run container workloads. In this case, we start up a container that processes a bunch of GOES-GLM data to create STAC items for the Planetary Computer . The workflow is essentially a big
for url in urls: local_file = download_url(url) stac."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/azure-batch-pyspy/><link crossorigin=anonymous href=/assets/css/stylesheet.ced21e6d3497ee93fed8f8b357448095840179bd510b5ea0e6013078712e6dd1.css integrity="sha256-ztIebTSX7pP+2PizV0SAlYQBeb1RC16g5gEweHEubdE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tomaugspurger.net/posts/azure-batch-pyspy/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="py-spy in Azure Batch"><meta property="og:description" content="Today, I was debugging a hanging task in Azure Batch. This short post records how I used py-spy to investigate the problem.
Background Azure Batch is a compute service that we use to run container workloads. In this case, we start up a container that processes a bunch of GOES-GLM data to create STAC items for the Planetary Computer . The workflow is essentially a big
for url in urls: local_file = download_url(url) stac."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/azure-batch-pyspy/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-22T15:11:37-06:00"><meta property="article:modified_time" content="2023-02-22T15:11:37-06:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="py-spy in Azure Batch"><meta name=twitter:description content="Today, I was debugging a hanging task in Azure Batch. This short post records how I used py-spy to investigate the problem.
Background Azure Batch is a compute service that we use to run container workloads. In this case, we start up a container that processes a bunch of GOES-GLM data to create STAC items for the Planetary Computer . The workflow is essentially a big
for url in urls: local_file = download_url(url) stac."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":2,"name":"py-spy in Azure Batch","item":"https://tomaugspurger.net/posts/azure-batch-pyspy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"py-spy in Azure Batch","name":"py-spy in Azure Batch","description":"Today, I was debugging a hanging task in Azure Batch. This short post records how I used py-spy to investigate the problem.\nBackground Azure Batch is a compute service that we use to run container workloads. In this case, we start up a container that processes a bunch of GOES-GLM data to create STAC items for the Planetary Computer . The workflow is essentially a big\nfor url in urls: local_file = download_url(url) stac.","keywords":[],"articleBody":"Today, I was debugging a hanging task in Azure Batch. This short post records how I used py-spy to investigate the problem.\nBackground Azure Batch is a compute service that we use to run container workloads. In this case, we start up a container that processes a bunch of GOES-GLM data to create STAC items for the Planetary Computer . The workflow is essentially a big\nfor url in urls: local_file = download_url(url) stac.create_item(local_file) We noticed that some Azure Batch tasks were hanging. Based on our logs, we knew it was somewhere in that for loop, but couldn’t determine exactly where things were hanging. The goes-glm stactools package we used does read a NetCDF file, and my experience with Dask biased me towards thinking the netcdf library (or the HDF5 reader it uses) was hanging. But I wanted to confirm that before trying to implement a fix.\nDebugging I wasn’t able to reproduce the hanging locally, so I needed some way to debug the actual hanging process itself. My go-to tool for this type of task is py-spy. It does a lot, but in this case we’ll use py-spy dump to get something like a traceback for what’s currently running (and hanging) in the process.\nAzure Batch has a handy feature for SSH-ing into the running task nodes. With an auto-generated user and password, I had a shell on the node with the hanging process.\nThe only wrinkle here is that we’re using containerized workloads, so the actual process was in a Docker container and not in the host’s process list (I’ll try to follow Jacob Tomlinson’s lead and be intentional about container terminology). The py-spy documentation has some details on how to use py-spy with docker. This comment in particular has some more details on how to run py-spy on the host to detect a process running in a container. The upshot is a command like this, run on the Azure Batch node:\n$ root@...:/home/yqjjaq/# docker run -it --pid=container:244fdfc65349 --rm --privileged --cap-add SYS_PTRACE python /bin/bash where 244fdfc65349 is the ID of the container with the hanging process. I used the python image and then pip installed py-spy in that debugging container (you could also use some container image with py-spy already installed). Finally, I was able to run py-spy dump inside that running container to get the trace:\nroot@306ad36c7ae3:/# py-spy dump --pid 1 Process 1: /opt/conda/bin/python /opt/conda/bin/pctasks task run blob://pctaskscommon/taskio/run/827e3fa4-be68-49c9-b8c3-3d63b31962ba/process-chunk/3/create-items/input --sas-token ... --account-url https://pctaskscommon.blob.core.windows.net/ Python v3.8.16 (/opt/conda/bin/python3.8) Thread 0x7F8C69A78740 (active): \"MainThread\" read (ssl.py:1099) recv_into (ssl.py:1241) readinto (socket.py:669) _read_status (http/client.py:277) begin (http/client.py:316) getresponse (http/client.py:1348) _make_request (urllib3/connectionpool.py:444) urlopen (urllib3/connectionpool.py:703) send (requests/adapters.py:489) send (requests/sessions.py:701) request (requests/sessions.py:587) send (core/pipeline/transport/_requests_basic.py:338) send (blob/_shared/base_client.py:333) send (blob/_shared/base_client.py:333) send (core/pipeline/_base.py:100) send (core/pipeline/_base.py:69) send (core/pipeline/_base.py:69) send (blob/_shared/policies.py:290) send (core/pipeline/_base.py:69) send (core/pipeline/_base.py:69) send (core/pipeline/_base.py:69) send (blob/_shared/policies.py:489) send (core/pipeline/_base.py:69) send (core/pipeline/policies/_redirect.py:160) send (core/pipeline/_base.py:69) send (core/pipeline/_base.py:69) send (core/pipeline/_base.py:69) send (core/pipeline/_base.py:69) send (core/pipeline/_base.py:69) run (core/pipeline/_base.py:205) download (blob/_generated/operations/_blob_operations.py:180) _initial_request (blob/_download.py:386) __init__ (blob/_download.py:349) download_blob (blob/_blob_client.py:848) wrapper_use_tracer (core/tracing/decorator.py:78) (core/storage/blob.py:514) with_backoff (core/utils/backoff.py:136) download_file (core/storage/blob.py:513) create_item (goes_glm.py:32) create_items (dataset/items/task.py:117) run (dataset/items/task.py:153) parse_and_run (task/task.py:53) run_task (task/run.py:138) run_cmd (task/_cli.py:32) run_cmd (task/cli.py:50) new_func (click/decorators.py:26) invoke (click/core.py:760) invoke (click/core.py:1404) invoke (click/core.py:1657) invoke (click/core.py:1657) main (click/core.py:1055) __call__ (click/core.py:1130) cli (cli/cli.py:140) (pctasks:8) Thread 0x7F8C4A84F700 (idle): \"fsspecIO\" select (selectors.py:468) _run_once (asyncio/base_events.py:1823) run_forever (asyncio/base_events.py:570) run (threading.py:870) _bootstrap_inner (threading.py:932) _bootstrap (threading.py:890) Thread 0x7F8C4A00E700 (active): \"ThreadPoolExecutor-0_0\" _worker (concurrent/futures/thread.py:78) run (threading.py:870) _bootstrap_inner (threading.py:932) _bootstrap (threading.py:890) And we’ve found our culprit! The line\ndownload_file (core/storage/blob.py:513) and everything above it indicates that the process is hanging in the download stage, not the NetCDF reading stage!\nThis fix “Fixing” this is pretty easy. The Python SDK for Azure Blob Storage includes the option to set a read_timeout when creating the connection client. Now if the download hangs it should raise a TimeoutError. Then our handler will automatically catch and retry it, and hopefully succeed. It doesn’t address the actual cause of something deep inside the networking stack hanging, but it’s good enough for our purposes.\nUpdate: 2023-02-28. Turns out, the “fix” wasn’t actually a fix. The process hung again the next day. Naturally, I turned to this blog post to find the incantations to run (which is why I wrote it in the first place).\nAs for getting closer to an actual cause of the hang, a colleague suggested upgrading Python versions since there were some fixes in that area between 3.8 and 3.11. After about a week, there have been zero hangs on Python 3.11.\n","wordCount":"726","inLanguage":"en","datePublished":"2023-02-22T15:11:37-06:00","dateModified":"2023-02-22T15:11:37-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/azure-batch-pyspy/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net/ accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">py-spy in Azure Batch</h1><div class=post-meta><span title='2023-02-22 15:11:37 -0600 -0600'>February 22, 2023</span></div></header><div class=post-content><p>Today, I was debugging a hanging task in <a href=https://learn.microsoft.com/en-us/azure/batch/batch-technical-overview>Azure Batch</a>.
This short post records how I used <a href=https://github.com/benfred/py-spy>py-spy</a> to investigate the problem.</p><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>Azure Batch is a compute service that we use to run <a href=https://learn.microsoft.com/en-us/azure/batch/batch-docker-container-workloads>container
workloads</a>.
In this case, we start up a container that processes a bunch of GOES-GLM data to
create <a href=https://stacspec.org/en>STAC items</a> for the <a href=http://planetarycomputer.microsoft.com/>Planetary
Computer</a> . The workflow is essentially
a big</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> url <span style=color:#f92672>in</span> urls:
</span></span><span style=display:flex><span>    local_file <span style=color:#f92672>=</span> download_url(url)
</span></span><span style=display:flex><span>    stac<span style=color:#f92672>.</span>create_item(local_file)
</span></span></code></pre></div><p>We noticed that some Azure Batch tasks were hanging. Based on our logs, we knew
it was somewhere in that for loop, but couldn&rsquo;t determine exactly where things
were hanging. The <a href=https://github.com/stactools-packages/goes-glm>goes-glm</a> stactools package we used does read a NetCDF file,
and my experience with Dask biased me towards thinking the <code>netcdf</code> library (or
the HDF5 reader it uses) was hanging. But I wanted to confirm that before trying
to implement a fix.</p><h2 id=debugging>Debugging<a hidden class=anchor aria-hidden=true href=#debugging>#</a></h2><p>I wasn&rsquo;t able to reproduce the hanging locally, so I needed some way to debug
the actual hanging process itself. My go-to tool for this type of task is
<a href=https://github.com/benfred/py-spy>py-spy</a>. It does a lot, but in this case we&rsquo;ll use <code>py-spy dump</code> to get
something like a traceback for what&rsquo;s currently running (and hanging) in the
process.</p><p>Azure Batch has a handy feature for SSH-ing into the running task nodes. With an
auto-generated user and password, I had a shell on the node with the hanging
process.</p><p>The only wrinkle here is that we&rsquo;re using containerized workloads, so the actual
process was in a Docker container and not in the host&rsquo;s process list (I&rsquo;ll try
to follow Jacob Tomlinson&rsquo;s lead and be <a href=https://jacobtomlinson.dev/posts/2023/being-intentional-with-container-terminology/>intentional</a> about
container terminology). The <code>py-spy</code> documentation has some details on how to
use <code>py-spy</code> with docker. This <a href=https://github.com/benfred/py-spy/issues/49>comment</a> in particular has some more
details on how to run py-spy on the host to detect a process running in a
container. The upshot is a command like this, run on the Azure Batch node:</p><pre tabindex=0><code>$ root@...:/home/yqjjaq/# docker run -it --pid=container:244fdfc65349 --rm --privileged --cap-add SYS_PTRACE python /bin/bash
</code></pre><p>where <code>244fdfc65349</code> is the ID of the container with the hanging process. I used
the <code>python</code> image and then <code>pip install</code>ed <code>py-spy</code> in that debugging container
(you could also use some container image with <code>py-spy</code> already installed).
Finally, I was able to run <code>py-spy dump</code> inside that running container to get
the trace:</p><pre tabindex=0><code>root@306ad36c7ae3:/# py-spy dump --pid 1
Process 1: /opt/conda/bin/python /opt/conda/bin/pctasks task run blob://pctaskscommon/taskio/run/827e3fa4-be68-49c9-b8c3-3d63b31962ba/process-chunk/3/create-items/input --sas-token ... --account-url https://pctaskscommon.blob.core.windows.net/
Python v3.8.16 (/opt/conda/bin/python3.8)

Thread 0x7F8C69A78740 (active): &#34;MainThread&#34;
    read (ssl.py:1099)
    recv_into (ssl.py:1241)
    readinto (socket.py:669)
    _read_status (http/client.py:277)
    begin (http/client.py:316)
    getresponse (http/client.py:1348)
    _make_request (urllib3/connectionpool.py:444)
    urlopen (urllib3/connectionpool.py:703)
    send (requests/adapters.py:489)
    send (requests/sessions.py:701)
    request (requests/sessions.py:587)
    send (core/pipeline/transport/_requests_basic.py:338)
    send (blob/_shared/base_client.py:333)
    send (blob/_shared/base_client.py:333)
    send (core/pipeline/_base.py:100)
    send (core/pipeline/_base.py:69)
    send (core/pipeline/_base.py:69)
    send (blob/_shared/policies.py:290)
    send (core/pipeline/_base.py:69)
    send (core/pipeline/_base.py:69)
    send (core/pipeline/_base.py:69)
    send (blob/_shared/policies.py:489)
    send (core/pipeline/_base.py:69)
    send (core/pipeline/policies/_redirect.py:160)
    send (core/pipeline/_base.py:69)
    send (core/pipeline/_base.py:69)
    send (core/pipeline/_base.py:69)
    send (core/pipeline/_base.py:69)
    send (core/pipeline/_base.py:69)
    run (core/pipeline/_base.py:205)
    download (blob/_generated/operations/_blob_operations.py:180)
    _initial_request (blob/_download.py:386)
    __init__ (blob/_download.py:349)
    download_blob (blob/_blob_client.py:848)
    wrapper_use_tracer (core/tracing/decorator.py:78)
    &lt;lambda&gt; (core/storage/blob.py:514)
    with_backoff (core/utils/backoff.py:136)
    download_file (core/storage/blob.py:513)
    create_item (goes_glm.py:32)
    create_items (dataset/items/task.py:117)
    run (dataset/items/task.py:153)
    parse_and_run (task/task.py:53)
    run_task (task/run.py:138)
    run_cmd (task/_cli.py:32)
    run_cmd (task/cli.py:50)
    new_func (click/decorators.py:26)
    invoke (click/core.py:760)
    invoke (click/core.py:1404)
    invoke (click/core.py:1657)
    invoke (click/core.py:1657)
    main (click/core.py:1055)
    __call__ (click/core.py:1130)
    cli (cli/cli.py:140)
    &lt;module&gt; (pctasks:8)
Thread 0x7F8C4A84F700 (idle): &#34;fsspecIO&#34;
    select (selectors.py:468)
    _run_once (asyncio/base_events.py:1823)
    run_forever (asyncio/base_events.py:570)
    run (threading.py:870)
    _bootstrap_inner (threading.py:932)
    _bootstrap (threading.py:890)
Thread 0x7F8C4A00E700 (active): &#34;ThreadPoolExecutor-0_0&#34;
    _worker (concurrent/futures/thread.py:78)
    run (threading.py:870)
    _bootstrap_inner (threading.py:932)
    _bootstrap (threading.py:890)
</code></pre><p>And we&rsquo;ve found our culprit! The line</p><pre tabindex=0><code>download_file (core/storage/blob.py:513)
</code></pre><p>and everything above it indicates that the process is hanging in the <em>download</em>
stage, not the NetCDF reading stage!</p><h2 id=this-fix>This fix<a hidden class=anchor aria-hidden=true href=#this-fix>#</a></h2><p>&ldquo;Fixing&rdquo; this is pretty easy. The Python SDK for Azure Blob Storage includes the
option to set a <code>read_timeout</code> when creating the connection client. Now if the
download hangs it should raise a <code>TimeoutError</code>. Then our handler will
automatically catch and retry it, and hopefully succeed. It doesn&rsquo;t address
the actual cause of something deep inside
the networking stack hanging, but it&rsquo;s good enough for our purposes.</p><p>Update: 2023-02-28. Turns out, the &ldquo;fix&rdquo; wasn&rsquo;t actually a fix. The process hung
again the next day. Naturally, I turned to this blog post to find the incantations
to run (which is why I wrote it in the first place).</p><p>As for getting closer to an actual cause of the hang, a colleague suggested upgrading
Python versions since there were some fixes in that area between 3.8 and 3.11. After
about a week, there have been zero hangs on Python 3.11.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://tomaugspurger.net/>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>