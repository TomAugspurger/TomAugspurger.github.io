<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scalable Machine Learning (Part 2): Partial Fit | Tom's Blog</title><meta name=keywords content><meta name=description content="This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
This is part two of my series on scalable machine learning.
Small Fit, Big Predict Scikit-Learn Partial Fit You can download a notebook of this post here.
Scikit-learn supports out-of-core learning (fitting a model on a dataset that doesn&rsquo;t fit in RAM), through it&rsquo;s partial_fit API. See here.
The basic idea is that, for certain estimators, learning can be done in batches."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/scalable-ml-02/><link crossorigin=anonymous href=/assets/css/stylesheet.3690c96d8a707265a16abd3b389bb33e4e3916869c3142cba43a3cfaaed4b5f9.css integrity="sha256-NpDJbYpwcmWhar07OJuzPk45FoacMULLpDo8+q7Utfk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Scalable Machine Learning (Part 2): Partial Fit"><meta property="og:description" content="This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
This is part two of my series on scalable machine learning.
Small Fit, Big Predict Scikit-Learn Partial Fit You can download a notebook of this post here.
Scikit-learn supports out-of-core learning (fitting a model on a dataset that doesn&rsquo;t fit in RAM), through it&rsquo;s partial_fit API. See here.
The basic idea is that, for certain estimators, learning can be done in batches."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/scalable-ml-02/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-09-15T00:00:00+00:00"><meta property="article:modified_time" content="2017-09-15T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scalable Machine Learning (Part 2): Partial Fit"><meta name=twitter:description content="This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
This is part two of my series on scalable machine learning.
Small Fit, Big Predict Scikit-Learn Partial Fit You can download a notebook of this post here.
Scikit-learn supports out-of-core learning (fitting a model on a dataset that doesn&rsquo;t fit in RAM), through it&rsquo;s partial_fit API. See here.
The basic idea is that, for certain estimators, learning can be done in batches."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":3,"name":"Scalable Machine Learning (Part 2): Partial Fit","item":"https://tomaugspurger.net/posts/scalable-ml-02/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scalable Machine Learning (Part 2): Partial Fit","name":"Scalable Machine Learning (Part 2): Partial Fit","description":"This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.\nThis is part two of my series on scalable machine learning.\nSmall Fit, Big Predict Scikit-Learn Partial Fit You can download a notebook of this post here.\nScikit-learn supports out-of-core learning (fitting a model on a dataset that doesn\u0026rsquo;t fit in RAM), through it\u0026rsquo;s partial_fit API. See here.\nThe basic idea is that, for certain estimators, learning can be done in batches.","keywords":[],"articleBody":"This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.\nThis is part two of my series on scalable machine learning.\nSmall Fit, Big Predict Scikit-Learn Partial Fit You can download a notebook of this post here.\nScikit-learn supports out-of-core learning (fitting a model on a dataset that doesn’t fit in RAM), through it’s partial_fit API. See here.\nThe basic idea is that, for certain estimators, learning can be done in batches. The estimator will see a batch, and then incrementally update whatever it’s learning (the coefficients, for example). This link has a list of the algorithms that implement partial_fit.\nUnfortunately, the partial_fit API doesn’t play that nicely with my favorite part of scikit-learn, pipelines, which we discussed at length in part 1. For pipelines to work, you would essentially need every step in the pipeline to have an out-of-core partial_fit version, which isn’t really feasible; some algorithms just have to see the entire dataset at once. Setting that aside, it wouldn’t be great for a user, since working with generators of datasets is awkward compared to the expressivity we get from pandas and NumPy.\nFortunately, we have great data containers for larger than memory arrays and dataframes: dask.array and dask.dataframe. We can\nUse dask for pre-processing data in an out-of-core manner Use scikit-learn to fit the actual model, out-of-core, using the partial_fit API And with a little bit of work, all of this can be done in a pipeline. The rest of this post shows how.\nBig Arrays If you follow along in the companion notebook, you’ll see that I generate a dataset, replicate it 100 times, and write the results out to disk. I then read it back in as a pair of dask.dataframes and convert them to a pair of dask.arrays. I’ll skip those details to focus on main goal: using sklearn.Pipelines on larger-than-memory datasets. Suffice to say, we have a function read that gives us our big X and y:\nX, y = read() X dask.array y dask.array So X is a 100,000,000 x 20 array of floats (I have float64s, you’re probably fine with float32s) that we’ll use to predict y. I generated the dataset, so I know that y is either 0 or 1. We’ll be doing classification.\n(X.nbytes + y.nbytes) / 10**9 16.8 My laptop has 16 GB of RAM, and the dataset is 16.8 GB. We can’t simply read the entire thing into memory. We’ll use dask for the preprocessing, and scikit-learn for the fitting. We’ll have a small pipeline\nScale the features by mean and variance Fit an SGDClassifier I’ve implemented a daskml.preprocessing.StandardScaler, using dask, in about 40 lines of code (see here). The scaling will be done completely in parallel and completely out-of-core.\nI haven’t implemented a custom SGDClassifier, because that’d be much more than 40 lines of code. Instead, I’ve put together a small wrapper that will use scikit-learn’s SGDClassifier.partial_fit to fit the model out-of-core (but not in parallel).\nfrom daskml.preprocessing import StandardScaler from daskml.linear_model import BigSGDClassifier # The wrapper from dask.diagnostics import ResourceProfiler, Profiler, ProgressBar from sklearn.pipeline import make_pipeline As a user, the API is the same as scikit-learn. Indeed, it is just a regular sklearn.pipeline.Pipeline.\npipe = make_pipeline( StandardScaler(), BigSGDClassifier(classes=[0, 1], max_iter=1000, tol=1e-3, random_state=2), ) And fitting is identical as well: pipe.fit(X, y). We’ll collect some performance metrics as well, so we can analyze our parallelism.\n%%time rp = ResourceProfiler() p = Profiler() with p, rp: pipe.fit(X, y) CPU times: user 2min 38s, sys: 1min 44s, total: 4min 22s Wall time: 1min 47s And that’s it. It’s just a regular scikit-learn pipeline, operating on a larger-than-memory data. pipe has has all the regular methods you would expect, predict, predict_proba, etc. You can get to the individual attributes like pipe.steps[1][1].coef_.\nOne important point to stress here: when we get to the BigSGDClassifier.fit at the end of the pipeline, everything is done serially. We can see that by plotting the Profiler we captured up above:\nThat graph shows the tasks (the rectangles) each worker (a core on my laptop) executed over time. Workers are along the vertical axis, and time is along the horizontal. Towards the start, when we’re reading off disk, converting to dask.arrays, and doing the StandardScaler, everything is in parallel. Once we get to the BigSGDClassifier, which is just a simple wrapper around sklearn.linear_model.SGDClassifier, we lose all our parallelism*.\nThe predict step is done entirely in parallel.\nwith rp, p: predictions = pipe.predict(X) predictions.to_dask_dataframe(columns='a').to_parquet('predictions.parq') That took about 40 seconds, from disk to prediction, and back to disk on 16 GB of data, using all 8 cores of my laptop.\nHow? When I had this idea last week, of feeding blocks of dask.array to a scikit-learn estimator’s partial_fit method, I thought it was pretty neat. Turns out Matt Rocklin already had the idea, and implemented it in dask, two years ago.\nRoughly speaking, the implementation is:\nclass BigSGDClassifier(SGDClassifier): ... def fit(self, X, y): # ... some setup for xx, yy in by_blocks(X, y): self.partial_fit(xx, yy) return self If you aren’t familiar with dask, its arrays are composed of many smaller NumPy arrays (blocks in the larger dask array). We iterate over the dask arrays block-wise, and pass them into the estimators partial_fit method. That’s exactly what you would be doing if you were using, say, a generator feed NumPy arrays to the partial_fit method. Only you can manipulate a dask.array like regular NumPy array, so things are more convenient.\nSome Challenges For our small pipeline, we had to make two passes over the data. One to fit the StandardScaler and one to fit the BigSGDClassifier. In general, with this approach, we’ll have to make one pass per stage of the pipeline, which isn’t great. I think this is unavoidable with the current design, but I’m considering ways around it.\nRecap We’ve seen a way to use scikit-learn’s existing estimators on larger-than-memory dask arrays by passing the blocks of a dask array to the partial_fit method. This enables us to use Pipelines on larger-than-memory datasets.\nLet me know what you think. I’m pretty excited about this because it removes some of the friction around using sckit-learn Pipelines with out-of-core estimators. In dask-ml, I’ve implemented similar wrappers for\nSGDRegressor PassiveAggressiveClassifier PassiveAggressiveRegressor Perceptron MPLClassifier MLPRegressor MiniBatchKMeans I’ll be packaging this up in daskml to make it more usable for the community over the next couple weeks. If this type of work interests you, please reach out on Twitter or by email at mailto:tom.w.augspurger@gmail.com. If you’re interested in contributing, I think a library of basic transformers that operate on NumPy and dask arrays and pandas and dask DataFrames would be extremely useful. I’ve started an issue to track this progress. Contributions would be extremely welcome.\nNext time we’ll be going back to smaller datasets. We’ll see how dask can help us parallelize our work to fit more models in less time.\n","wordCount":"1155","inLanguage":"en","datePublished":"2017-09-15T00:00:00Z","dateModified":"2017-09-15T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/scalable-ml-02/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Scalable Machine Learning (Part 2): Partial Fit</h1><div class=post-meta><span title='2017-09-15 00:00:00 +0000 UTC'>September 15, 2017</span></div></header><div class=post-content><p><em>This work is supported by <a href=https://www.anaconda.com/>Anaconda, Inc.</a> and the
Data Driven Discovery Initiative from the <a href=https://www.moore.org/>Moore Foundation</a>.</em></p><p>This is part two of my series on scalable machine learning.</p><ul><li><a href=scalable-ml-01>Small Fit, Big Predict</a></li><li><a href=scalable-ml-02>Scikit-Learn Partial Fit</a></li></ul><p>You can download a notebook of this post <a href=http://nbviewer.jupyter.org/github/TomAugspurger/scalable-ml/blob/master/partial.ipynb>here</a>.</p><hr><p>Scikit-learn supports out-of-core learning (fitting a model on a dataset that
doesn&rsquo;t fit in RAM), through it&rsquo;s <code>partial_fit</code> API. See
<a href=http://scikit-learn.org/stable/modules/scaling_strategies.html#scaling-with-instances-using-out-of-core-learning>here</a>.</p><p>The basic idea is that, <em>for certain estimators</em>, learning can be done in
batches. The estimator will see a batch, and then incrementally update whatever
it&rsquo;s learning (the coefficients, for example). <a href=http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning>This
link</a>
has a list of the algorithms that implement <code>partial_fit</code>.</p><p>Unfortunately, the <code>partial_fit</code> API doesn&rsquo;t play that nicely with my favorite
part of scikit-learn,
<a href=http://scikit-learn.org/stable/modules/pipeline.html#pipeline>pipelines</a>,
which we discussed at length in <a href=scalable-ml-01>part 1</a>. For pipelines to work,
you would essentially need every step in the pipeline to have an out-of-core
<code>partial_fit</code> version, which isn&rsquo;t really feasible; some algorithms just have to
see the entire dataset at once. Setting that aside, it wouldn&rsquo;t be great for a
user, since working with generators of datasets is awkward compared to the
expressivity we get from pandas and NumPy.</p><p>Fortunately, we <em>have</em> great data containers for larger than memory arrays and
dataframes: <code>dask.array</code> and <code>dask.dataframe</code>. We can</p><ol><li>Use dask for pre-processing data in an out-of-core manner</li><li>Use scikit-learn to fit the actual model, out-of-core, using the
<code>partial_fit</code> API</li></ol><p>And with a little bit of work, all of this can be done in a pipeline. The rest
of this post shows how.</p><h2 id=big-arrays>Big Arrays<a hidden class=anchor aria-hidden=true href=#big-arrays>#</a></h2><p>If you follow along in the <a href=http://nbviewer.jupyter.org/github/TomAugspurger/scalable-ml/blob/master/partial.ipynb>companion notebook</a>, you&rsquo;ll see that I
generate a dataset, replicate it 100 times, and write the results out to disk. I
then read it back in as a pair of <code>dask.dataframe</code>s and convert them to a pair
of <code>dask.array</code>s. I&rsquo;ll skip those details to focus on main goal: using
<code>sklearn.Pipeline</code>s on larger-than-memory datasets. Suffice to say, we have a
function <code>read</code> that gives us our big <code>X</code> and <code>y</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>X, y <span style=color:#f92672>=</span> read()
</span></span><span style=display:flex><span>X
</span></span></code></pre></div><pre><code>dask.array&lt;concatenate, shape=(100000000, 20), dtype=float64, chunksize=(500000, 20)&gt;
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y
</span></span></code></pre></div><pre><code>dask.array&lt;squeeze, shape=(100000000,), dtype=float64, chunksize=(500000,)&gt;
</code></pre><p>So <code>X</code> is a 100,000,000 x 20 array of floats (I have float64s, you&rsquo;re probably
fine with float32s) that we&rsquo;ll use to predict <code>y</code>. I generated the dataset, so I
know that <code>y</code> is either 0 or 1. We&rsquo;ll be doing classification.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>(X<span style=color:#f92672>.</span>nbytes <span style=color:#f92672>+</span> y<span style=color:#f92672>.</span>nbytes) <span style=color:#f92672>/</span> <span style=color:#ae81ff>10</span><span style=color:#f92672>**</span><span style=color:#ae81ff>9</span>
</span></span></code></pre></div><pre><code>16.8
</code></pre><p>My laptop has 16 GB of RAM, and the dataset is 16.8 GB. We can&rsquo;t simply read the
entire thing into memory. We&rsquo;ll use dask for the preprocessing, and scikit-learn
for the fitting. We&rsquo;ll have a small pipeline</p><ol><li>Scale the features by mean and variance</li><li>Fit an SGDClassifier</li></ol><p>I&rsquo;ve implemented a <code>daskml.preprocessing.StandardScaler</code>, using dask, in about
40 lines of code (see <a href=https://github.com/dask/dask-ml/blob/9e85ba282a93c0f62afbe68dabe088fbd59ada40/daskml/preprocessing/data.py#L8>here</a>).
The scaling will be done completely in parallel and completely out-of-core.</p><p>I <em>haven&rsquo;t</em> implemented a custom <code>SGDClassifier</code>, because that&rsquo;d be much more
than 40 lines of code. Instead, I&rsquo;ve put together a small wrapper that will use
scikit-learn&rsquo;s <code>SGDClassifier.partial_fit</code> to fit the model out-of-core (but not
in parallel).</p><pre tabindex=0><code>from daskml.preprocessing import StandardScaler
from daskml.linear_model import BigSGDClassifier  # The wrapper

from dask.diagnostics import ResourceProfiler, Profiler, ProgressBar
from sklearn.pipeline import make_pipeline
</code></pre><p>As a user, the API is the same as <code>scikit-learn</code>. Indeed, it <em>is</em> just a regular
<code>sklearn.pipeline.Pipeline</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pipe <span style=color:#f92672>=</span> make_pipeline(
</span></span><span style=display:flex><span>    StandardScaler(),
</span></span><span style=display:flex><span>    BigSGDClassifier(classes<span style=color:#f92672>=</span>[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>], max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, tol<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-3</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>And fitting is identical as well: <code>pipe.fit(X, y)</code>. We&rsquo;ll collect some
performance metrics as well, so we can analyze our parallelism.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span>rp <span style=color:#f92672>=</span> ResourceProfiler()
</span></span><span style=display:flex><span>p <span style=color:#f92672>=</span> Profiler()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> p, rp:
</span></span><span style=display:flex><span>    pipe<span style=color:#f92672>.</span>fit(X, y)
</span></span></code></pre></div><pre><code>CPU times: user 2min 38s, sys: 1min 44s, total: 4min 22s
Wall time: 1min 47s
</code></pre><p>And that&rsquo;s it. It&rsquo;s just a regular scikit-learn pipeline, operating on a
larger-than-memory data. <code>pipe</code> has has all the regular methods you would
expect, <code>predict</code>, <code>predict_proba</code>, etc. You can get to the individual
attributes like <code>pipe.steps[1][1].coef_</code>.</p><p>One important point to stress here: when we get to the <code>BigSGDClassifier.fit</code>
at the end of the pipeline, everything is done serially. We can see that by
plotting the <code>Profiler</code> we captured up above:</p><p><img loading=lazy src=/images/sml-02-fit.png alt="Training parallelism"></p><p>That graph shows the tasks (the rectangles) each worker (a core on my laptop)
executed over time. Workers are along the vertical axis, and time is along the
horizontal. Towards the start, when we&rsquo;re reading off disk, converting to
<code>dask.array</code>s, and doing the <code>StandardScaler</code>, everything is in parallel. Once
we get to the <code>BigSGDClassifier</code>, which is just a simple wrapper around
<code>sklearn.linear_model.SGDClassifier</code>, we lose all our parallelism*.</p><p>The predict step <em>is</em> done entirely in parallel.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> rp, p:
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> pipe<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>    predictions<span style=color:#f92672>.</span>to_dask_dataframe(columns<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;a&#39;</span>)<span style=color:#f92672>.</span>to_parquet(<span style=color:#e6db74>&#39;predictions.parq&#39;</span>)
</span></span></code></pre></div><p><img loading=lazy src=/images/sml-02-predict.png alt="Predicting parallelism"></p><p>That took about 40 seconds, from disk to prediction, and back to disk on 16 GB
of data, using all 8 cores of my laptop.</p><h2 id=how>How?<a hidden class=anchor aria-hidden=true href=#how>#</a></h2><p>When I had this idea last week, of feeding blocks of <code>dask.array</code> to a
scikit-learn estimator&rsquo;s <code>partial_fit</code> method, I thought it was pretty neat.
Turns out Matt Rocklin already had the idea, and implemented it in dask, two
years ago.</p><p>Roughly speaking, the implementation is:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BigSGDClassifier</span>(SGDClassifier):
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y):
</span></span><span style=display:flex><span>        <span style=color:#75715e># ... some setup</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> xx, yy <span style=color:#f92672>in</span> by_blocks(X, y):
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>partial_fit(xx, yy)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span></code></pre></div><p>If you aren&rsquo;t familiar with <code>dask</code>, its arrays are composed of many smaller
NumPy arrays (blocks in the larger dask array). We iterate over the dask arrays
block-wise, and pass them into the estimators <code>partial_fit</code> method. That&rsquo;s exactly
what you would be doing if you were using, say, a generator feed NumPy arrays to
the <code>partial_fit</code> method. Only you can manipulate a <code>dask.array</code> like regular
NumPy array, so things are more convenient.</p><h2 id=some-challenges>Some Challenges<a hidden class=anchor aria-hidden=true href=#some-challenges>#</a></h2><p>For our small pipeline, we had to make two passes over the data. One to fit the
<code>StandardScaler</code> and one to fit the <code>BigSGDClassifier</code>. In general, with
this approach, we&rsquo;ll have to make one pass per stage of the pipeline, which
isn&rsquo;t great. I <em>think</em> this is unavoidable with the current design, but I&rsquo;m
considering ways around it.</p><h2 id=recap>Recap<a hidden class=anchor aria-hidden=true href=#recap>#</a></h2><p>We&rsquo;ve seen <em>a</em> way to use scikit-learn&rsquo;s existing estimators on
larger-than-memory dask arrays by passing the blocks of a dask array to the
<code>partial_fit</code> method. This enables us to use <code>Pipeline</code>s on larger-than-memory
datasets.</p><p>Let me know what you think. I&rsquo;m pretty excited about this because it removes
some of the friction around using sckit-learn Pipelines with out-of-core
estimators. In <a href=https://github.com/dask/dask-ml><code>dask-ml</code></a>, I&rsquo;ve implemented similar wrappers for</p><ul><li>SGDRegressor</li><li>PassiveAggressiveClassifier</li><li>PassiveAggressiveRegressor</li><li>Perceptron</li><li>MPLClassifier</li><li>MLPRegressor</li><li>MiniBatchKMeans</li></ul><p>I&rsquo;ll be packaging this up in <a href=https://github.com/dask/dask-ml><code>daskml</code></a> to make it more usable for the
community over the next couple weeks. If this type of work interests you, please
reach out on <a href=http://twitter.com/TomAugspurger>Twitter</a> or by
email at <a href=mailto:tom.w.augspurger@gmail.com>mailto:tom.w.augspurger@gmail.com</a>. If you&rsquo;re interested in contributing, I
think a library of basic transformers that operate on NumPy and dask arrays and
pandas and dask DataFrames would be <em>extremely</em> useful. I&rsquo;ve started <a href=https://github.com/dask/dask-ml/issues/6>an
issue</a> to track this progress.
Contributions would be extremely welcome.</p><p>Next time we&rsquo;ll be going back to smaller datasets. We&rsquo;ll see how dask can help
us parallelize our work to fit more models in less time.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://tomaugspurger.net>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>