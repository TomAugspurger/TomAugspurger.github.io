<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Easy distributed training with Joblib and dask | Tom's Blog</title>
<meta name=keywords content><meta name=description content="This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.
This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I&rsquo;m thankful to them for hosting me and Anaconda for sending me there. This article will talk about some improvements we made to improve training scikit-learn models using a cluster."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/distributed-joblib/><link crossorigin=anonymous href=/assets/css/stylesheet.3690c96d8a707265a16abd3b389bb33e4e3916869c3142cba43a3cfaaed4b5f9.css integrity="sha256-NpDJbYpwcmWhar07OJuzPk45FoacMULLpDo8+q7Utfk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Easy distributed training with Joblib and dask"><meta property="og:description" content="This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.
This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I&rsquo;m thankful to them for hosting me and Anaconda for sending me there. This article will talk about some improvements we made to improve training scikit-learn models using a cluster."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/distributed-joblib/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-02-05T00:00:00+00:00"><meta property="article:modified_time" content="2018-02-05T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Easy distributed training with Joblib and dask"><meta name=twitter:description content="This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.
This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I&rsquo;m thankful to them for hosting me and Anaconda for sending me there. This article will talk about some improvements we made to improve training scikit-learn models using a cluster."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":3,"name":"Easy distributed training with Joblib and dask","item":"https://tomaugspurger.net/posts/distributed-joblib/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Easy distributed training with Joblib and dask","name":"Easy distributed training with Joblib and dask","description":"This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.\nThis past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I\u0026rsquo;m thankful to them for hosting me and Anaconda for sending me there. This article will talk about some improvements we made to improve training scikit-learn models using a cluster.","keywords":[],"articleBody":"This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.\nThis past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I’m thankful to them for hosting me and Anaconda for sending me there. This article will talk about some improvements we made to improve training scikit-learn models using a cluster.\nScikit-learn uses joblib for simple parallelism in many places. Anywhere you pass an n_jobs keyword, scikit-learn is internally calling joblib.Parallel(...), and doing a batch of work in parallel. The estimator may have an embarrassingly parallel step internally (fitting each of the trees in a RandomForest for example). Or your meta-estimator like GridSearchCV may try out many combinations of hyper-parameters in parallel.\nYou can think of joblib as a broker between the user and the algorithm author. The user comes along and says, “I have n_jobs cores, please use them!”. Scikit-Learn says “I have all these embarrassingly parallel tasks to be run as part of fitting this estimator.” Joblib accepts the cores from the user and the tasks from scikit-learn, runs the tasks on the cores, and hands the completed tasks back to scikit-learn.\nJoblib offers a few “backends” for how to do your parallelism, but they all boil down to using many processes versus using many threads.\nParallelism in Python A quick digression on single-machine parallelism in Python. We can’t say up front that using threads is always better or worse than using processes. Unfortunately the relative performance depends on the specific workload. But we do have some general heuristics that come down to serialization overhead and Python’s Global Interpreter Lock (GIL).\nThe GIL is part of CPython, the C program that interprets and runs your Python program. It limits your Python process so that only one thread is executing Python at once, defeating your parallelism. Fortunately, much of the numerical Python stack is written in C, Cython, C++, Fortran, or numba, and may be able to release the GIL. This means your “Python” program, which is calling into Cython or C via NumPy or pandas, can get real thread-based parallelism without being limited by the GIL. The main caveat here that manipulating strings or Python objects (lists, dicts, sets, etc) typically requires holding the GIL.\nSo, if we have the option of choosing threads or processes, which do we want? For most numeric / scientific workloads, threads are better than processes because of shared memory. Each thread in a thread-pool can view (and modify!) the same large NumPy array. With multiple processes, data must be serialized between processes (perhaps using pickle). For large arrays or dataframes this can be slow, and it may blow up your memory if the data a decent fraction of your machine’s RAM. You’ll have a full copy in each processes.\nSee Matthew Rocklin’s article and David Beazley’s page if you want to learn more.\nDistributed Training with dask.distributed For a while now, you’ve been able to use dask.distributed as a backend for joblib. This means that in most places scikit-learn offers an n_jobs keyword, you’re able to do the parallel computation on your cluster.\nThis is great when\nYour dataset is not too large (since the data must be sent to each worker) The runtime of each task is long enough that the overhead of serializing the data across the network to the worker doesn’t dominate the runtime You have many parallel tasks to run (else, you’d just use a local thread or process pool and avoid the network delay) Fitting a RandomForest is a good example of this. Each tree in a forest may be built independently of every other tree. This next code chunk shows how you can parallelize fitting a RandomForestClassifier across a cluster, though as discussed later this won’t work on the currently released versions of scikit-learn and joblib.\nfrom sklearn.externals import joblib from dask.distributed import Client import distributed.joblib # register the joblib backend client = Client('dask-scheduler:8786') with joblib.parallel_backend(\"dask\", scatter=[X_train, y_train]): clf.fit(X_train, y_train) The .fit call is parallelized across all the workers in your cluster. Here’s the distributed dashboard during that training.\nYour browser doesn't support HTML5 video. The center pane shows the task stream as they complete. Each rectangle is a single task, building a single tree in a random forest in this case. Workers are represented vertically. My cluster had 8 workers with 4 cores each, which means up to 32 tasks can be processed simultaneously. We fit the 200 trees in about 20 seconds.\nChanges to Joblib Above, I said that distributed training worked in most places in scikit-learn. Getting it to work everywhere required a bit more work, and was part of last week’s focus.\nFirst, dask.distributed’s joblib backend didn’t handle nested parallelism well. This may occur if you do something like\ngs = GridSearchCV(Estimator(n_jobs=-1), n_jobs=-1) gs.fit(X, y) Previously, that caused deadlocks. Inside GridSearchCV, there’s a call like\n# In GridSearchCV.fit, the outer layer results = joblib.Parallel(n_jobs=n_jobs)(fit_estimator)(...) where fit_estimator is a function that itself tries to do things in parallel\n# In fit_estimator, the inner layer results = joblib.Parallel(n_jobs=n_jobs)(fit_one)(...) So the outer level kicks off a bunch of joblib.Parallel calls, and waits around for the results. For each of those Parallel calls, the inner level tries to make a bunch of joblib.Parallel calls. When joblib tried to start the inner ones, it would ask the distributed scheduler for a free worker. But all the workers were “busy” waiting around for the outer Parallel calls to finish, which weren’t progressing because there weren’t any free workers! Deadlock!\ndask.distributed has a solution for this case (workers secede from the thread pool when they start a long-running Parllel call, and rejoin when they’re done), but we needed a way to negotiate with joblib about when the secede and rejoin should happen. Joblib now has an API for backends to control some setup and teardown around the actual function execution. This work was done in Joblib #538 and dask-distributed #1705.\nSecond, some places in scikit-learn hard-code the backend they want to use in their Parallel() call, meaning the cluster isn’t used. This may be because the algorithm author knows that one backend performs better than others. For example, RandomForest.fit performs better with threads, since it’s purely numeric and releases the GIL. In this case we would say the Parallel call prefers threads, since you’d get the same result with processes, it’d just be slower.\nAnother reason for hard-coding the backend is if the correctness of the implementation relies on it. For example, RandomForest.predict preallocates the output array and mutates it from many threads (it knows not to mutate the same place from multiple threads). In this case, we’d say the Parallel call requires shared memory, because you’d get an incorrect result using processes.\nThe solution was to enhance joblib.Parallel to take two new keywords, prefer and require. If a Parallel call prefers threads, it’ll use them, unless it’s in a context saying “use this backend instead”, like\ndef fit(n_jobs=-1): return joblib.Parallel(n_jobs=n_jobs, prefer=\"threads\")(...) with joblib.parallel_backend('dask'): # This uses dask's workers, not threads fit() On the other hand, if a Parallel requires a specific backend, it’ll get it.\ndef fit(n_jobs=-1): return joblib.Parallel(n_jobs=n_jobs, require=\"sharedmem\")(...) with joblib.parallel_backend('dask'): # This uses the threading backend, since shared memory is required fit() This is a elegant way to negotiate a compromise between\nThe user, who knows best about what resources are available, as specified by the joblib.parallel_backend context manager. And, The algorithm author, who knows best about the GIL handling and shared memory requirements. This work was done in Joblib #602.\nAfter the next joblib release, scikit-learn will be updated to use these options in places where the backend is currently hard-coded. My example above used a branch with those changes.\nLook forward for these changes in the upcoming joblib, dask, and scikit-learn releases. As always, let me know if you have any feedback.\n","wordCount":"1320","inLanguage":"en","datePublished":"2018-02-05T00:00:00Z","dateModified":"2018-02-05T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/distributed-joblib/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Easy distributed training with Joblib and dask</h1><div class=post-meta>&lt;span title='2018-02-05 00:00:00 +0000 UTC'>February 5, 2018&lt;/span></div></header><div class=post-content><p><em>This work is supported by <a href=http://anaconda.com/>Anaconda Inc</a> and the Data
Driven Discovery Initiative from the <a href=https://www.moore.org/>Moore
Foundation</a>.</em></p><p>This past week, I had a chance to visit some of the scikit-learn developers at
Inria in Paris. It was a fun and productive week, and I&rsquo;m thankful to them for
hosting me and Anaconda for sending me there. This article will talk about some
improvements we made to improve training scikit-learn models using a cluster.</p><p>Scikit-learn uses <a href=https://pythonhosted.org/joblib/parallel.html>joblib</a> for
simple parallelism in many places. Anywhere you pass an <code>n_jobs</code> keyword,
scikit-learn is internally calling <code>joblib.Parallel(...)</code>, and doing a batch of
work in parallel. The estimator may have an embarrassingly parallel step
internally (fitting each of the trees in a <code>RandomForest</code> for example). Or your
meta-estimator like <code>GridSearchCV</code> may try out many combinations of
hyper-parameters in parallel.</p><p>You can think of joblib as a broker between the user and the algorithm author.
The user comes along and says, &ldquo;I have <code>n_jobs</code> cores, please use them!&rdquo;.
Scikit-Learn says &ldquo;I have all these embarrassingly parallel tasks to be run as
part of fitting this estimator.&rdquo; Joblib accepts the cores from the user and the
tasks from scikit-learn, runs the tasks on the cores, and hands the completed
tasks back to scikit-learn.</p><p>Joblib offers a few &ldquo;backends&rdquo; for how to do your parallelism, but they all boil
down to using many processes versus using many threads.</p><h2 id=parallelism-in-python>Parallelism in Python<a hidden class=anchor aria-hidden=true href=#parallelism-in-python>#</a></h2><p>A quick digression on <em>single-machine</em> parallelism in Python. We can&rsquo;t say up
front that using threads is always better or worse than using processes.
Unfortunately the relative performance depends on the specific workload. But we
do have some general heuristics that come down to serialization overhead and
Python&rsquo;s Global Interpreter Lock (GIL).</p><p>The GIL is part of CPython, the C program that interprets and runs your Python
program. It limits your Python process so that only one thread is executing
<em>Python</em> at once, defeating your parallelism. Fortunately, much of the numerical
Python stack is written in C, Cython, C++, Fortran, or numba, and <em>may</em> be able
to release the GIL. This means your &ldquo;Python&rdquo; program, which is calling into
Cython or C via NumPy or pandas, can get real thread-based parallelism without
being limited by the GIL. The main caveat here that manipulating strings or
Python objects (lists, dicts, sets, etc) typically requires holding the GIL.</p><p>So, if we have the <em>option</em> of choosing threads or processes, which do we want?
For most numeric / scientific workloads, threads are better than processes
because of <em>shared memory</em>. Each thread in a thread-pool can view (and modify!)
the <em>same</em> large NumPy array. With multiple processes, data must be <em>serialized</em>
between processes (perhaps using pickle). For large arrays or dataframes this
can be slow, and it may blow up your memory if the data a decent fraction of
your machine&rsquo;s RAM. You&rsquo;ll have a full copy in each processes.</p><p>See <a href=http://matthewrocklin.com/blog/work/2015/03/10/PyData-GIL>Matthew Rocklin&rsquo;s
article</a> and <a href=http://www.dabeaz.com/GIL/>David
Beazley&rsquo;s page</a> if you want to learn more.</p><h2 id=distributed-training-with-daskdistributed>Distributed Training with dask.distributed<a hidden class=anchor aria-hidden=true href=#distributed-training-with-daskdistributed>#</a></h2><p>For a while now, you&rsquo;ve been able to use
<a href=http://distributed.readthedocs.io/en/latest/><code>dask.distributed</code></a> as a
backend for joblib. This means that in <em>most</em> places scikit-learn offers an
<code>n_jobs</code> keyword, you&rsquo;re able to do the parallel computation on your cluster.</p><p>This is great when</p><ol><li>Your dataset is not too large (since the data must be sent to each worker)</li><li>The runtime of each task is long enough that the overhead of serializing the
data across the network to the worker doesn&rsquo;t dominate the runtime</li><li>You have many parallel tasks to run (else, you&rsquo;d just use a local thread or
process pool and avoid the network delay)</li></ol><p>Fitting a <code>RandomForest</code> is a good example of this. Each tree in a forest may be
built independently of every other tree. This next code chunk shows how you can
parallelize fitting a <code>RandomForestClassifier</code> across a cluster, though as
discussed later this won&rsquo;t work on the currently released versions of
scikit-learn and joblib.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.externals <span style=color:#f92672>import</span> joblib
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dask.distributed <span style=color:#f92672>import</span> Client
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> distributed.joblib  <span style=color:#75715e># register the joblib backend</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> Client(<span style=color:#e6db74>&#39;dask-scheduler:8786&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> joblib<span style=color:#f92672>.</span>parallel_backend(<span style=color:#e6db74>&#34;dask&#34;</span>, scatter<span style=color:#f92672>=</span>[X_train, y_train]):
</span></span><span style=display:flex><span>    clf<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span></code></pre></div><p>The <code>.fit</code> call is parallelized across all the workers in your cluster. Here&rsquo;s
the distributed dashboard during that training.</p><video src=/images/distributed-joblib-cluster.webm autoplay controls loop width=80%>
Your browser doesn't support HTML5 video.</video><p>The center pane shows the task stream as they complete. Each rectangle is a
single task, building a single tree in a random forest in this case. Workers are
represented vertically. My cluster had 8 workers with 4 cores each, which means
up to 32 tasks can be processed simultaneously. We fit the 200 trees in about 20
seconds.</p><h2 id=changes-to-joblib>Changes to Joblib<a hidden class=anchor aria-hidden=true href=#changes-to-joblib>#</a></h2><p>Above, I said that distributed training worked in <em>most</em> places in scikit-learn.
Getting it to work everywhere required a bit more work, and was part of last
week&rsquo;s focus.</p><p>First, <code>dask.distributed</code>&rsquo;s joblib backend didn&rsquo;t handle <em>nested</em> parallelism
well. This may occur if you do something like</p><pre tabindex=0><code class=language-pytohn data-lang=pytohn>gs = GridSearchCV(Estimator(n_jobs=-1), n_jobs=-1)
gs.fit(X, y)
</code></pre><p>Previously, that caused deadlocks. Inside <code>GridSearchCV</code>, there&rsquo;s a call like</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># In GridSearchCV.fit, the outer layer</span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> joblib<span style=color:#f92672>.</span>Parallel(n_jobs<span style=color:#f92672>=</span>n_jobs)(fit_estimator)(<span style=color:#f92672>...</span>)
</span></span></code></pre></div><p>where <code>fit_estimator</code> is a function that <em>itself</em> tries to do things in parallel</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># In fit_estimator, the inner layer</span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> joblib<span style=color:#f92672>.</span>Parallel(n_jobs<span style=color:#f92672>=</span>n_jobs)(fit_one)(<span style=color:#f92672>...</span>)
</span></span></code></pre></div><p>So the outer level kicks off a bunch of <code>joblib.Parallel</code> calls, and waits
around for the results. For each of those <code>Parallel</code> calls, the inner level
tries to make a bunch of <code>joblib.Parallel</code> calls. When joblib tried to start the
inner ones, it would ask the distributed scheduler for a free worker. But all
the workers were &ldquo;busy&rdquo; waiting around for the outer <code>Parallel</code> calls to finish,
which weren&rsquo;t progressing because there weren&rsquo;t any free workers! Deadlock!</p><p><code>dask.distributed</code> has a solution for this case (workers
<a href=http://distributed.readthedocs.io/en/latest/api.html#distributed.secede><code>secede</code></a>
from the thread pool when they start a long-running <code>Parllel</code> call, and
<a href=http://distributed.readthedocs.io/en/latest/api.html#distributed.rejoin><code>rejoin</code></a>
when they&rsquo;re done), but we needed a way to negotiate with joblib about when the
<code>secede</code> and <code>rejoin</code> should happen. Joblib now has an API for backends to
control some setup and teardown around the actual function execution. This work
was done in <a href=https://github.com/joblib/joblib/pull/538>Joblib #538</a> and
<a href=https://github.com/dask/distributed/pull/1705>dask-distributed #1705</a>.</p><p>Second, some places in scikit-learn hard-code the backend they want to use in
their <code>Parallel()</code> call, meaning the cluster isn&rsquo;t used. This may be because the
algorithm author knows that one backend performs better than others. For
example, <code>RandomForest.fit</code> performs better with threads, since it&rsquo;s purely
numeric and releases the GIL. In this case we would say the <code>Parallel</code> call
<em>prefers</em> threads, since you&rsquo;d get the same result with processes, it&rsquo;d just be
slower.</p><p>Another reason for hard-coding the backend is if the <em>correctness</em> of the
implementation relies on it. For example, <code>RandomForest.predict</code> preallocates
the output array and mutates it from many threads (it knows not to mutate the
same place from multiple threads). In this case, we&rsquo;d say the <code>Parallel</code> call
<em>requires</em> shared memory, because you&rsquo;d get an incorrect result using processes.</p><p>The solution was to enhance <code>joblib.Parallel</code> to take two new keywords, <code>prefer</code>
and <code>require</code>. If a <code>Parallel</code> call <em>prefers</em> threads, it&rsquo;ll use them, unless
it&rsquo;s in a context saying &ldquo;use this backend instead&rdquo;, like</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> joblib<span style=color:#f92672>.</span>Parallel(n_jobs<span style=color:#f92672>=</span>n_jobs, prefer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;threads&#34;</span>)(<span style=color:#f92672>...</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> joblib<span style=color:#f92672>.</span>parallel_backend(<span style=color:#e6db74>&#39;dask&#39;</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This uses dask&#39;s workers, not threads</span>
</span></span><span style=display:flex><span>    fit()
</span></span></code></pre></div><p>On the other hand, if a <code>Parallel</code> requires a specific backend, it&rsquo;ll get it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> joblib<span style=color:#f92672>.</span>Parallel(n_jobs<span style=color:#f92672>=</span>n_jobs, require<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sharedmem&#34;</span>)(<span style=color:#f92672>...</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> joblib<span style=color:#f92672>.</span>parallel_backend(<span style=color:#e6db74>&#39;dask&#39;</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This uses the threading backend, since shared memory is required</span>
</span></span><span style=display:flex><span>    fit()
</span></span></code></pre></div><p>This is a elegant way to negotiate a compromise between</p><ol><li>The <em>user</em>, who knows best about what resources are available, as specified
by the <code>joblib.parallel_backend</code> context manager. And,</li><li>The <em>algorithm author</em>, who knows best about the GIL handling and shared
memory requirements.</li></ol><p>This work was done in <a href=https://github.com/joblib/joblib/pull/602>Joblib #602</a>.</p><p>After the next joblib release, scikit-learn will be updated to use these options
in places where the backend is currently hard-coded. My example above used a
branch with those changes.</p><p>Look forward for these changes in the upcoming joblib, dask, and scikit-learn
releases. As always, let me know if you have any feedback.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://tomaugspurger.net>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>