<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>dask-ml | Tom's Blog</title>
<meta name=keywords content><meta name=description content="Today we released the first version of dask-ml, a library for parallel and distributed machine learning. Read the documentation or install it with
pip install dask-ml Packages are currently building for conda-forge, and will be up later today.
conda install -c conda-forge dask-ml The Goals dask is, to quote the docs, &ldquo;a flexible parallel computing library for analytic computing.&rdquo; dask.array and dask.dataframe have done a great job scaling NumPy arrays and pandas dataframes; dask-ml hopes to do the same in the machine learning domain."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/dask-ml-announce/><link crossorigin=anonymous href=/assets/css/stylesheet.ced21e6d3497ee93fed8f8b357448095840179bd510b5ea0e6013078712e6dd1.css integrity="sha256-ztIebTSX7pP+2PizV0SAlYQBeb1RC16g5gEweHEubdE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tomaugspurger.net/posts/dask-ml-announce/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="dask-ml"><meta property="og:description" content="Today we released the first version of dask-ml, a library for parallel and distributed machine learning. Read the documentation or install it with
pip install dask-ml Packages are currently building for conda-forge, and will be up later today.
conda install -c conda-forge dask-ml The Goals dask is, to quote the docs, &ldquo;a flexible parallel computing library for analytic computing.&rdquo; dask.array and dask.dataframe have done a great job scaling NumPy arrays and pandas dataframes; dask-ml hopes to do the same in the machine learning domain."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/dask-ml-announce/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-10-26T00:00:00+00:00"><meta property="article:modified_time" content="2017-10-26T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="dask-ml"><meta name=twitter:description content="Today we released the first version of dask-ml, a library for parallel and distributed machine learning. Read the documentation or install it with
pip install dask-ml Packages are currently building for conda-forge, and will be up later today.
conda install -c conda-forge dask-ml The Goals dask is, to quote the docs, &ldquo;a flexible parallel computing library for analytic computing.&rdquo; dask.array and dask.dataframe have done a great job scaling NumPy arrays and pandas dataframes; dask-ml hopes to do the same in the machine learning domain."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":2,"name":"dask-ml","item":"https://tomaugspurger.net/posts/dask-ml-announce/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"dask-ml","name":"dask-ml","description":"Today we released the first version of dask-ml, a library for parallel and distributed machine learning. Read the documentation or install it with\npip install dask-ml Packages are currently building for conda-forge, and will be up later today.\nconda install -c conda-forge dask-ml The Goals dask is, to quote the docs, \u0026ldquo;a flexible parallel computing library for analytic computing.\u0026rdquo; dask.array and dask.dataframe have done a great job scaling NumPy arrays and pandas dataframes; dask-ml hopes to do the same in the machine learning domain.","keywords":[],"articleBody":"Today we released the first version of dask-ml, a library for parallel and distributed machine learning. Read the documentation or install it with\npip install dask-ml Packages are currently building for conda-forge, and will be up later today.\nconda install -c conda-forge dask-ml The Goals dask is, to quote the docs, “a flexible parallel computing library for analytic computing.” dask.array and dask.dataframe have done a great job scaling NumPy arrays and pandas dataframes; dask-ml hopes to do the same in the machine learning domain.\nPut simply, we want\nest = MyEstimator() est.fit(X, y) to work well in parallel and potentially distributed across a cluster. dask provides us with the building blocks to do that.\nWhat’s Been Done dask-ml collects some efforts that others already built:\ndistributed joblib: scaling out some scikit-learn operations to clusters (from distributed.joblib) hyper-parameter search: Some drop in replacements for scikit-learn’s GridSearchCV and RandomizedSearchCV classes (from dask-searchcv) distributed GLMs: Fit large Generalized Linear Models on your cluster (from dask-glm) dask + xgboost: Peer a dask.distributed cluster with XGBoost running in distributed mode (from dask-xgboost) dask + tensorflow: Peer a dask.distributed cluster with TensorFlow running in distributed mode (from dask-tensorflow) Out-of-core learning in pipelines: Reuse scikit-learn’s out-of-core .partial_fit API in pipelines (from dask.array.learn) In addition to providing a single home for these existing efforts, we’ve implemented some algorithms that are designed to run in parallel and distributed across a cluster.\nKMeans: Uses the k-means|| algorithm for initialization, and a parallelized Lloyd’s algorithm for the EM step. Preprocessing: These are estimators that can be dropped into scikit-learn Pipelines, but they operate in parallel on dask collections. They’ll work well on datasets in distributed memory, and may be faster for NumPy arrays (depending on the overhead from parallelizing, and whether or not the scikit-learn implementation is already parallel). Help Contribute! Scikit-learn is a robust, mature, stable library. dask-ml is… not. Which means there are plenty of places to contribute! Dask makes writing parallel and distributed implementations of algorithms fun. For the most part, you don’t even have to think about “where’s my data? How do I parallelize this?” Dask does that for you.\nHave a look at the issues or propose a new one. I’d love to hear issues that you’ve run into when scaling the “traditional” scientific python stack out to larger problems.\n","wordCount":"384","inLanguage":"en","datePublished":"2017-10-26T00:00:00Z","dateModified":"2017-10-26T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/dask-ml-announce/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net/ accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">dask-ml</h1><div class=post-meta><span title='2017-10-26 00:00:00 +0000 UTC'>October 26, 2017</span></div></header><div class=post-content><p>Today we released the first version of <code>dask-ml</code>, a library for parallel and
distributed machine learning. Read the <a href=http://dask-ml.readthedocs.io/en/latest/>documentation</a> or install it with</p><pre tabindex=0><code>pip install dask-ml
</code></pre><p>Packages are currently building for conda-forge, and will be up later today.</p><pre tabindex=0><code>conda install -c conda-forge dask-ml
</code></pre><h2 id=the-goals>The Goals<a hidden class=anchor aria-hidden=true href=#the-goals>#</a></h2><p><a href=http://dask.pydata.org/en/latest/>dask</a> is, to quote the docs, &ldquo;a flexible parallel computing library for
analytic computing.&rdquo; <code>dask.array</code> and <code>dask.dataframe</code> have done a great job
scaling NumPy arrays and pandas dataframes; <code>dask-ml</code> hopes to do the same in
the machine learning domain.</p><p>Put simply, we want</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>est <span style=color:#f92672>=</span> MyEstimator()
</span></span><span style=display:flex><span>est<span style=color:#f92672>.</span>fit(X, y)
</span></span></code></pre></div><p>to work well in parallel and potentially distributed across a cluster. <code>dask</code>
provides us with the building blocks to do that.</p><h2 id=whats-been-done>What&rsquo;s Been Done<a hidden class=anchor aria-hidden=true href=#whats-been-done>#</a></h2><p><code>dask-ml</code> collects some efforts that others already built:</p><ul><li><a href=http://dask-ml.readthedocs.io/en/latest/joblib.html>distributed joblib</a>:
scaling out some scikit-learn operations to clusters (from
<code>distributed.joblib</code>)</li><li><a href=http://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html>hyper-parameter
search</a>:
Some drop in replacements for scikit-learn&rsquo;s <code>GridSearchCV</code> and
<code>RandomizedSearchCV</code> classes (from <code>dask-searchcv</code>)</li><li><a href=http://dask-ml.readthedocs.io/en/latest/glm.html>distributed GLMs</a>: Fit
large Generalized Linear Models on your cluster (from <code>dask-glm</code>)</li><li><a href=http://dask-ml.readthedocs.io/en/latest/xgboost.html>dask + xgboost</a>: Peer a
<code>dask.distributed</code> cluster with XGBoost running in distributed mode (from
<code>dask-xgboost</code>)</li><li><a href=http://dask-ml.readthedocs.io/en/latest/tensorflow.html>dask + tensorflow</a>:
Peer a <code>dask.distributed</code> cluster with TensorFlow running in distributed mode
(from <code>dask-tensorflow</code>)</li><li><a href=http://dask-ml.readthedocs.io/en/latest/incremental.html>Out-of-core learning in
pipelines</a>: Reuse
scikit-learn&rsquo;s out-of-core <code>.partial_fit</code> API in pipelines (from
<code>dask.array.learn</code>)</li></ul><p>In addition to providing a single home for these existing efforts, we&rsquo;ve
implemented some algorithms that are designed to run in parallel and distributed
across a cluster.</p><ul><li><a href=http://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.cluster.KMeans.html#dask_ml.cluster.KMeans><code>KMeans</code></a>:
Uses the <code>k-means||</code> algorithm for initialization, and a parallelized Lloyd&rsquo;s
algorithm for the EM step.</li><li><a href=http://dask-ml.readthedocs.io/en/latest/modules/api.html#module-dask_ml.preprocessing>Preprocessing</a>:
These are estimators that can be dropped into scikit-learn Pipelines, but they
operate in parallel on dask collections. They&rsquo;ll work well on datasets in
distributed memory, and may be faster for NumPy arrays (depending on the
overhead from parallelizing, and whether or not the scikit-learn
implementation is already parallel).</li></ul><h2 id=help-contribute>Help Contribute!<a hidden class=anchor aria-hidden=true href=#help-contribute>#</a></h2><p>Scikit-learn is a robust, mature, stable library. <code>dask-ml</code> is&mldr; not. Which
means there are plenty of places to contribute! Dask makes writing parallel and
distributed implementations of algorithms fun. For the most part, you don&rsquo;t even
have to think about &ldquo;where&rsquo;s my data? How do I parallelize this?&rdquo; Dask does that
for you.</p><p>Have a look at the <a href=https://github.com/dask/dask-ml/issues>issues</a> or propose a
new one. I&rsquo;d love to hear issues that you&rsquo;ve run into when scaling the
&ldquo;traditional&rdquo; scientific python stack out to larger problems.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://tomaugspurger.net/>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>