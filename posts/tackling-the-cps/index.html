<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using Python to tackle the CPS | Tom's Blog</title>
<meta name=keywords content="pandas"><meta name=description content="The Current Population Survey is an important source of data for economists. It&rsquo;s modern form took shape in the 70&rsquo;s and unfortunately the data format and distribution shows its age. Some centers like IPUMS have attempted to put a nicer face on accessing the data, but they haven&rsquo;t done everything yet. In this series I&rsquo;ll describe methods I used to fetch, parse, and analyze CPS data for my second year paper. Today I&rsquo;ll describe fetching the data. Everything is available at the paper&rsquo;s GitHub Repository."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/tackling-the-cps/><link crossorigin=anonymous href=/assets/css/stylesheet.ced21e6d3497ee93fed8f8b357448095840179bd510b5ea0e6013078712e6dd1.css integrity="sha256-ztIebTSX7pP+2PizV0SAlYQBeb1RC16g5gEweHEubdE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tomaugspurger.net/posts/tackling-the-cps/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Using Python to tackle the CPS"><meta property="og:description" content="The Current Population Survey is an important source of data for economists. It&rsquo;s modern form took shape in the 70&rsquo;s and unfortunately the data format and distribution shows its age. Some centers like IPUMS have attempted to put a nicer face on accessing the data, but they haven&rsquo;t done everything yet. In this series I&rsquo;ll describe methods I used to fetch, parse, and analyze CPS data for my second year paper. Today I&rsquo;ll describe fetching the data. Everything is available at the paper&rsquo;s GitHub Repository."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/tackling-the-cps/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2014-01-27T00:00:00+00:00"><meta property="article:modified_time" content="2014-01-27T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Using Python to tackle the CPS"><meta name=twitter:description content="The Current Population Survey is an important source of data for economists. It&rsquo;s modern form took shape in the 70&rsquo;s and unfortunately the data format and distribution shows its age. Some centers like IPUMS have attempted to put a nicer face on accessing the data, but they haven&rsquo;t done everything yet. In this series I&rsquo;ll describe methods I used to fetch, parse, and analyze CPS data for my second year paper. Today I&rsquo;ll describe fetching the data. Everything is available at the paper&rsquo;s GitHub Repository."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":2,"name":"Using Python to tackle the CPS","item":"https://tomaugspurger.net/posts/tackling-the-cps/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using Python to tackle the CPS","name":"Using Python to tackle the CPS","description":"The Current Population Survey is an important source of data for economists. It\u0026rsquo;s modern form took shape in the 70\u0026rsquo;s and unfortunately the data format and distribution shows its age. Some centers like IPUMS have attempted to put a nicer face on accessing the data, but they haven\u0026rsquo;t done everything yet. In this series I\u0026rsquo;ll describe methods I used to fetch, parse, and analyze CPS data for my second year paper. Today I\u0026rsquo;ll describe fetching the data. Everything is available at the paper\u0026rsquo;s GitHub Repository.\n","keywords":["pandas"],"articleBody":"The Current Population Survey is an important source of data for economists. It’s modern form took shape in the 70’s and unfortunately the data format and distribution shows its age. Some centers like IPUMS have attempted to put a nicer face on accessing the data, but they haven’t done everything yet. In this series I’ll describe methods I used to fetch, parse, and analyze CPS data for my second year paper. Today I’ll describe fetching the data. Everything is available at the paper’s GitHub Repository.\nBefore diving in, you should know a bit about the data. I was working with the monthly microdata files from the CPS. These are used to estimate things like the unemployment rate you see reported every month. Since around 2002, about 60,000 households are interviewed 8 times each over a year. They’re interviewed for 4 months, take 4 months off, and are interviewed for 4 more months after the break. Questions are asked about demographics, education, economic activity (and more).\nFetching the Data This was probably the easiest part of the whole project. The CPS website has links to all the monthly files and some associated data dictionaries describing the layout of the files (more on this later).\nIn monthly_data_downloader.py I fetch files from the CPS website and save them locally. A common trial was the CPS’s inconsistency. Granted, consistency and backwards compatibility are difficult, and sometimes there are valid reasons for making a break, but at times the changes felt excessive and random. Anyway for January 1976 to December 2009 the URL pattern is http://www.nber.org/cps-basic/cpsb****.Z, and from January 2010 on its http://www.nber.org/cps-basic/jan10.\nIf you’re curious the python regex used to match those two patterns is re.compile(r'cpsb\\d{4}.Z|\\w{3}\\d{2}pub.zip|\\.[ddf,asc]$'). Yes that’s much clearer.\nI used python’s builtin urllib2 to fetch the site contents and parse with lxml. You should really just use requests, instead of urllib2 but I wanted to keep dependencies for my project slim (I gave up on this hope later).\nA common pattern I used was to parse all of the links on a website, filter out the ones I don’t want, and do something with the ones I do want. Here’s an example:\nfor link in ifilter(partial_matcher, root.iterlinks()): _, _, _fname, _ = link fname = _fname.split('/')[-1] existing = _exists(os.path.join(out_dir, fname)) if not existing: downloader(fname, out_dir) print('Added {}'.format(fname)) root is just the parsed html from lxml.parse. iterlinks() returns an iterable, which I filter through partial_matcher, a function that matches the filename patterns I described above. Iterators are my favorite feature of Python (not that they are exclusive to Python; I just love easy and flexible they are). The idea of having a list, filtering it, and applying a function to the ones you want is so simple, but so generally applicable. I could have even been a bit more functional and written it as imap(downloader(ifilter(existing, ifilter(partial_matcher, root.iterlinks())). Lovely in its own way!\nI do some checking to see if the file exists (so that I can easily download new months). If it is a new month, the filename gets passed to downloader:\ndef downloader(link, out_dir, dl_base=\"http://www.nber.org/cps-basic/\"): \"\"\" Link is a str like cpsmar06.zip; It is both the end of the url and the filename to be used. \"\"\" content = urllib2.urlopen(dl_base + link) with open(out_dir + link, 'w') as f: f.write(content.read()) This reads the data from at url and write writes it do a file.\nFinally, I run renamer.py to clean up the file names. Just because the CPS is inconsistent doesn’t mean that we have to be.\nIn the next post I’ll describe parsing the files we just downloaded.\n","wordCount":"597","inLanguage":"en","datePublished":"2014-01-27T00:00:00Z","dateModified":"2014-01-27T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/tackling-the-cps/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net/ accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Using Python to tackle the CPS</h1><div class=post-meta><span title='2014-01-27 00:00:00 +0000 UTC'>January 27, 2014</span></div></header><div class=post-content><p>The <a href=http://www.census.gov/cps/>Current Population Survey</a> is an important source of data for economists. It&rsquo;s modern form took shape in the 70&rsquo;s and unfortunately the data format and distribution shows its age. Some centers like <a href=https://cps.ipums.org/cps/>IPUMS</a> have attempted to put a nicer face on accessing the data, but they haven&rsquo;t done everything yet. In this series I&rsquo;ll describe methods I used to fetch, parse, and analyze CPS data for my second year paper. Today I&rsquo;ll describe fetching the data. Everything is available at the paper&rsquo;s <a href=https://github.com/TomAugspurger/dnwr-zlb>GitHub Repository</a>.</p><p>Before diving in, you should know a bit about the data. I was working with the monthly microdata files from the CPS. These are used to estimate things like the unemployment rate you see reported every month. Since around 2002, about 60,000 households are interviewed 8 times each over a year. They&rsquo;re interviewed for 4 months, take 4 months off, and are interviewed for 4 more months after the break. Questions are asked about demographics, education, economic activity (and more).</p><h2 id=fetching-the-data>Fetching the Data<a hidden class=anchor aria-hidden=true href=#fetching-the-data>#</a></h2><p>This was probably the easiest part of the whole project.
The <a href=http://www.nber.org/data/cps_basic.html>CPS website</a> has links to all the monthly files and some associated data dictionaries describing the layout of the files (more on this later).</p><p>In <a href=https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/monthly_data_downloader.py><code>monthly_data_downloader.py</code></a> I fetch files from the CPS website and save them locally. A common trial was the CPS&rsquo;s inconsistency. Granted, consistency and backwards compatibility are difficult, and sometimes there are valid reasons for making a break, but at times the changes felt excessive and random. Anyway for January 1976 to December 2009 the URL pattern is <code>http://www.nber.org/cps-basic/cpsb****.Z</code>, and from January 2010 on its <code>http://www.nber.org/cps-basic/jan10</code>.</p><p>If you&rsquo;re curious the python regex used to match those two patterns is <code>re.compile(r'cpsb\d{4}.Z|\w{3}\d{2}pub.zip|\.[ddf,asc]$')</code>. Yes that&rsquo;s much clearer.</p><p>I used python&rsquo;s builtin <a href=http://docs.python.org/2/library/urllib2.html><code>urllib2</code></a> to fetch the site contents and parse with <code>lxml</code>. You should <em>really</em> just use <a href=http://docs.python-requests.org/en/latest/>requests</a>, instead of <code>urllib2</code> but I wanted to keep dependencies for my project slim (I gave up on this hope later).</p><p>A common pattern I used was to parse all of the links on a website, filter out the ones I don&rsquo;t want, and do something with the ones I do want. Here&rsquo;s an example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> link <span style=color:#f92672>in</span> ifilter(partial_matcher, root<span style=color:#f92672>.</span>iterlinks()):
</span></span><span style=display:flex><span>    _, _, _fname, _ <span style=color:#f92672>=</span> link
</span></span><span style=display:flex><span>    fname <span style=color:#f92672>=</span> _fname<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;/&#39;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    existing <span style=color:#f92672>=</span> _exists(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(out_dir, fname))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> existing:
</span></span><span style=display:flex><span>        downloader(fname, out_dir)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#39;Added </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>format(fname))
</span></span></code></pre></div><p><code>root</code> is just the parsed html from <code>lxml.parse</code>. <code>iterlinks()</code> returns an iterable, which I filter through <code>partial_matcher</code>, a function that matches the filename patterns I described above. Iterators are my favorite feature of Python (not that they are exclusive to Python; I just love easy and flexible they are). The idea of having a list, filtering it, and applying a function to the ones you want is so simple, but so generally applicable. I could have even been a bit more functional and written it as <code>imap(downloader(ifilter(existing, ifilter(partial_matcher, root.iterlinks()))</code>. Lovely in its own way!</p><p>I do some checking to see if the file exists (so that I can easily download new months). If it is a new month, the filename gets passed to <code>downloader</code>:</p><pre tabindex=0><code>def downloader(link, out_dir, dl_base=&#34;http://www.nber.org/cps-basic/&#34;):
    &#34;&#34;&#34;
    Link is a str like cpsmar06.zip; It is both the end of the url
    and the filename to be used.
    &#34;&#34;&#34;
    content = urllib2.urlopen(dl_base + link)
    with open(out_dir + link, &#39;w&#39;) as f:
        f.write(content.read())
</code></pre><p>This reads the data from at url and write writes it do a file.</p><p>Finally, I run <a href=https://github.com/TomAugspurger/dnwr-zlb/blob/master/data_wrangling/cps_wrangling/panel_construction/renamer.py><code>renamer.py</code></a> to clean up the file names. Just because the CPS is inconsistent doesn&rsquo;t mean that we have to be.</p><p>In the <a href=http://tomaugspurger.net/blog/2014/02/04/tackling%20the%20cps%20(part%202)/>next post</a> I&rsquo;ll describe parsing the files we just downloaded.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tomaugspurger.net/tags/pandas/>Pandas</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://tomaugspurger.net/>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>