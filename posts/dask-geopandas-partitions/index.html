<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Dask-GeoPandas Spatial Partitioning Performance | Tom's Blog</title>
<meta name=keywords content><meta name=description content="A college reached out yesterday about a performance issue they were hitting when working with the Microsoft Building Footprints dataset we host on the Planetary Computer. They wanted to get the building footprints for a small section of Turkey, but noticed that the performance was relatively slow and it seemed like a lot of data was being read.
This post details how we debugged what was going on, and the steps we took to fix it."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/dask-geopandas-partitions/><link crossorigin=anonymous href=/assets/css/stylesheet.ced21e6d3497ee93fed8f8b357448095840179bd510b5ea0e6013078712e6dd1.css integrity="sha256-ztIebTSX7pP+2PizV0SAlYQBeb1RC16g5gEweHEubdE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Dask-GeoPandas Spatial Partitioning Performance"><meta property="og:description" content="A college reached out yesterday about a performance issue they were hitting when working with the Microsoft Building Footprints dataset we host on the Planetary Computer. They wanted to get the building footprints for a small section of Turkey, but noticed that the performance was relatively slow and it seemed like a lot of data was being read.
This post details how we debugged what was going on, and the steps we took to fix it."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/dask-geopandas-partitions/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-09T08:10:59-06:00"><meta property="article:modified_time" content="2023-02-09T08:10:59-06:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dask-GeoPandas Spatial Partitioning Performance"><meta name=twitter:description content="A college reached out yesterday about a performance issue they were hitting when working with the Microsoft Building Footprints dataset we host on the Planetary Computer. They wanted to get the building footprints for a small section of Turkey, but noticed that the performance was relatively slow and it seemed like a lot of data was being read.
This post details how we debugged what was going on, and the steps we took to fix it."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":3,"name":"Dask-GeoPandas Spatial Partitioning Performance","item":"https://tomaugspurger.net/posts/dask-geopandas-partitions/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Dask-GeoPandas Spatial Partitioning Performance","name":"Dask-GeoPandas Spatial Partitioning Performance","description":"A college reached out yesterday about a performance issue they were hitting when working with the Microsoft Building Footprints dataset we host on the Planetary Computer. They wanted to get the building footprints for a small section of Turkey, but noticed that the performance was relatively slow and it seemed like a lot of data was being read.\nThis post details how we debugged what was going on, and the steps we took to fix it.","keywords":[],"articleBody":"A college reached out yesterday about a performance issue they were hitting when working with the Microsoft Building Footprints dataset we host on the Planetary Computer. They wanted to get the building footprints for a small section of Turkey, but noticed that the performance was relatively slow and it seemed like a lot of data was being read.\nThis post details how we debugged what was going on, and the steps we took to fix it.\nThe problem First, my college sent a minimal, complete, and verifiable example of the problem. This let me very easily reproduce it. From his report, the first thing I suspected was an issue with the spatial partitioning. The files were supposed to be partitioned by quadkey, so that all the building footprints in a single area are in the same partition. Then spatial queries will be very fast: you only need to load a small subset of the data.\nWhen I benchmarked things, it took about:\n16 seconds to read the metadata with dask_geopandas.read_parquet 60 seconds to read the data and clip it to the area of interest Looking at the spatial partitions of the data showed that it was clearly not spatially partitioned:\nIt’s clearer zoomed in, but the box is a bit fuzzy because it’s actually a bunch of boxes with very slightly different extents.\nTurns out we dropped a few of the newer ms-buildings STAC items, which were spatially partitioned, during our last release. Oops. (Don’t worry, we’re working on a better system for this.)\nOnce I got those items re-ingested, things did look better.\nIt wasn’t all good news, though. Our timings went to\n56 seconds to read the metadata with dask_geopandas.read_parquet (ouch) 0.5 seconds to read the data and clip it to the area of interest (yay!) The speedup from 60 seconds to 0.5 seconds is exactly why we want to spatially partition the data. When you’re querying for a small area of interest, the spatially partitioned data means you can ignore most of the data and speed things up a lot. But what’s going on with the slowdown for the first stage (reading metadata)?\nThe spatially partitioned dataset also had many more partitions in the Parquet dataset, i.e. many more individual files in Blob Storage (a few hundred instead of 5-6). At the moment, dask-geopandas needs to open each individual file to read its spatial bounds. That was fine when we only had a few files, but when you have a few hundred the small amount of time it takes to read each file adds up. In this case, it added up to about 56 seconds of waiting just to read the metadata.\nSpeeding up the metadata reading To speed up the metadata reading, we use the tried-and-true method of parallelizing it with Dask (yes, we’re using Dask to speed up Dask). Instead of doing a dask_geopandas.read_parquet on the client (which in turn executes some pyarrow.parquet stuff to read the fragments and get the metadata from each file) in serial, we’ll run a bunch of dask_geopandas.read_parquet calls on the cluster in parallel (I’m just using a LocalCluster in this example).\nThe snippet below re-uses dask_geopandas.read_parquet, but applies it in parallel using client.map. We’ll make a bunch of Dask DataFrames on the cluster (one per file) and then we use client.gather to bring back the Dask DataFrames (just the metadata, not the data!) to the client and concat them together into one big Dask DataFrame.\nThere’s a small bug in dask-geopandas around serializing the spatial partitions on a Dask GeoDataFrame. Once my fix is merged then this will be a bit cleaner: everything to do with spatial_partitions can be deleted and you’re just left with reading the metadata on the cluster, bringing it back to the client, and concatenating at the end.\nimport distributed import dask_geopandas import dask.dataframe as dd import fsspec def read_parquet(paths, storage_options): client = distributed.get_client() # Read each partition's metadata on the cluster df_futures = client.map( dask_geopandas.read_parquet, paths, storage_options=storage_options ) # workaround https://github.com/geopandas/dask-geopandas/issues/237 def get_spatial_partitions(x): return x.spatial_partitions spatial_partitions_futures = client.map(get_spatial_partitions, df_futures) # Pull back locally. This takes the most time, waiting for computation dfs, spatial_partitions = client.gather([df_futures, spatial_partitions_futures]) for df, sp in zip(dfs, spatial_partitions): df.spatial_partitions = None full_country = dd.concat(dfs) full_country.spatial_partitions = pd.concat(spatial_partitions, ignore_index=True) return full_country We get the paths with something like\n\u003e\u003e\u003e fs, token, [root] = fsspec.get_fs_token_paths(asset.href, storage_options=asset.extra_fields[\"table:storage_options\"]) # Get the raw paths (fast enough to do this locally. Could be done on the cluster too) \u003e\u003e\u003e paths = [f\"az://{p}\" for p in fs.ls(root)] Note that there’s an open issue on Dask to do this kind of thing by default.\nOverall, we brought the metadata read time down the 30 seconds (which would be faster with more workers). Still not great, but an improvement. At some point we’ll need to embrace a broader solution to this metadata access issue using something like Apache Iceberg.\nSee this example notebook for the full thing.\n","wordCount":"818","inLanguage":"en","datePublished":"2023-02-09T08:10:59-06:00","dateModified":"2023-02-09T08:10:59-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/dask-geopandas-partitions/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Dask-GeoPandas Spatial Partitioning Performance</h1><div class=post-meta><span title='2023-02-09 08:10:59 -0600 -0600'>February 9, 2023</span></div></header><div class=post-content><p>A college reached out yesterday about a performance issue they were hitting when
working with the <a href=https://planetarycomputer.microsoft.com/dataset/ms-buildings>Microsoft Building Footprints</a> dataset we host
on the Planetary Computer. They wanted to get the building footprints for a
small section of Turkey, but noticed that the performance was relatively slow
and it seemed like a lot of data was being read.</p><p>This post details how we debugged what was going on, and the steps we took to
fix it.</p><h2 id=the-problem>The problem<a hidden class=anchor aria-hidden=true href=#the-problem>#</a></h2><p>First, my college sent a <a href=https://matthewrocklin.com/minimal-bug-reports>minimal, complete, and verifiable example</a> of
the problem. This let me very easily reproduce it. From his report, the first
thing I suspected was an issue with the spatial partitioning. The files were
<em>supposed</em> to be partitioned by <a href=https://learn.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system>quadkey</a>, so that all the building
footprints in a single area are in the same partition. Then spatial queries will
be very fast: you only need to load a small subset of the data.</p><p>When I benchmarked things, it took about:</p><ol><li>16 seconds to read the metadata with <code>dask_geopandas.read_parquet</code></li><li>60 seconds to read the data and clip it to the area of interest</li></ol><p>Looking at the spatial partitions of the data showed that it was clearly not
spatially partitioned:</p><p><img loading=lazy src=/images/dask-geopandas-spatial-partitions-bad.png alt=non-partitioned></p><p>It&rsquo;s clearer zoomed in, but the box is a bit fuzzy because it&rsquo;s actually
a bunch of boxes with very slightly different extents.</p><p>Turns out we dropped a few of the newer ms-buildings STAC items, which were
spatially partitioned, during our last release. Oops.
(Don&rsquo;t worry, we&rsquo;re working on a better system for this.)</p><p>Once I got those items re-ingested, things did look better.</p><p><img loading=lazy src=/images/dask-geopandas-spatial-partitions-good.png alt=partitioned></p><p>It wasn&rsquo;t all good news, though. Our timings went to</p><ol><li>56 seconds to read the metadata with <code>dask_geopandas.read_parquet</code> (ouch)</li><li>0.5 seconds to read the data and clip it to the area of interest (yay!)</li></ol><p>The speedup from 60 seconds to 0.5 seconds is exactly why we want to spatially
partition the data. When you&rsquo;re querying for a small area of interest,
the spatially partitioned data means you can ignore most of the data and speed
things up a lot. But what&rsquo;s going on with the slowdown for the first stage
(reading metadata)?</p><p>The spatially partitioned dataset also had many more partitions in the Parquet
dataset, i.e. many more individual files in Blob Storage (a few hundred instead of 5-6).
At the moment, <code>dask-geopandas</code> needs to open each individual file to read its spatial bounds. That was fine
when we only had a few files, but when you have a few hundred the small amount of time it takes to read
each file adds up. In this case, it added up to about 56 seconds of waiting just to read the metadata.</p><h2 id=speeding-up-the-metadata-reading>Speeding up the metadata reading<a hidden class=anchor aria-hidden=true href=#speeding-up-the-metadata-reading>#</a></h2><p>To speed up the metadata reading, we use the tried-and-true method of parallelizing it
with Dask (yes, we&rsquo;re using Dask to speed up Dask). Instead of doing a <code>dask_geopandas.read_parquet</code>
on the client (which in turn executes some <code>pyarrow.parquet</code> stuff to read the fragments and get
the metadata from each file) in serial, we&rsquo;ll run a bunch of <code>dask_geopandas.read_parquet</code>
calls on the cluster in parallel (I&rsquo;m just using a <code>LocalCluster</code> in this example).</p><p>The snippet below re-uses <code>dask_geopandas.read_parquet</code>, but applies it in
parallel using <code>client.map</code>. We&rsquo;ll make a bunch of Dask DataFrames on the
cluster (one per file) and then we use <code>client.gather</code> to bring back the Dask
DataFrames (just the <em>metadata</em>, not the data!) to the client and concat them
together into one big Dask DataFrame.</p><p>There&rsquo;s a <a href=https://github.com/geopandas/dask-geopandas/issues/237>small bug</a> in dask-geopandas around serializing the spatial
partitions on a Dask GeoDataFrame. Once <a href=https://github.com/geopandas/dask-geopandas/pull/238>my fix</a> is merged then this will
be a bit cleaner: everything to do with <code>spatial_partitions</code> can be deleted
and you&rsquo;re just left with reading the metadata on the cluster, bringing it
back to the client, and concatenating at the end.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> distributed
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> dask_geopandas
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> dask.dataframe <span style=color:#66d9ef>as</span> dd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> fsspec
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>read_parquet</span>(paths, storage_options):
</span></span><span style=display:flex><span>    client <span style=color:#f92672>=</span> distributed<span style=color:#f92672>.</span>get_client()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Read each partition&#39;s metadata on the cluster</span>
</span></span><span style=display:flex><span>    df_futures <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>map(
</span></span><span style=display:flex><span>        dask_geopandas<span style=color:#f92672>.</span>read_parquet, paths, storage_options<span style=color:#f92672>=</span>storage_options
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># workaround https://github.com/geopandas/dask-geopandas/issues/237</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_spatial_partitions</span>(x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x<span style=color:#f92672>.</span>spatial_partitions
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    spatial_partitions_futures <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>map(get_spatial_partitions, df_futures)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Pull back locally. This takes the most time, waiting for computation</span>
</span></span><span style=display:flex><span>    dfs, spatial_partitions <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>gather([df_futures, spatial_partitions_futures])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> df, sp <span style=color:#f92672>in</span> zip(dfs, spatial_partitions):
</span></span><span style=display:flex><span>        df<span style=color:#f92672>.</span>spatial_partitions <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    full_country <span style=color:#f92672>=</span> dd<span style=color:#f92672>.</span>concat(dfs)
</span></span><span style=display:flex><span>    full_country<span style=color:#f92672>.</span>spatial_partitions <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>concat(spatial_partitions, ignore_index<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> full_country
</span></span></code></pre></div><p>We get the paths with something like</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> fs, token, [root] <span style=color:#f92672>=</span> fsspec<span style=color:#f92672>.</span>get_fs_token_paths(asset<span style=color:#f92672>.</span>href, storage_options<span style=color:#f92672>=</span>asset<span style=color:#f92672>.</span>extra_fields[<span style=color:#e6db74>&#34;table:storage_options&#34;</span>])
</span></span><span style=display:flex><span><span style=color:#75715e># Get the raw paths (fast enough to do this locally. Could be done on the cluster too)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> paths <span style=color:#f92672>=</span> [<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;az://</span><span style=color:#e6db74>{</span>p<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> fs<span style=color:#f92672>.</span>ls(root)]
</span></span></code></pre></div><p>Note that there&rsquo;s an <a href=https://github.com/dask/dask/issues/5380>open issue</a> on Dask to do this kind of thing by default.</p><p>Overall, we brought the metadata read time down the 30 seconds (which would be
faster with more workers). Still not great, but an improvement. At some point
we&rsquo;ll need to embrace a broader solution to this metadata access issue using
something like <a href=https://iceberg.apache.org/>Apache Iceberg</a>.</p><p>See <a href="https://notebooksharing.space/view/88055f29ae1c26b22f61a1ef5f673cf971f434f2e513933d8de2001d7f49162a#displayOptions=">this example notebook</a> for the full thing.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://tomaugspurger.net>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>