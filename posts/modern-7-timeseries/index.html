<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Modern Pandas (Part 7): Timeseries | Tom's Blog</title><meta name=keywords content="pandas"><meta name=description content="This is part 7 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Timeseries Pandas started out in the financial world, so naturally it has strong timeseries support.
The first half of this post will look at pandas&rsquo; capabilities for manipulating time series data. The second half will discuss modelling time series data with statsmodels.
%matplotlib inline import os import numpy as np import pandas as pd import pandas_datareader."><meta name=author content><link rel=canonical href=https://tomaugspurger.github.io/posts/modern-7-timeseries/><link crossorigin=anonymous href=/assets/css/stylesheet.67b4a5a78be69ca812d40e5543fbd83efd0b0cc5025c2505fd7758fd5d32ec2e.css integrity="sha256-Z7Slp4vmnKgS1A5VQ/vYPv0LDMUCXCUF/XdY/V0y7C4=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tomaugspurger.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Modern Pandas (Part 7): Timeseries"><meta property="og:description" content="This is part 7 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Timeseries Pandas started out in the financial world, so naturally it has strong timeseries support.
The first half of this post will look at pandas&rsquo; capabilities for manipulating time series data. The second half will discuss modelling time series data with statsmodels.
%matplotlib inline import os import numpy as np import pandas as pd import pandas_datareader."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.github.io/posts/modern-7-timeseries/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-05-13T00:00:00+00:00"><meta property="article:modified_time" content="2016-05-13T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Modern Pandas (Part 7): Timeseries"><meta name=twitter:description content="This is part 7 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Timeseries Pandas started out in the financial world, so naturally it has strong timeseries support.
The first half of this post will look at pandas&rsquo; capabilities for manipulating time series data. The second half will discuss modelling time series data with statsmodels.
%matplotlib inline import os import numpy as np import pandas as pd import pandas_datareader."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Modern Pandas (Part 7): Timeseries","item":"https://tomaugspurger.github.io/posts/modern-7-timeseries/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Modern Pandas (Part 7): Timeseries","name":"Modern Pandas (Part 7): Timeseries","description":"This is part 7 in my series on writing modern idiomatic pandas.\nModern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Timeseries Pandas started out in the financial world, so naturally it has strong timeseries support.\nThe first half of this post will look at pandas\u0026rsquo; capabilities for manipulating time series data. The second half will discuss modelling time series data with statsmodels.\n%matplotlib inline import os import numpy as np import pandas as pd import pandas_datareader.","keywords":["pandas"],"articleBody":" This is part 7 in my series on writing modern idiomatic pandas.\nModern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Timeseries Pandas started out in the financial world, so naturally it has strong timeseries support.\nThe first half of this post will look at pandas’ capabilities for manipulating time series data. The second half will discuss modelling time series data with statsmodels.\n%matplotlib inline import os import numpy as np import pandas as pd import pandas_datareader.data as web import seaborn as sns import matplotlib.pyplot as plt sns.set(style='ticks', context='talk') if int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)): import prep # noqa Let’s grab some stock data for Goldman Sachs using the pandas-datareader package, which spun off of pandas:\ngs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01', end='2010-01-01') gs.head().round(2) Open High Low Close Adj Close Volume Date 2006-01-03 126.70 129.44 124.23 128.87 112.34 6188700 2006-01-04 127.35 128.91 126.38 127.09 110.79 4861600 2006-01-05 126.00 127.32 125.61 127.04 110.74 3717400 2006-01-06 127.29 129.25 127.29 128.84 112.31 4319600 2006-01-09 128.50 130.62 128.00 130.39 113.66 4723500 There isn’t a special data-container just for time series in pandas, they’re just Series or DataFrames with a DatetimeIndex.\nSpecial Slicing Looking at the elements of gs.index, we see that DatetimeIndexes are made up of pandas.Timestamps:\nLooking at the elements of gs.index, we see that DatetimeIndexes are made up of pandas.Timestamps:\ngs.index[0] Timestamp('2006-01-03 00:00:00') A Timestamp is mostly compatible with the datetime.datetime class, but much amenable to storage in arrays.\nWorking with Timestamps can be awkward, so Series and DataFrames with DatetimeIndexes have some special slicing rules. The first special case is partial-string indexing. Say we wanted to select all the days in 2006. Even with Timestamp’s convenient constructors, it’s a pai\ngs.loc[pd.Timestamp('2006-01-01'):pd.Timestamp('2006-12-31')].head() Open High Low Close Adj Close Volume Date 2006-01-03 126.699997 129.440002 124.230003 128.869995 112.337547 6188700 2006-01-04 127.349998 128.910004 126.379997 127.089996 110.785889 4861600 2006-01-05 126.000000 127.320000 125.610001 127.040001 110.742340 3717400 2006-01-06 127.290001 129.250000 127.290001 128.839996 112.311401 4319600 2006-01-09 128.500000 130.619995 128.000000 130.389999 113.662605 4723500 Thanks to partial-string indexing, it’s as simple as\ngs.loc['2006'].head() Open High Low Close Adj Close Volume Date 2006-01-03 126.699997 129.440002 124.230003 128.869995 112.337547 6188700 2006-01-04 127.349998 128.910004 126.379997 127.089996 110.785889 4861600 2006-01-05 126.000000 127.320000 125.610001 127.040001 110.742340 3717400 2006-01-06 127.290001 129.250000 127.290001 128.839996 112.311401 4319600 2006-01-09 128.500000 130.619995 128.000000 130.389999 113.662605 4723500 Since label slicing is inclusive, this slice selects any observation where the year is 2006.\nThe second “convenience” is __getitem__ (square-bracket) fall-back indexing. I’m only going to mention it here, with the caveat that you should never use it. DataFrame __getitem__ typically looks in the column: gs['2006'] would search gs.columns for '2006', not find it, and raise a KeyError. But DataFrames with a DatetimeIndex catch that KeyError and try to slice the index. If it succeeds in slicing the index, the result like gs.loc['2006'] is returned. If it fails, the KeyError is re-raised. This is confusing because in pretty much every other case DataFrame.__getitem__ works on columns, and it’s fragile because if you happened to have a column '2006' you would get just that column, and no fall-back indexing would occur. Just use gs.loc['2006'] when slicing DataFrame indexes.\nSpecial Methods Resampling Resampling is similar to a groupby: you split the time series into groups (5-day buckets below), apply a function to each group (mean), and combine the result (one row per group).\ngs.resample(\"5d\").mean().head() Open High Low Close Adj Close Volume Date 2006-01-03 126.834999 128.730002 125.877501 127.959997 111.544294 4.771825e+06 2006-01-08 130.349998 132.645000 130.205002 131.660000 114.769649 4.664300e+06 2006-01-13 131.510002 133.395005 131.244995 132.924995 115.872357 3.258250e+06 2006-01-18 132.210002 133.853333 131.656667 132.543335 115.611125 4.997767e+06 2006-01-23 133.771997 136.083997 133.310001 135.153998 118.035918 3.968500e+06 gs.resample(\"W\").agg(['mean', 'sum']).head() Open High Low Close Adj Close Volume mean sum mean sum mean sum mean sum mean sum mean sum Date 2006-01-08 126.834999 507.339996 128.730002 514.920006 125.877501 503.510002 127.959997 511.839988 111.544294 446.177177 4771825.0 19087300 2006-01-15 130.684000 653.419998 132.848001 664.240006 130.544000 652.720001 131.979999 659.899994 115.048592 575.242958 4310420.0 21552100 2006-01-22 131.907501 527.630005 133.672501 534.690003 131.389999 525.559998 132.555000 530.220000 115.603432 462.413728 4653725.0 18614900 2006-01-29 133.771997 668.859986 136.083997 680.419983 133.310001 666.550003 135.153998 675.769989 118.035918 590.179588 3968500.0 19842500 2006-02-05 140.900000 704.500000 142.467999 712.339996 139.937998 699.689988 141.618002 708.090011 123.681204 618.406020 3920120.0 19600600 You can up-sample to convert to a higher frequency. The new points are filled with NaNs.\ngs.resample(\"6H\").mean().head() Open High Low Close Adj Close Volume Date 2006-01-03 00:00:00 126.699997 129.440002 124.230003 128.869995 112.337547 6188700.0 2006-01-03 06:00:00 NaN NaN NaN NaN NaN NaN 2006-01-03 12:00:00 NaN NaN NaN NaN NaN NaN 2006-01-03 18:00:00 NaN NaN NaN NaN NaN NaN 2006-01-04 00:00:00 127.349998 128.910004 126.379997 127.089996 110.785889 4861600.0 Rolling / Expanding / EW These methods aren’t unique to DatetimeIndexes, but they often make sense with time series, so I’ll show them here.\ngs.Close.plot(label='Raw') gs.Close.rolling(28).mean().plot(label='28D MA') gs.Close.expanding().mean().plot(label='Expanding Average') gs.Close.ewm(alpha=0.03).mean().plot(label='EWMA($\\\\alpha=.03$)') plt.legend(bbox_to_anchor=(1.25, .5)) plt.tight_layout() plt.ylabel(\"Close ($)\") sns.despine() Each of .rolling, .expanding, and .ewm return a deferred object, similar to a GroupBy.\nroll = gs.Close.rolling(30, center=True) roll Rolling [window=30,center=True,axis=0] m = roll.agg(['mean', 'std']) ax = m['mean'].plot() ax.fill_between(m.index, m['mean'] - m['std'], m['mean'] + m['std'], alpha=.25) plt.tight_layout() plt.ylabel(\"Close ($)\") sns.despine() Grab Bag Offsets These are similar to dateutil.relativedelta, but works with arrays.\ngs.index + pd.DateOffset(months=3, days=-2) DatetimeIndex(['2006-04-01', '2006-04-02', '2006-04-03', '2006-04-04', '2006-04-07', '2006-04-08', '2006-04-09', '2006-04-10', '2006-04-11', '2006-04-15', ... '2010-03-15', '2010-03-16', '2010-03-19', '2010-03-20', '2010-03-21', '2010-03-22', '2010-03-26', '2010-03-27', '2010-03-28', '2010-03-29'], dtype='datetime64[ns]', name='Date', length=1007, freq=None) Holiday Calendars There are a whole bunch of special calendars, useful for traders probabaly.\nfrom pandas.tseries.holiday import USColumbusDay USColumbusDay.dates('2015-01-01', '2020-01-01') DatetimeIndex(['2015-10-12', '2016-10-10', '2017-10-09', '2018-10-08', '2019-10-14'], dtype='datetime64[ns]', freq='WOM-2MON') Timezones Pandas works with pytz for nice timezone-aware datetimes. The typical workflow is\nlocalize timezone-naive timestamps to some timezone convert to desired timezone If you already have timezone-aware Timestamps, there’s no need for step one.\n# tz naiive -\u003e tz aware..... to desired UTC gs.tz_localize('US/Eastern').tz_convert('UTC').head() Open High Low Close Adj Close Volume Date 2006-01-03 05:00:00+00:00 126.699997 129.440002 124.230003 128.869995 112.337547 6188700 2006-01-04 05:00:00+00:00 127.349998 128.910004 126.379997 127.089996 110.785889 4861600 2006-01-05 05:00:00+00:00 126.000000 127.320000 125.610001 127.040001 110.742340 3717400 2006-01-06 05:00:00+00:00 127.290001 129.250000 127.290001 128.839996 112.311401 4319600 2006-01-09 05:00:00+00:00 128.500000 130.619995 128.000000 130.389999 113.662605 4723500 Modeling Time Series The rest of this post will focus on time series in the econometric sense. My indented reader for this section isn’t all that clear, so I apologize upfront for any sudden shifts in complexity. I’m roughly targeting material that could be presented in a first or second semester applied statisctics course. What follows certainly isn’t a replacement for that. Any formality will be restricted to footnotes for the curious. I’ve put a whole bunch of resources at the end for people earger to learn more.\nWe’ll focus on modelling Average Monthly Flights. Let’s download the data. If you’ve been following along in the series, you’ve seen most of this code before, so feel free to skip.\nimport os import io import glob import zipfile from utils import download_timeseries import statsmodels.api as sm def download_many(start, end): months = pd.period_range(start, end=end, freq='M') # We could easily parallelize this loop. for i, month in enumerate(months): download_timeseries(month) def time_to_datetime(df, columns): ''' Combine all time items into datetimes. 2014-01-01,1149.0 -\u003e 2014-01-01T11:49:00 ''' def converter(col): timepart = (col.astype(str) .str.replace('\\.0$', '') # NaNs force float dtype .str.pad(4, fillchar='0')) return pd.to_datetime(df['fl_date'] + ' ' + timepart.str.slice(0, 2) + ':' + timepart.str.slice(2, 4), errors='coerce') return datetime_part df[columns] = df[columns].apply(converter) return df def read_one(fp): df = (pd.read_csv(fp, encoding='latin1') .rename(columns=str.lower) .drop('unnamed: 6', axis=1) .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time', 'crs_dep_time']) .assign(fl_date=lambda x: pd.to_datetime(x['fl_date']))) return df /Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead. from pandas.core import datetools store = 'data/ts.hdf5' if not os.path.exists(store): download_many('2000-01-01', '2016-01-01') zips = glob.glob(os.path.join('data', 'timeseries', '*.zip')) csvs = [unzip_one(fp) for fp in zips] dfs = [read_one(fp) for fp in csvs] df = pd.concat(dfs, ignore_index=True) df['origin'] = df['origin'].astype('category') df.to_hdf(store, 'ts', format='table') else: df = pd.read_hdf(store, 'ts') with pd.option_context('display.max_rows', 100): print(df.dtypes) fl_date datetime64[ns] origin category crs_dep_time datetime64[ns] dep_time datetime64[ns] crs_arr_time datetime64[ns] arr_time datetime64[ns] dtype: object We can calculate the historical values with a resample.\ndaily = df.fl_date.value_counts().sort_index() y = daily.resample('MS').mean() y.head() 2000-01-01 15176.677419 2000-02-01 15327.551724 2000-03-01 15578.838710 2000-04-01 15442.100000 2000-05-01 15448.677419 Freq: MS, Name: fl_date, dtype: float64 Note that I use the \"MS\" frequency code there. Pandas defaults to end of month (or end of year). Append an 'S' to get the start.\nax = y.plot() ax.set(ylabel='Average Monthly Flights') sns.despine() import statsmodels.formula.api as smf import statsmodels.tsa.api as smt import statsmodels.api as sm One note of warning: I’m using the development version of statsmodels (commit de15ec8 to be precise). Not all of the items I’ve shown here are available in the currently-released version.\nThink back to a typical regression problem, ignoring anything to do with time series for now. The usual task is to predict some value $y$ using some a linear combination of features in $X$.\n$$y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p + \\epsilon$$\nWhen working with time series, some of the most important (and sometimes only) features are the previous, or lagged, values of $y$.\nLet’s start by trying just that “manually”: running a regression of y on lagged values of itself. We’ll see that this regression suffers from a few problems: multicollinearity, autocorrelation, non-stationarity, and seasonality. I’ll explain what each of those are in turn and why they’re problems. Afterwards, we’ll use a second model, seasonal ARIMA, which handles those problems for us.\nFirst, let’s create a dataframe with our lagged values of y using the .shift method, which shifts the index i periods, so it lines up with that observation.\nX = (pd.concat([y.shift(i) for i in range(6)], axis=1, keys=['y'] + ['L%s' % i for i in range(1, 6)]) .dropna()) X.head() y L1 L2 L3 L4 L5 2000-06-01 15703.333333 15448.677419 15442.100000 15578.838710 15327.551724 15176.677419 2000-07-01 15591.677419 15703.333333 15448.677419 15442.100000 15578.838710 15327.551724 2000-08-01 15850.516129 15591.677419 15703.333333 15448.677419 15442.100000 15578.838710 2000-09-01 15436.566667 15850.516129 15591.677419 15703.333333 15448.677419 15442.100000 2000-10-01 15669.709677 15436.566667 15850.516129 15591.677419 15703.333333 15448.677419 We can fit the lagged model using statsmodels (which uses patsy to translate the formula string to a design matrix).\nmod_lagged = smf.ols('y ~ trend + L1 + L2 + L3 + L4 + L5', data=X.assign(trend=np.arange(len(X)))) res_lagged = mod_lagged.fit() res_lagged.summary() OLS Regression Results Dep. Variable: y R-squared: 0.896 Model: OLS Adj. R-squared: 0.893 Method: Least Squares F-statistic: 261.1 Date: Sun, 03 Sep 2017 Prob (F-statistic): 2.61e-86 Time: 11:21:46 Log-Likelihood: -1461.2 No. Observations: 188 AIC: 2936. Df Residuals: 181 BIC: 2959. Df Model: 6 Covariance Type: nonrobust coef std err t P\u003e|t| [0.025 0.975] Intercept 1055.4443 459.096 2.299 0.023 149.575 1961.314 trend -1.0395 0.795 -1.307 0.193 -2.609 0.530 L1 1.0143 0.075 13.543 0.000 0.867 1.162 L2 -0.0769 0.106 -0.725 0.470 -0.286 0.133 L3 -0.0666 0.106 -0.627 0.531 -0.276 0.143 L4 0.1311 0.106 1.235 0.219 -0.078 0.341 L5 -0.0567 0.075 -0.758 0.449 -0.204 0.091 Omnibus: 74.709 Durbin-Watson: 1.979 Prob(Omnibus): 0.000 Jarque-Bera (JB): 851.300 Skew: 1.114 Prob(JB): 1.39e-185 Kurtosis: 13.184 Cond. No. 4.24e+05 There are a few problems with this approach though. Since our lagged values are highly correlated with each other, our regression suffers from multicollinearity. That ruins our estimates of the slopes.\nsns.heatmap(X.corr()); Second, we’d intuitively expect the $\\beta_i$s to gradually decline to zero. The immediately preceding period should be most important ($\\beta_1$ is the largest coefficient in absolute value), followed by $\\beta_2$, and $\\beta_3$… Looking at the regression summary and the bar graph below, this isn’t the case (the cause is related to multicollinearity).\nax = res_lagged.params.drop(['Intercept', 'trend']).plot.bar(rot=0) plt.ylabel('Coefficeint') sns.despine() Finally, our degrees of freedom drop since we lose two for each variable (one for estimating the coefficient, one for the lost observation as a result of the shift). At least in (macro)econometrics, each observation is precious and we’re loath to throw them away, though sometimes that’s unavoidable.\nAutocorrelation Another problem our lagged model suffered from is autocorrelation (also know as serial correlation). Roughly speaking, autocorrelation is when there’s a clear pattern in the residuals of your regression (the observed minus the predicted). Let’s fit a simple model of $y = \\beta_0 + \\beta_1 T + \\epsilon$, where T is the time trend (np.arange(len(y))).\n# `Results.resid` is a Series of residuals: y - ŷ mod_trend = sm.OLS.from_formula( 'y ~ trend', data=y.to_frame(name='y') .assign(trend=np.arange(len(y)))) res_trend = mod_trend.fit() Residuals (the observed minus the expected, or $\\hat{e_t} = y_t - \\hat{y_t}$) are supposed to be white noise. That’s one of the assumptions many of the properties of linear regression are founded upon. In this case there’s a correlation between one residual and the next: if the residual at time $t$ was above expectation, then the residual at time $t + 1$ is much more likely to be above average as well ($e_t \u003e 0 \\implies E_t[e_{t+1}] \u003e 0$).\nWe’ll define a helper function to plot the residuals time series, and some diagnostics about them.\ndef tsplot(y, lags=None, figsize=(10, 8)): fig = plt.figure(figsize=figsize) layout = (2, 2) ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2) acf_ax = plt.subplot2grid(layout, (1, 0)) pacf_ax = plt.subplot2grid(layout, (1, 1)) y.plot(ax=ts_ax) smt.graphics.plot_acf(y, lags=lags, ax=acf_ax) smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax) [ax.set_xlim(1.5) for ax in [acf_ax, pacf_ax]] sns.despine() plt.tight_layout() return ts_ax, acf_ax, pacf_ax Calling it on the residuals from the linear trend:\ntsplot(res_trend.resid, lags=36); The top subplot shows the time series of our residuals $e_t$, which should be white noise (but it isn’t). The bottom shows the autocorrelation of the residuals as a correlogram. It measures the correlation between a value and it’s lagged self, e.g. $corr(e_t, e_{t-1}), corr(e_t, e_{t-2}), \\ldots$. The partial autocorrelation plot in the bottom-right shows a similar concept. It’s partial in the sense that the value for $corr(e_t, e_{t-k})$ is the correlation between those two periods, after controlling for the values at all shorter lags.\nAutocorrelation is a problem in regular regressions like above, but we’ll use it to our advantage when we setup an ARIMA model below. The basic idea is pretty sensible: if your regression residuals have a clear pattern, then there’s clearly some structure in the data that you aren’t taking advantage of. If a positive residual today means you’ll likely have a positive residual tomorrow, why not incorporate that information into your forecast, and lower your forecasted value for tomorrow? That’s pretty much what ARIMA does.\nIt’s important that your dataset be stationary, otherwise you run the risk of finding spurious correlations. A common example is the relationship between number of TVs per person and life expectancy. It’s not likely that there’s an actual causal relationship there. Rather, there could be a third variable that’s driving both (wealth, say). Granger and Newbold (1974) had some stern words for the econometrics literature on this.\nWe find it very curious that whereas virtually every textbook on econometric methodology contains explicit warnings of the dangers of autocorrelated errors, this phenomenon crops up so frequently in well-respected applied work.\n(:fire:), but in that academic passive-aggressive way.\nThe typical way to handle non-stationarity is to difference the non-stationary variable until is is stationary.\ny.to_frame(name='y').assign(Δy=lambda x: x.y.diff()).plot(subplots=True) sns.despine() Our original series actually doesn’t look that bad. It doesn’t look like nominal GDP say, where there’s a clearly rising trend. But we have more rigorous methods for detecting whether a series is non-stationary than simply plotting and squinting at it. One popular method is the Augmented Dickey-Fuller test. It’s a statistical hypothesis test that roughly says:\n$H_0$ (null hypothesis): $y$ is non-stationary, needs to be differenced\n$H_A$ (alternative hypothesis): $y$ is stationary, doesn’t need to be differenced\nI don’t want to get into the weeds on exactly what the test statistic is, and what the distribution looks like. This is implemented in statsmodels as smt.adfuller. The return type is a bit busy for me, so we’ll wrap it in a namedtuple.\nfrom collections import namedtuple ADF = namedtuple(\"ADF\", \"adf pvalue usedlag nobs critical icbest\") ADF(*smt.adfuller(y))._asdict() OrderedDict([('adf', -1.3206520699512339), ('pvalue', 0.61967180643147923), ('usedlag', 15), ('nobs', 177), ('critical', {'1%': -3.4678453197999071, '10%': -2.575551186759871, '5%': -2.8780117454974392}), ('icbest', 2710.6120408261486)]) So we failed to reject the null hypothesis that the original series was non-stationary. Let’s difference it.\nADF(*smt.adfuller(y.diff().dropna()))._asdict() OrderedDict([('adf', -3.6412428797327996), ('pvalue', 0.0050197770854934548), ('usedlag', 14), ('nobs', 177), ('critical', {'1%': -3.4678453197999071, '10%': -2.575551186759871, '5%': -2.8780117454974392}), ('icbest', 2696.3891181091631)]) This looks better. It’s not statistically significant at the 5% level, but who cares what statisticins say anyway.\nWe’ll fit another OLS model of $\\Delta y = \\beta_0 + \\beta_1 L \\Delta y_{t-1} + e_t$\ndata = (y.to_frame(name='y') .assign(Δy=lambda df: df.y.diff()) .assign(LΔy=lambda df: df.Δy.shift())) mod_stationary = smf.ols('Δy ~ LΔy', data=data.dropna()) res_stationary = mod_stationary.fit() tsplot(res_stationary.resid, lags=24); So we’ve taken care of multicolinearity, autocorelation, and stationarity, but we still aren’t done.\nSeasonality We have strong monthly seasonality:\nsmt.seasonal_decompose(y).plot(); There are a few ways to handle seasonality. We’ll just rely on the SARIMAX method to do it for us. For now, recognize that it’s a problem to be solved.\nARIMA So, we’ve sketched the problems with regular old regression: multicollinearity, autocorrelation, non-stationarity, and seasonality. Our tool of choice, smt.SARIMAX, which stands for Seasonal ARIMA with eXogenous regressors, can handle all these. We’ll walk through the components in pieces.\nARIMA stands for AutoRegressive Integrated Moving Average. It’s a relatively simple yet flexible way of modeling univariate time series. It’s made up of three components, and is typically written as $\\mathrm{ARIMA}(p, d, q)$.\nARIMA stands for AutoRegressive Integrated Moving Average, and it’s a relatively simple way of modeling univariate time series. It’s made up of three components, and is typically written as $\\mathrm{ARIMA}(p, d, q)$.\nAutoRegressive The idea is to predict a variable by a linear combination of its lagged values (auto-regressive as in regressing a value on its past self). An AR(p), where $p$ represents the number of lagged values used, is written as\n$$y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + e_t$$\n$c$ is a constant and $e_t$ is white noise. This looks a lot like a linear regression model with multiple predictors, but the predictors happen to be lagged values of $y$ (though they are estimated differently).\nIntegrated Integrated is like the opposite of differencing, and is the part that deals with stationarity. If you have to difference your dataset 1 time to get it stationary, then $d=1$. We’ll introduce one bit of notation for differencing: $\\Delta y_t = y_t - y_{t-1}$ for $d=1$.\nMoving Average MA models look somewhat similar to the AR component, but it’s dealing with different values.\n$$y_t = c + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + \\ldots + \\theta_q e_{t-q}$$\n$c$ again is a constant and $e_t$ again is white noise. But now the coefficients are the residuals from previous predictions.\nCombining Putting that together, an ARIMA(1, 1, 1) process is written as\n$$\\Delta y_t = c + \\phi_1 \\Delta y_{t-1} + \\theta_t e_{t-1} + e_t$$\nUsing lag notation, where $L y_t = y_{t-1}$, i.e. y.shift() in pandas, we can rewrite that as\n$$(1 - \\phi_1 L) (1 - L)y_t = c + (1 + \\theta L)e_t$$\nThat was for our specific $\\mathrm{ARIMA}(1, 1, 1)$ model. For the general $\\mathrm{ARIMA}(p, d, q)$, that becomes\n$$(1 - \\phi_1 L - \\ldots - \\phi_p L^p) (1 - L)^d y_t = c + (1 + \\theta L + \\ldots + \\theta_q L^q)e_t$$\nWe went through that extremely quickly, so don’t feel bad if things aren’t clear. Fortunately, the model is pretty easy to use with statsmodels (using it correctly, in a statistical sense, is another matter).\nmod = smt.SARIMAX(y, trend='c', order=(1, 1, 1)) res = mod.fit() tsplot(res.resid[2:], lags=24); res.summary() Statespace Model Results Dep. Variable: fl_date No. Observations: 193 Model: SARIMAX(1, 1, 1) Log Likelihood -1494.618 Date: Sun, 03 Sep 2017 AIC 2997.236 Time: 11:21:50 BIC 3010.287 Sample: 01-01-2000 HQIC 3002.521 - 01-01-2016 Covariance Type: opg coef std err z P\u003e|z| [0.025 0.975] intercept -5.4306 66.818 -0.081 0.935 -136.391 125.529 ar.L1 -0.0327 2.689 -0.012 0.990 -5.303 5.237 ma.L1 0.0775 2.667 0.029 0.977 -5.149 5.305 sigma2 3.444e+05 1.69e+04 20.392 0.000 3.11e+05 3.77e+05 Ljung-Box (Q): 225.58 Jarque-Bera (JB): 1211.00 Prob(Q): 0.00 Prob(JB): 0.00 Heteroskedasticity (H): 0.67 Skew: 1.20 Prob(H) (two-sided): 0.12 Kurtosis: 15.07 There’s a bunch of output there with various tests, estimated parameters, and information criteria. Let’s just say that things are looking better, but we still haven’t accounted for seasonality.\nA seasonal ARIMA model is written as $\\mathrm{ARIMA}(p,d,q)×(P,D,Q)_s$. Lowercase letters are for the non-seasonal component, just like before. Upper-case letters are a similar specification for the seasonal component, where $s$ is the periodicity (4 for quarterly, 12 for monthly).\nIt’s like we have two processes, one for non-seasonal component and one for seasonal components, and we multiply them together with regular algebra rules.\nThe general form of that looks like (quoting the statsmodels docs here)\n$$\\phi_p(L)\\tilde{\\phi}_P(L^S)\\Delta^d\\Delta_s^D y_t = A(t) + \\theta_q(L)\\tilde{\\theta}_Q(L^s)e_t$$\nwhere\n$\\phi_p(L)$ is the non-seasonal autoregressive lag polynomial $\\tilde{\\phi}_P(L^S)$ is the seasonal autoregressive lag polynomial $\\Delta^d\\Delta_s^D$ is the time series, differenced $d$ times, and seasonally differenced $D$ times. $A(t)$ is the trend polynomial (including the intercept) $\\theta_q(L)$ is the non-seasonal moving average lag polynomial $\\tilde{\\theta}_Q(L^s)$ is the seasonal moving average lag polynomial I don’t find that to be very clear, but maybe an example will help. We’ll fit a seasonal ARIMA$(1,1,2)×(0, 1, 2)_{12}$.\nSo the nonseasonal component is\n$p=1$: period autoregressive: use $y_{t-1}$ $d=1$: one first-differencing of the data (one month) $q=2$: use the previous two non-seasonal residual, $e_{t-1}$ and $e_{t-2}$, to forecast And the seasonal component is\n$P=0$: Don’t use any previous seasonal values $D=1$: Difference the series 12 periods back: y.diff(12) $Q=2$: Use the two previous seasonal residuals mod_seasonal = smt.SARIMAX(y, trend='c', order=(1, 1, 2), seasonal_order=(0, 1, 2, 12), simple_differencing=False) res_seasonal = mod_seasonal.fit() res_seasonal.summary() Statespace Model Results Dep. Variable: fl_date No. Observations: 193 Model: SARIMAX(1, 1, 2)x(0, 1, 2, 12) Log Likelihood -1357.847 Date: Sun, 03 Sep 2017 AIC 2729.694 Time: 11:21:53 BIC 2752.533 Sample: 01-01-2000 HQIC 2738.943 - 01-01-2016 Covariance Type: opg coef std err z P\u003e|z| [0.025 0.975] intercept -17.5871 44.920 -0.392 0.695 -105.628 70.454 ar.L1 -0.9988 0.013 -74.479 0.000 -1.025 -0.973 ma.L1 0.9956 0.109 9.130 0.000 0.782 1.209 ma.L2 0.0042 0.110 0.038 0.969 -0.211 0.219 ma.S.L12 -0.7836 0.059 -13.286 0.000 -0.899 -0.668 ma.S.L24 0.2118 0.041 5.154 0.000 0.131 0.292 sigma2 1.842e+05 1.21e+04 15.240 0.000 1.61e+05 2.08e+05 Ljung-Box (Q): 32.57 Jarque-Bera (JB): 1298.39 Prob(Q): 0.79 Prob(JB): 0.00 Heteroskedasticity (H): 0.17 Skew: -1.33 Prob(H) (two-sided): 0.00 Kurtosis: 15.89 tsplot(res_seasonal.resid[12:], lags=24); Things look much better now.\nOne thing I didn’t really talk about is order selection. How to choose $p, d, q, P, D$ and $Q$. R’s forecast package does have a handy auto.arima function that does this for you. Python / statsmodels don’t have that at the minute. The alternative seems to be experience (boo), intuition (boo), and good-old grid-search. You can fit a bunch of models for a bunch of combinations of the parameters and use the AIC or BIC to choose the best. Here is a useful reference, and this StackOverflow answer recommends a few options.\nForecasting Now that we fit that model, let’s put it to use. First, we’ll make a bunch of one-step ahead forecasts. At each point (month), we take the history up to that point and make a forecast for the next month. So the forecast for January 2014 has available all the data up through December 2013.\npred = res_seasonal.get_prediction(start='2001-03-01') pred_ci = pred.conf_int() ax = y.plot(label='observed') pred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7) ax.fill_between(pred_ci.index, pred_ci.iloc[:, 0], pred_ci.iloc[:, 1], color='k', alpha=.2) ax.set_ylabel(\"Monthly Flights\") plt.legend() sns.despine() There are a few places where the observed series slips outside the 95% confidence interval. The series seems especially unstable before 2005.\nAlternatively, we can make dynamic forecasts as of some month (January 2013 in the example below). That means the forecast from that point forward only use information available as of January 2013. The predictions are generated in a similar way: a bunch of one-step forecasts. Only instead of plugging in the actual values beyond January 2013, we plug in the forecast values.\npred_dy = res_seasonal.get_prediction(start='2002-03-01', dynamic='2013-01-01') pred_dy_ci = pred_dy.conf_int() ax = y.plot(label='observed') pred_dy.predicted_mean.plot(ax=ax, label='Forecast') ax.fill_between(pred_dy_ci.index, pred_dy_ci.iloc[:, 0], pred_dy_ci.iloc[:, 1], color='k', alpha=.25) ax.set_ylabel(\"Monthly Flights\") # Highlight the forecast area ax.fill_betweenx(ax.get_ylim(), pd.Timestamp('2013-01-01'), y.index[-1], alpha=.1, zorder=-1) ax.annotate('Dynamic $\\\\longrightarrow$', (pd.Timestamp('2013-02-01'), 550)) plt.legend() sns.despine() Resources This is a collection of links for those interested.\nTime series modeling in Python Statsmodels Statespace Notebooks Statsmodels VAR tutorial ARCH Library by Kevin Sheppard General Textbooks Forecasting: Principles and Practice: A great introduction Stock and Watson: Readable undergraduate resource, has a few chapters on time series Greene’s Econometric Analysis: My favorite PhD level textbook Hamilton’s Time Series Analysis: A classic Lutkehpohl’s New Introduction to Multiple Time Series Analysis: Extremely dry, but useful if you’re implementing this stuff Conclusion Congratulations if you made it this far, this piece just kept growing (and I still had to cut stuff). The main thing cut was talking about how SARIMAX is implemented on top of using statsmodels’ statespace framework. The statespace framework, developed mostly by Chad Fulton over the past couple years, is really nice. You can pretty easily extend it with custom models, but still get all the benefits of the framework’s estimation and results facilities. I’d recommend reading the notebooks. We also didn’t get to talk at all about Skipper Seabold’s work on VARs, but maybe some other time.\nAs always, feedback is welcome.\n","wordCount":"4120","inLanguage":"en","datePublished":"2016-05-13T00:00:00Z","dateModified":"2016-05-13T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.github.io/posts/modern-7-timeseries/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.github.io accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.github.io/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.github.io/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Modern Pandas (Part 7): Timeseries</h1><div class=post-meta><span title='2016-05-13 00:00:00 +0000 UTC'>May 13, 2016</span></div></header><div class=post-content><hr><p>This is part 7 in my series on writing modern idiomatic pandas.</p><ul><li><a href=modern-1-intro>Modern Pandas</a></li><li><a href=method-chaining>Method Chaining</a></li><li><a href=modern-3-indexes>Indexes</a></li><li><a href=modern-4-performance>Fast Pandas</a></li><li><a href=modern-5-tidy>Tidy Data</a></li><li><a href=modern-6-visualization>Visualization</a></li><li><a href=modern-7-timeseries>Time Series</a></li><li><a href=modern-8-scaling>Scaling</a></li></ul><hr><h1 id=timeseries>Timeseries<a hidden class=anchor aria-hidden=true href=#timeseries>#</a></h1><p>Pandas started out in the financial world, so naturally it has strong timeseries support.</p><p>The first half of this post will look at pandas&rsquo; capabilities for manipulating time series data.
The second half will discuss modelling time series data with statsmodels.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas_datareader.data <span style=color:#66d9ef>as</span> web
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>set(style<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ticks&#39;</span>, context<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;talk&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> int(os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;MODERN_PANDAS_EPUB&#34;</span>, <span style=color:#ae81ff>0</span>)):
</span></span><span style=display:flex><span>    <span style=color:#f92672>import</span> prep <span style=color:#75715e># noqa</span>
</span></span></code></pre></div><p>Let&rsquo;s grab some stock data for Goldman Sachs using the <a href=http://pandas-datareader.readthedocs.io/en/latest/><code>pandas-datareader</code></a> package, which spun off of pandas:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gs <span style=color:#f92672>=</span> web<span style=color:#f92672>.</span>DataReader(<span style=color:#e6db74>&#34;GS&#34;</span>, data_source<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;yahoo&#39;</span>, start<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;2006-01-01&#39;</span>,
</span></span><span style=display:flex><span>                    end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;2010-01-01&#39;</span>)
</span></span><span style=display:flex><span>gs<span style=color:#f92672>.</span>head()<span style=color:#f92672>.</span>round(<span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>Open</th><th>High</th><th>Low</th><th>Close</th><th>Adj Close</th><th>Volume</th></tr><tr><th>Date</th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>2006-01-03</th><td>126.70</td><td>129.44</td><td>124.23</td><td>128.87</td><td>112.34</td><td>6188700</td></tr><tr><th>2006-01-04</th><td>127.35</td><td>128.91</td><td>126.38</td><td>127.09</td><td>110.79</td><td>4861600</td></tr><tr><th>2006-01-05</th><td>126.00</td><td>127.32</td><td>125.61</td><td>127.04</td><td>110.74</td><td>3717400</td></tr><tr><th>2006-01-06</th><td>127.29</td><td>129.25</td><td>127.29</td><td>128.84</td><td>112.31</td><td>4319600</td></tr><tr><th>2006-01-09</th><td>128.50</td><td>130.62</td><td>128.00</td><td>130.39</td><td>113.66</td><td>4723500</td></tr></tbody></table></div><p>There isn&rsquo;t a special data-container just for time series in pandas, they&rsquo;re just <code>Series</code> or <code>DataFrame</code>s with a <code>DatetimeIndex</code>.</p><h2 id=special-slicing>Special Slicing<a hidden class=anchor aria-hidden=true href=#special-slicing>#</a></h2><p>Looking at the elements of <code>gs.index</code>, we see that <code>DatetimeIndex</code>es are made up of <code>pandas.Timestamp</code>s:</p><p>Looking at the elements of <code>gs.index</code>, we see that <code>DatetimeIndex</code>es are made up of <code>pandas.Timestamp</code>s:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gs<span style=color:#f92672>.</span>index[<span style=color:#ae81ff>0</span>]
</span></span></code></pre></div><pre><code>Timestamp('2006-01-03 00:00:00')
</code></pre><p>A <code>Timestamp</code> is mostly compatible with the <code>datetime.datetime</code> class, but much amenable to storage in arrays.</p><p>Working with <code>Timestamp</code>s can be awkward, so Series and DataFrames with <code>DatetimeIndexes</code> have some special slicing rules.
The first special case is <em>partial-string indexing</em>. Say we wanted to select all the days in 2006. Even with <code>Timestamp</code>&rsquo;s convenient constructors, it&rsquo;s a pai</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gs<span style=color:#f92672>.</span>loc[pd<span style=color:#f92672>.</span>Timestamp(<span style=color:#e6db74>&#39;2006-01-01&#39;</span>):pd<span style=color:#f92672>.</span>Timestamp(<span style=color:#e6db74>&#39;2006-12-31&#39;</span>)]<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>Open</th><th>High</th><th>Low</th><th>Close</th><th>Adj Close</th><th>Volume</th></tr><tr><th>Date</th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>2006-01-03</th><td>126.699997</td><td>129.440002</td><td>124.230003</td><td>128.869995</td><td>112.337547</td><td>6188700</td></tr><tr><th>2006-01-04</th><td>127.349998</td><td>128.910004</td><td>126.379997</td><td>127.089996</td><td>110.785889</td><td>4861600</td></tr><tr><th>2006-01-05</th><td>126.000000</td><td>127.320000</td><td>125.610001</td><td>127.040001</td><td>110.742340</td><td>3717400</td></tr><tr><th>2006-01-06</th><td>127.290001</td><td>129.250000</td><td>127.290001</td><td>128.839996</td><td>112.311401</td><td>4319600</td></tr><tr><th>2006-01-09</th><td>128.500000</td><td>130.619995</td><td>128.000000</td><td>130.389999</td><td>113.662605</td><td>4723500</td></tr></tbody></table></div><p>Thanks to partial-string indexing, it&rsquo;s as simple as</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gs<span style=color:#f92672>.</span>loc[<span style=color:#e6db74>&#39;2006&#39;</span>]<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>Open</th><th>High</th><th>Low</th><th>Close</th><th>Adj Close</th><th>Volume</th></tr><tr><th>Date</th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>2006-01-03</th><td>126.699997</td><td>129.440002</td><td>124.230003</td><td>128.869995</td><td>112.337547</td><td>6188700</td></tr><tr><th>2006-01-04</th><td>127.349998</td><td>128.910004</td><td>126.379997</td><td>127.089996</td><td>110.785889</td><td>4861600</td></tr><tr><th>2006-01-05</th><td>126.000000</td><td>127.320000</td><td>125.610001</td><td>127.040001</td><td>110.742340</td><td>3717400</td></tr><tr><th>2006-01-06</th><td>127.290001</td><td>129.250000</td><td>127.290001</td><td>128.839996</td><td>112.311401</td><td>4319600</td></tr><tr><th>2006-01-09</th><td>128.500000</td><td>130.619995</td><td>128.000000</td><td>130.389999</td><td>113.662605</td><td>4723500</td></tr></tbody></table></div><p>Since label slicing is inclusive, this slice selects any observation where the year is 2006.</p><p>The second &ldquo;convenience&rdquo; is <code>__getitem__</code> (square-bracket) fall-back indexing. I&rsquo;m only going to mention it here, with the caveat that you should never use it.
DataFrame <code>__getitem__</code> typically looks in the column: <code>gs['2006']</code> would search <code>gs.columns</code> for <code>'2006'</code>, not find it, and raise a <code>KeyError</code>. But DataFrames with a <code>DatetimeIndex</code> catch that <code>KeyError</code> and try to slice the index.
If it succeeds in slicing the index, the result like <code>gs.loc['2006']</code> is returned.
If it fails, the <code>KeyError</code> is re-raised.
This is confusing because in pretty much every other case <code>DataFrame.__getitem__</code> works on columns, and it&rsquo;s fragile because if you happened to have a column <code>'2006'</code> you <em>would</em> get just that column, and no fall-back indexing would occur. Just use <code>gs.loc['2006']</code> when slicing DataFrame indexes.</p><h2 id=special-methods>Special Methods<a hidden class=anchor aria-hidden=true href=#special-methods>#</a></h2><h3 id=resampling>Resampling<a hidden class=anchor aria-hidden=true href=#resampling>#</a></h3><p>Resampling is similar to a <code>groupby</code>: you split the time series into groups (5-day buckets below), apply a function to each group (<code>mean</code>), and combine the result (one row per group).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gs<span style=color:#f92672>.</span>resample(<span style=color:#e6db74>&#34;5d&#34;</span>)<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>Open</th><th>High</th><th>Low</th><th>Close</th><th>Adj Close</th><th>Volume</th></tr><tr><th>Date</th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>2006-01-03</th><td>126.834999</td><td>128.730002</td><td>125.877501</td><td>127.959997</td><td>111.544294</td><td>4.771825e+06</td></tr><tr><th>2006-01-08</th><td>130.349998</td><td>132.645000</td><td>130.205002</td><td>131.660000</td><td>114.769649</td><td>4.664300e+06</td></tr><tr><th>2006-01-13</th><td>131.510002</td><td>133.395005</td><td>131.244995</td><td>132.924995</td><td>115.872357</td><td>3.258250e+06</td></tr><tr><th>2006-01-18</th><td>132.210002</td><td>133.853333</td><td>131.656667</td><td>132.543335</td><td>115.611125</td><td>4.997767e+06</td></tr><tr><th>2006-01-23</th><td>133.771997</td><td>136.083997</td><td>133.310001</td><td>135.153998</td><td>118.035918</td><td>3.968500e+06</td></tr></tbody></table></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gs<span style=color:#f92672>.</span>resample(<span style=color:#e6db74>&#34;W&#34;</span>)<span style=color:#f92672>.</span>agg([<span style=color:#e6db74>&#39;mean&#39;</span>, <span style=color:#e6db74>&#39;sum&#39;</span>])<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr><th></th><th colspan=2 halign=left>Open</th><th colspan=2 halign=left>High</th><th colspan=2 halign=left>Low</th><th colspan=2 halign=left>Close</th><th colspan=2 halign=left>Adj Close</th><th colspan=2 halign=left>Volume</th></tr><tr><th></th><th>mean</th><th>sum</th><th>mean</th><th>sum</th><th>mean</th><th>sum</th><th>mean</th><th>sum</th><th>mean</th><th>sum</th><th>mean</th><th>sum</th></tr><tr><th>Date</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>2006-01-08</th><td>126.834999</td><td>507.339996</td><td>128.730002</td><td>514.920006</td><td>125.877501</td><td>503.510002</td><td>127.959997</td><td>511.839988</td><td>111.544294</td><td>446.177177</td><td>4771825.0</td><td>19087300</td></tr><tr><th>2006-01-15</th><td>130.684000</td><td>653.419998</td><td>132.848001</td><td>664.240006</td><td>130.544000</td><td>652.720001</td><td>131.979999</td><td>659.899994</td><td>115.048592</td><td>575.242958</td><td>4310420.0</td><td>21552100</td></tr><tr><th>2006-01-22</th><td>131.907501</td><td>527.630005</td><td>133.672501</td><td>534.690003</td><td>131.389999</td><td>525.559998</td><td>132.555000</td><td>530.220000</td><td>115.603432</td><td>462.413728</td><td>4653725.0</td><td>18614900</td></tr><tr><th>2006-01-29</th><td>133.771997</td><td>668.859986</td><td>136.083997</td><td>680.419983</td><td>133.310001</td><td>666.550003</td><td>135.153998</td><td>675.769989</td><td>118.035918</td><td>590.179588</td><td>3968500.0</td><td>19842500</td></tr><tr><th>2006-02-05</th><td>140.900000</td><td>704.500000</td><td>142.467999</td><td>712.339996</td><td>139.937998</td><td>699.689988</td><td>141.618002</td><td>708.090011</td><td>123.681204</td><td>618.406020</td><td>3920120.0</td><td>19600600</td></tr></tbody></table></div><p>You can up-sample to convert to a higher frequency.
The new points are filled with NaNs.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gs<span style=color:#f92672>.</span>resample(<span style=color:#e6db74>&#34;6H&#34;</span>)<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>Open</th><th>High</th><th>Low</th><th>Close</th><th>Adj Close</th><th>Volume</th></tr><tr><th>Date</th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>2006-01-03 00:00:00</th><td>126.699997</td><td>129.440002</td><td>124.230003</td><td>128.869995</td><td>112.337547</td><td>6188700.0</td></tr><tr><th>2006-01-03 06:00:00</th><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><th>2006-01-03 12:00:00</th><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><th>2006-01-03 18:00:00</th><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td><td>NaN</td></tr><tr><th>2006-01-04 00:00:00</th><td>127.349998</td><td>128.910004</td><td>126.379997</td><td>127.089996</td><td>110.785889</td><td>4861600.0</td></tr></tbody></table></div><h3 id=rolling--expanding--ew>Rolling / Expanding / EW<a hidden class=anchor aria-hidden=true href=#rolling--expanding--ew>#</a></h3><p>These methods aren&rsquo;t unique to <code>DatetimeIndex</code>es, but they often make sense with time series, so I&rsquo;ll show them here.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gs<span style=color:#f92672>.</span>Close<span style=color:#f92672>.</span>plot(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Raw&#39;</span>)
</span></span><span style=display:flex><span>gs<span style=color:#f92672>.</span>Close<span style=color:#f92672>.</span>rolling(<span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>plot(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;28D MA&#39;</span>)
</span></span><span style=display:flex><span>gs<span style=color:#f92672>.</span>Close<span style=color:#f92672>.</span>expanding()<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>plot(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Expanding Average&#39;</span>)
</span></span><span style=display:flex><span>gs<span style=color:#f92672>.</span>Close<span style=color:#f92672>.</span>ewm(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.03</span>)<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>plot(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;EWMA($</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>alpha=.03$)&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend(bbox_to_anchor<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1.25</span>, <span style=color:#ae81ff>.5</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>tight_layout()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Close ($)&#34;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_23_0.png alt=png></p><p>Each of <code>.rolling</code>, <code>.expanding</code>, and <code>.ewm</code> return a deferred object, similar to a GroupBy.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>roll <span style=color:#f92672>=</span> gs<span style=color:#f92672>.</span>Close<span style=color:#f92672>.</span>rolling(<span style=color:#ae81ff>30</span>, center<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>roll
</span></span></code></pre></div><pre><code>Rolling [window=30,center=True,axis=0]
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>m <span style=color:#f92672>=</span> roll<span style=color:#f92672>.</span>agg([<span style=color:#e6db74>&#39;mean&#39;</span>, <span style=color:#e6db74>&#39;std&#39;</span>])
</span></span><span style=display:flex><span>ax <span style=color:#f92672>=</span> m[<span style=color:#e6db74>&#39;mean&#39;</span>]<span style=color:#f92672>.</span>plot()
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>fill_between(m<span style=color:#f92672>.</span>index, m[<span style=color:#e6db74>&#39;mean&#39;</span>] <span style=color:#f92672>-</span> m[<span style=color:#e6db74>&#39;std&#39;</span>], m[<span style=color:#e6db74>&#39;mean&#39;</span>] <span style=color:#f92672>+</span> m[<span style=color:#e6db74>&#39;std&#39;</span>],
</span></span><span style=display:flex><span>                alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>.25</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>tight_layout()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Close ($)&#34;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_26_0.png alt=png></p><h2 id=grab-bag>Grab Bag<a hidden class=anchor aria-hidden=true href=#grab-bag>#</a></h2><h3 id=offsets>Offsets<a hidden class=anchor aria-hidden=true href=#offsets>#</a></h3><p>These are similar to <code>dateutil.relativedelta</code>, but works with arrays.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gs<span style=color:#f92672>.</span>index <span style=color:#f92672>+</span> pd<span style=color:#f92672>.</span>DateOffset(months<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, days<span style=color:#f92672>=-</span><span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><pre><code>DatetimeIndex(['2006-04-01', '2006-04-02', '2006-04-03', '2006-04-04',
               '2006-04-07', '2006-04-08', '2006-04-09', '2006-04-10',
               '2006-04-11', '2006-04-15',
               ...
               '2010-03-15', '2010-03-16', '2010-03-19', '2010-03-20',
               '2010-03-21', '2010-03-22', '2010-03-26', '2010-03-27',
               '2010-03-28', '2010-03-29'],
              dtype='datetime64[ns]', name='Date', length=1007, freq=None)
</code></pre><h3 id=holiday-calendars>Holiday Calendars<a hidden class=anchor aria-hidden=true href=#holiday-calendars>#</a></h3><p>There are a whole bunch of special calendars, useful for traders probabaly.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pandas.tseries.holiday <span style=color:#f92672>import</span> USColumbusDay
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>USColumbusDay<span style=color:#f92672>.</span>dates(<span style=color:#e6db74>&#39;2015-01-01&#39;</span>, <span style=color:#e6db74>&#39;2020-01-01&#39;</span>)
</span></span></code></pre></div><pre><code>DatetimeIndex(['2015-10-12', '2016-10-10', '2017-10-09', '2018-10-08',
               '2019-10-14'],
              dtype='datetime64[ns]', freq='WOM-2MON')
</code></pre><h3 id=timezones>Timezones<a hidden class=anchor aria-hidden=true href=#timezones>#</a></h3><p>Pandas works with <code>pytz</code> for nice timezone-aware datetimes.
The typical workflow is</p><ol><li>localize timezone-naive timestamps to some timezone</li><li>convert to desired timezone</li></ol><p>If you already have timezone-aware Timestamps, there&rsquo;s no need for step one.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># tz naiive -&gt; tz aware..... to desired UTC</span>
</span></span><span style=display:flex><span>gs<span style=color:#f92672>.</span>tz_localize(<span style=color:#e6db74>&#39;US/Eastern&#39;</span>)<span style=color:#f92672>.</span>tz_convert(<span style=color:#e6db74>&#39;UTC&#39;</span>)<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>Open</th><th>High</th><th>Low</th><th>Close</th><th>Adj Close</th><th>Volume</th></tr><tr><th>Date</th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>2006-01-03 05:00:00+00:00</th><td>126.699997</td><td>129.440002</td><td>124.230003</td><td>128.869995</td><td>112.337547</td><td>6188700</td></tr><tr><th>2006-01-04 05:00:00+00:00</th><td>127.349998</td><td>128.910004</td><td>126.379997</td><td>127.089996</td><td>110.785889</td><td>4861600</td></tr><tr><th>2006-01-05 05:00:00+00:00</th><td>126.000000</td><td>127.320000</td><td>125.610001</td><td>127.040001</td><td>110.742340</td><td>3717400</td></tr><tr><th>2006-01-06 05:00:00+00:00</th><td>127.290001</td><td>129.250000</td><td>127.290001</td><td>128.839996</td><td>112.311401</td><td>4319600</td></tr><tr><th>2006-01-09 05:00:00+00:00</th><td>128.500000</td><td>130.619995</td><td>128.000000</td><td>130.389999</td><td>113.662605</td><td>4723500</td></tr></tbody></table></div><h2 id=modeling-time-series>Modeling Time Series<a hidden class=anchor aria-hidden=true href=#modeling-time-series>#</a></h2><p>The rest of this post will focus on time series in the econometric sense.
My indented reader for this section isn&rsquo;t all that clear, so I apologize upfront for any sudden shifts in complexity.
I&rsquo;m roughly targeting material that could be presented in a first or second semester applied statisctics course.
What follows certainly isn&rsquo;t a replacement for that.
Any formality will be restricted to footnotes for the curious.
I&rsquo;ve put a whole bunch of resources at the end for people earger to learn more.</p><p>We&rsquo;ll focus on modelling Average Monthly Flights. Let&rsquo;s download the data.
If you&rsquo;ve been following along in the series, you&rsquo;ve seen most of this code before, so feel free to skip.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> io
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> glob
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> zipfile
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> utils <span style=color:#f92672>import</span> download_timeseries
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> statsmodels.api <span style=color:#66d9ef>as</span> sm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>download_many</span>(start, end):
</span></span><span style=display:flex><span>    months <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>period_range(start, end<span style=color:#f92672>=</span>end, freq<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;M&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># We could easily parallelize this loop.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, month <span style=color:#f92672>in</span> enumerate(months):
</span></span><span style=display:flex><span>        download_timeseries(month)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>time_to_datetime</span>(df, columns):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Combine all time items into datetimes.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    2014-01-01,1149.0 -&gt; 2014-01-01T11:49:00
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>converter</span>(col):
</span></span><span style=display:flex><span>        timepart <span style=color:#f92672>=</span> (col<span style=color:#f92672>.</span>astype(str)
</span></span><span style=display:flex><span>                       <span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;\.0$&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>)  <span style=color:#75715e># NaNs force float dtype</span>
</span></span><span style=display:flex><span>                       <span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>pad(<span style=color:#ae81ff>4</span>, fillchar<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;0&#39;</span>))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span>  pd<span style=color:#f92672>.</span>to_datetime(df[<span style=color:#e6db74>&#39;fl_date&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39; &#39;</span> <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                               timepart<span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>slice(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>) <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;:&#39;</span> <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>                               timepart<span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>slice(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>),
</span></span><span style=display:flex><span>                               errors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;coerce&#39;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> datetime_part
</span></span><span style=display:flex><span>    df[columns] <span style=color:#f92672>=</span> df[columns]<span style=color:#f92672>.</span>apply(converter)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> df
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>read_one</span>(fp):
</span></span><span style=display:flex><span>    df <span style=color:#f92672>=</span> (pd<span style=color:#f92672>.</span>read_csv(fp, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;latin1&#39;</span>)
</span></span><span style=display:flex><span>            <span style=color:#f92672>.</span>rename(columns<span style=color:#f92672>=</span>str<span style=color:#f92672>.</span>lower)
</span></span><span style=display:flex><span>            <span style=color:#f92672>.</span>drop(<span style=color:#e6db74>&#39;unnamed: 6&#39;</span>, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            <span style=color:#f92672>.</span>pipe(time_to_datetime, [<span style=color:#e6db74>&#39;dep_time&#39;</span>, <span style=color:#e6db74>&#39;arr_time&#39;</span>, <span style=color:#e6db74>&#39;crs_arr_time&#39;</span>,
</span></span><span style=display:flex><span>                                     <span style=color:#e6db74>&#39;crs_dep_time&#39;</span>])
</span></span><span style=display:flex><span>            <span style=color:#f92672>.</span>assign(fl_date<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> x: pd<span style=color:#f92672>.</span>to_datetime(x[<span style=color:#e6db74>&#39;fl_date&#39;</span>])))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> df
</span></span></code></pre></div><pre><code>/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.
  from pandas.core import datetools
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>store <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;data/ts.hdf5&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(store):
</span></span><span style=display:flex><span>    download_many(<span style=color:#e6db74>&#39;2000-01-01&#39;</span>, <span style=color:#e6db74>&#39;2016-01-01&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    zips <span style=color:#f92672>=</span> glob<span style=color:#f92672>.</span>glob(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(<span style=color:#e6db74>&#39;data&#39;</span>, <span style=color:#e6db74>&#39;timeseries&#39;</span>, <span style=color:#e6db74>&#39;*.zip&#39;</span>))
</span></span><span style=display:flex><span>    csvs <span style=color:#f92672>=</span> [unzip_one(fp) <span style=color:#66d9ef>for</span> fp <span style=color:#f92672>in</span> zips]
</span></span><span style=display:flex><span>    dfs <span style=color:#f92672>=</span> [read_one(fp) <span style=color:#66d9ef>for</span> fp <span style=color:#f92672>in</span> csvs]
</span></span><span style=display:flex><span>    df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>concat(dfs, ignore_index<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    df[<span style=color:#e6db74>&#39;origin&#39;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;origin&#39;</span>]<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#39;category&#39;</span>)
</span></span><span style=display:flex><span>    df<span style=color:#f92672>.</span>to_hdf(store, <span style=color:#e6db74>&#39;ts&#39;</span>, format<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;table&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>    df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_hdf(store, <span style=color:#e6db74>&#39;ts&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> pd<span style=color:#f92672>.</span>option_context(<span style=color:#e6db74>&#39;display.max_rows&#39;</span>, <span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    print(df<span style=color:#f92672>.</span>dtypes)
</span></span></code></pre></div><pre><code>fl_date         datetime64[ns]
origin                category
crs_dep_time    datetime64[ns]
dep_time        datetime64[ns]
crs_arr_time    datetime64[ns]
arr_time        datetime64[ns]
dtype: object
</code></pre><p>We can calculate the historical values with a resample.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>daily <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>fl_date<span style=color:#f92672>.</span>value_counts()<span style=color:#f92672>.</span>sort_index()
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> daily<span style=color:#f92672>.</span>resample(<span style=color:#e6db74>&#39;MS&#39;</span>)<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>y<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><pre><code>2000-01-01    15176.677419
2000-02-01    15327.551724
2000-03-01    15578.838710
2000-04-01    15442.100000
2000-05-01    15448.677419
Freq: MS, Name: fl_date, dtype: float64
</code></pre><p>Note that I use the <code>"MS"</code> frequency code there.
Pandas defaults to end of month (or end of year).
Append an <code>'S'</code> to get the start.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ax <span style=color:#f92672>=</span> y<span style=color:#f92672>.</span>plot()
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>set(ylabel<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Average Monthly Flights&#39;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_41_0.png alt=png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> statsmodels.formula.api <span style=color:#66d9ef>as</span> smf
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> statsmodels.tsa.api <span style=color:#66d9ef>as</span> smt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> statsmodels.api <span style=color:#66d9ef>as</span> sm
</span></span></code></pre></div><p>One note of warning: I&rsquo;m using the development version of statsmodels (commit <code>de15ec8</code> to be precise).
Not all of the items I&rsquo;ve shown here are available in the currently-released version.</p><p>Think back to a typical regression problem, ignoring anything to do with time series for now.
The usual task is to predict some value $y$ using some a linear combination of features in $X$.</p><p>$$y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p + \epsilon$$</p><p>When working with time series, some of the most important (and sometimes <em>only</em>) features are the previous, or <em>lagged</em>, values of $y$.</p><p>Let&rsquo;s start by trying just that &ldquo;manually&rdquo;: running a regression of <code>y</code> on lagged values of itself.
We&rsquo;ll see that this regression suffers from a few problems: multicollinearity, autocorrelation, non-stationarity, and seasonality.
I&rsquo;ll explain what each of those are in turn and why they&rsquo;re problems.
Afterwards, we&rsquo;ll use a second model, seasonal ARIMA, which handles those problems for us.</p><p>First, let&rsquo;s create a dataframe with our lagged values of <code>y</code> using the <code>.shift</code> method, which shifts the index <code>i</code> periods, so it lines up with that observation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>X <span style=color:#f92672>=</span> (pd<span style=color:#f92672>.</span>concat([y<span style=color:#f92672>.</span>shift(i) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>6</span>)], axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>               keys<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;y&#39;</span>] <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;L</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> i <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>)])
</span></span><span style=display:flex><span>       <span style=color:#f92672>.</span>dropna())
</span></span><span style=display:flex><span>X<span style=color:#f92672>.</span>head()
</span></span></code></pre></div><div><style>.dataframe thead tr:only-child th{text-align:right}<pre><code>.dataframe thead th{text-align:left}.dataframe tbody tr th{vertical-align:top}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>y</th><th>L1</th><th>L2</th><th>L3</th><th>L4</th><th>L5</th></tr></thead><tbody><tr><th>2000-06-01</th><td>15703.333333</td><td>15448.677419</td><td>15442.100000</td><td>15578.838710</td><td>15327.551724</td><td>15176.677419</td></tr><tr><th>2000-07-01</th><td>15591.677419</td><td>15703.333333</td><td>15448.677419</td><td>15442.100000</td><td>15578.838710</td><td>15327.551724</td></tr><tr><th>2000-08-01</th><td>15850.516129</td><td>15591.677419</td><td>15703.333333</td><td>15448.677419</td><td>15442.100000</td><td>15578.838710</td></tr><tr><th>2000-09-01</th><td>15436.566667</td><td>15850.516129</td><td>15591.677419</td><td>15703.333333</td><td>15448.677419</td><td>15442.100000</td></tr><tr><th>2000-10-01</th><td>15669.709677</td><td>15436.566667</td><td>15850.516129</td><td>15591.677419</td><td>15703.333333</td><td>15448.677419</td></tr></tbody></table></div><p>We can fit the lagged model using statsmodels (which uses <a href=http://patsy.readthedocs.org>patsy</a> to translate the formula string to a design matrix).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>mod_lagged <span style=color:#f92672>=</span> smf<span style=color:#f92672>.</span>ols(<span style=color:#e6db74>&#39;y ~ trend + L1 + L2 + L3 + L4 + L5&#39;</span>,
</span></span><span style=display:flex><span>                     data<span style=color:#f92672>=</span>X<span style=color:#f92672>.</span>assign(trend<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>arange(len(X))))
</span></span><span style=display:flex><span>res_lagged <span style=color:#f92672>=</span> mod_lagged<span style=color:#f92672>.</span>fit()
</span></span><span style=display:flex><span>res_lagged<span style=color:#f92672>.</span>summary()
</span></span></code></pre></div><table class=simpletable><caption>OLS Regression Results</caption><tr><th>Dep. Variable:</th><td>y</td><th>R-squared:</th><td>0.896</td></tr><tr><th>Model:</th><td>OLS</td><th>Adj. R-squared:</th><td>0.893</td></tr><tr><th>Method:</th><td>Least Squares</td><th>F-statistic:</th><td>261.1</td></tr><tr><th>Date:</th><td>Sun, 03 Sep 2017</td><th>Prob (F-statistic):</th><td>2.61e-86</td></tr><tr><th>Time:</th><td>11:21:46</td><th>Log-Likelihood:</th><td>-1461.2</td></tr><tr><th>No. Observations:</th><td>188</td><th>AIC:</th><td>2936.</td></tr><tr><th>Df Residuals:</th><td>181</td><th>BIC:</th><td>2959.</td></tr><tr><th>Df Model:</th><td>6</td><th></th><td></td></tr><tr><th>Covariance Type:</th><td>nonrobust</td><th></th><td></td></tr></table><table class=simpletable><tr><td></td><th>coef</th><th>std err</th><th>t</th><th>P>|t|</th><th>[0.025</th><th>0.975]</th></tr><tr><th>Intercept</th><td>1055.4443</td><td>459.096</td><td>2.299</td><td>0.023</td><td>149.575</td><td>1961.314</td></tr><tr><th>trend</th><td>-1.0395</td><td>0.795</td><td>-1.307</td><td>0.193</td><td>-2.609</td><td>0.530</td></tr><tr><th>L1</th><td>1.0143</td><td>0.075</td><td>13.543</td><td>0.000</td><td>0.867</td><td>1.162</td></tr><tr><th>L2</th><td>-0.0769</td><td>0.106</td><td>-0.725</td><td>0.470</td><td>-0.286</td><td>0.133</td></tr><tr><th>L3</th><td>-0.0666</td><td>0.106</td><td>-0.627</td><td>0.531</td><td>-0.276</td><td>0.143</td></tr><tr><th>L4</th><td>0.1311</td><td>0.106</td><td>1.235</td><td>0.219</td><td>-0.078</td><td>0.341</td></tr><tr><th>L5</th><td>-0.0567</td><td>0.075</td><td>-0.758</td><td>0.449</td><td>-0.204</td><td>0.091</td></tr></table><table class=simpletable><tr><th>Omnibus:</th><td>74.709</td><th>Durbin-Watson:</th><td>1.979</td></tr><tr><th>Prob(Omnibus):</th><td>0.000</td><th>Jarque-Bera (JB):</th><td>851.300</td></tr><tr><th>Skew:</th><td>1.114</td><th>Prob(JB):</th><td>1.39e-185</td></tr><tr><th>Kurtosis:</th><td>13.184</td><th>Cond. No.</th><td>4.24e+05</td></tr></table><p>There are a few problems with this approach though.
Since our lagged values are highly correlated with each other, our regression suffers from <a href=https://en.wikipedia.org/wiki/Multicollinearity>multicollinearity</a>.
That ruins our estimates of the slopes.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sns<span style=color:#f92672>.</span>heatmap(X<span style=color:#f92672>.</span>corr());
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_48_0.png alt=png></p><p>Second, we&rsquo;d intuitively expect the $\beta_i$s to gradually decline to zero.
The immediately preceding period <em>should</em> be most important ($\beta_1$ is the largest coefficient in absolute value), followed by $\beta_2$, and $\beta_3$&mldr;
Looking at the regression summary and the bar graph below, this isn&rsquo;t the case (the cause is related to multicollinearity).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ax <span style=color:#f92672>=</span> res_lagged<span style=color:#f92672>.</span>params<span style=color:#f92672>.</span>drop([<span style=color:#e6db74>&#39;Intercept&#39;</span>, <span style=color:#e6db74>&#39;trend&#39;</span>])<span style=color:#f92672>.</span>plot<span style=color:#f92672>.</span>bar(rot<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Coefficeint&#39;</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_50_0.png alt=png></p><p>Finally, our degrees of freedom drop since we lose two for each variable (one for estimating the coefficient, one for the lost observation as a result of the <code>shift</code>).
At least in (macro)econometrics, each observation is precious and we&rsquo;re loath to throw them away, though sometimes that&rsquo;s unavoidable.</p><h3 id=autocorrelation>Autocorrelation<a hidden class=anchor aria-hidden=true href=#autocorrelation>#</a></h3><p>Another problem our lagged model suffered from is <a href=https://en.wikipedia.org/wiki/Autocorrelation>autocorrelation</a> (also know as serial correlation).
Roughly speaking, autocorrelation is when there&rsquo;s a clear pattern in the residuals of your regression (the observed minus the predicted).
Let&rsquo;s fit a simple model of $y = \beta_0 + \beta_1 T + \epsilon$, where <code>T</code> is the time trend (<code>np.arange(len(y))</code>).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># `Results.resid` is a Series of residuals: y - ŷ</span>
</span></span><span style=display:flex><span>mod_trend <span style=color:#f92672>=</span> sm<span style=color:#f92672>.</span>OLS<span style=color:#f92672>.</span>from_formula(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;y ~ trend&#39;</span>, data<span style=color:#f92672>=</span>y<span style=color:#f92672>.</span>to_frame(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;y&#39;</span>)
</span></span><span style=display:flex><span>                       <span style=color:#f92672>.</span>assign(trend<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>arange(len(y))))
</span></span><span style=display:flex><span>res_trend <span style=color:#f92672>=</span> mod_trend<span style=color:#f92672>.</span>fit()
</span></span></code></pre></div><p>Residuals (the observed minus the expected, or $\hat{e_t} = y_t - \hat{y_t}$) are supposed to be <a href=https://en.wikipedia.org/wiki/White_noise>white noise</a>.
That&rsquo;s <a href=https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem>one of the assumptions</a> many of the properties of linear regression are founded upon.
In this case there&rsquo;s a correlation between one residual and the next: if the residual at time $t$ was above expectation, then the residual at time $t + 1$ is <em>much</em> more likely to be above average as well ($e_t > 0 \implies E_t[e_{t+1}] > 0$).</p><p>We&rsquo;ll define a helper function to plot the residuals time series, and some diagnostics about them.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tsplot</span>(y, lags<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>8</span>)):
</span></span><span style=display:flex><span>    fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>figsize)
</span></span><span style=display:flex><span>    layout <span style=color:#f92672>=</span> (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    ts_ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplot2grid(layout, (<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>), colspan<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    acf_ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplot2grid(layout, (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>    pacf_ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplot2grid(layout, (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    y<span style=color:#f92672>.</span>plot(ax<span style=color:#f92672>=</span>ts_ax)
</span></span><span style=display:flex><span>    smt<span style=color:#f92672>.</span>graphics<span style=color:#f92672>.</span>plot_acf(y, lags<span style=color:#f92672>=</span>lags, ax<span style=color:#f92672>=</span>acf_ax)
</span></span><span style=display:flex><span>    smt<span style=color:#f92672>.</span>graphics<span style=color:#f92672>.</span>plot_pacf(y, lags<span style=color:#f92672>=</span>lags, ax<span style=color:#f92672>=</span>pacf_ax)
</span></span><span style=display:flex><span>    [ax<span style=color:#f92672>.</span>set_xlim(<span style=color:#ae81ff>1.5</span>) <span style=color:#66d9ef>for</span> ax <span style=color:#f92672>in</span> [acf_ax, pacf_ax]]
</span></span><span style=display:flex><span>    sns<span style=color:#f92672>.</span>despine()
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>tight_layout()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ts_ax, acf_ax, pacf_ax
</span></span></code></pre></div><p>Calling it on the residuals from the linear trend:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tsplot(res_trend<span style=color:#f92672>.</span>resid, lags<span style=color:#f92672>=</span><span style=color:#ae81ff>36</span>);
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_58_0.png alt=png></p><p>The top subplot shows the time series of our residuals $e_t$, which should be white noise (but it isn&rsquo;t).
The bottom shows the <a href=https://www.otexts.org/fpp/2/2#autocorrelation>autocorrelation</a> of the residuals as a correlogram.
It measures the correlation between a value and it&rsquo;s lagged self, e.g. $corr(e_t, e_{t-1}), corr(e_t, e_{t-2}), \ldots$.
The partial autocorrelation plot in the bottom-right shows a similar concept.
It&rsquo;s partial in the sense that the value for $corr(e_t, e_{t-k})$ is the correlation between those two periods, after controlling for the values at all shorter lags.</p><p>Autocorrelation is a problem in regular regressions like above, but we&rsquo;ll use it to our advantage when we setup an ARIMA model below.
The basic idea is pretty sensible: if your regression residuals have a clear pattern, then there&rsquo;s clearly some structure in the data that you aren&rsquo;t taking advantage of.
If a positive residual today means you&rsquo;ll likely have a positive residual tomorrow, why not incorporate that information into your forecast, and lower your forecasted value for tomorrow?
That&rsquo;s pretty much what ARIMA does.</p><p>It&rsquo;s important that your dataset be stationary, otherwise you run the risk of finding <a href=http://www.tylervigen.com/spurious-correlations>spurious correlations</a>.
A common example is the relationship between number of TVs per person and life expectancy.
It&rsquo;s not likely that there&rsquo;s an actual causal relationship there.
Rather, there could be a third variable that&rsquo;s driving both (wealth, say).
<a href=http://wolfweb.unr.edu/homepage/zal/STAT758/Granger_Newbold_1974.pdf>Granger and Newbold (1974)</a> had some stern words for the econometrics literature on this.</p><blockquote><p>We find it very curious that whereas virtually every textbook on econometric methodology contains explicit warnings of the dangers of autocorrelated errors, this phenomenon crops up so frequently in well-respected applied work.</p></blockquote><p>(:fire:), but in that academic passive-aggressive way.</p><p>The typical way to handle non-stationarity is to difference the non-stationary variable until is is stationary.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y<span style=color:#f92672>.</span>to_frame(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;y&#39;</span>)<span style=color:#f92672>.</span>assign(Δy<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>.</span>y<span style=color:#f92672>.</span>diff())<span style=color:#f92672>.</span>plot(subplots<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_61_0.png alt=png></p><p>Our original series actually doesn&rsquo;t look <em>that</em> bad.
It doesn&rsquo;t look like nominal GDP say, where there&rsquo;s a clearly rising trend.
But we have more rigorous methods for detecting whether a series is non-stationary than simply plotting and squinting at it.
One popular method is the Augmented Dickey-Fuller test.
It&rsquo;s a statistical hypothesis test that roughly says:</p><p>$H_0$ (null hypothesis): $y$ is non-stationary, needs to be differenced</p><p>$H_A$ (alternative hypothesis): $y$ is stationary, doesn&rsquo;t need to be differenced</p><p>I don&rsquo;t want to get into the weeds on exactly what the test statistic is, and what the distribution looks like.
This is implemented in statsmodels as <a href=http://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html><code>smt.adfuller</code></a>.
The return type is a bit busy for me, so we&rsquo;ll wrap it in a <code>namedtuple</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> namedtuple
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ADF <span style=color:#f92672>=</span> namedtuple(<span style=color:#e6db74>&#34;ADF&#34;</span>, <span style=color:#e6db74>&#34;adf pvalue usedlag nobs critical icbest&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ADF(<span style=color:#f92672>*</span>smt<span style=color:#f92672>.</span>adfuller(y))<span style=color:#f92672>.</span>_asdict()
</span></span></code></pre></div><pre><code>OrderedDict([('adf', -1.3206520699512339),
             ('pvalue', 0.61967180643147923),
             ('usedlag', 15),
             ('nobs', 177),
             ('critical',
              {'1%': -3.4678453197999071,
               '10%': -2.575551186759871,
               '5%': -2.8780117454974392}),
             ('icbest', 2710.6120408261486)])
</code></pre><p>So we failed to reject the null hypothesis that the original series was non-stationary.
Let&rsquo;s difference it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ADF(<span style=color:#f92672>*</span>smt<span style=color:#f92672>.</span>adfuller(y<span style=color:#f92672>.</span>diff()<span style=color:#f92672>.</span>dropna()))<span style=color:#f92672>.</span>_asdict()
</span></span></code></pre></div><pre><code>OrderedDict([('adf', -3.6412428797327996),
             ('pvalue', 0.0050197770854934548),
             ('usedlag', 14),
             ('nobs', 177),
             ('critical',
              {'1%': -3.4678453197999071,
               '10%': -2.575551186759871,
               '5%': -2.8780117454974392}),
             ('icbest', 2696.3891181091631)])
</code></pre><p>This looks better.
It&rsquo;s not statistically significant at the 5% level, but who cares what statisticins say anyway.</p><p>We&rsquo;ll fit another OLS model of $\Delta y = \beta_0 + \beta_1 L \Delta y_{t-1} + e_t$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>data <span style=color:#f92672>=</span> (y<span style=color:#f92672>.</span>to_frame(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;y&#39;</span>)
</span></span><span style=display:flex><span>         <span style=color:#f92672>.</span>assign(Δy<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> df: df<span style=color:#f92672>.</span>y<span style=color:#f92672>.</span>diff())
</span></span><span style=display:flex><span>         <span style=color:#f92672>.</span>assign(LΔy<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> df: df<span style=color:#f92672>.</span>Δy<span style=color:#f92672>.</span>shift()))
</span></span><span style=display:flex><span>mod_stationary <span style=color:#f92672>=</span> smf<span style=color:#f92672>.</span>ols(<span style=color:#e6db74>&#39;Δy ~ LΔy&#39;</span>, data<span style=color:#f92672>=</span>data<span style=color:#f92672>.</span>dropna())
</span></span><span style=display:flex><span>res_stationary <span style=color:#f92672>=</span> mod_stationary<span style=color:#f92672>.</span>fit()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tsplot(res_stationary<span style=color:#f92672>.</span>resid, lags<span style=color:#f92672>=</span><span style=color:#ae81ff>24</span>);
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_69_0.png alt=png></p><p>So we&rsquo;ve taken care of multicolinearity, autocorelation, and stationarity, but we still aren&rsquo;t done.</p><h2 id=seasonality>Seasonality<a hidden class=anchor aria-hidden=true href=#seasonality>#</a></h2><p>We have strong monthly seasonality:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>smt<span style=color:#f92672>.</span>seasonal_decompose(y)<span style=color:#f92672>.</span>plot();
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_73_0.png alt=png></p><p>There are a few ways to handle seasonality.
We&rsquo;ll just rely on the <code>SARIMAX</code> method to do it for us.
For now, recognize that it&rsquo;s a problem to be solved.</p><h2 id=arima>ARIMA<a hidden class=anchor aria-hidden=true href=#arima>#</a></h2><p>So, we&rsquo;ve sketched the problems with regular old regression: multicollinearity, autocorrelation, non-stationarity, and seasonality.
Our tool of choice, <code>smt.SARIMAX</code>, which stands for Seasonal ARIMA with eXogenous regressors, can handle all these.
We&rsquo;ll walk through the components in pieces.</p><p>ARIMA stands for AutoRegressive Integrated Moving Average.
It&rsquo;s a relatively simple yet flexible way of modeling univariate time series.
It&rsquo;s made up of three components, and is typically written as $\mathrm{ARIMA}(p, d, q)$.</p><p>ARIMA stands for AutoRegressive Integrated Moving Average, and it&rsquo;s a relatively simple way of modeling univariate time series.
It&rsquo;s made up of three components, and is typically written as $\mathrm{ARIMA}(p, d, q)$.</p><h3 id=autoregressivehttpswwwotextsorgfpp83><a href=https://www.otexts.org/fpp/8/3>AutoRegressive</a><a hidden class=anchor aria-hidden=true href=#autoregressivehttpswwwotextsorgfpp83>#</a></h3><p>The idea is to predict a variable by a linear combination of its lagged values (<em>auto</em>-regressive as in regressing a value on its past <em>self</em>).
An AR(p), where $p$ represents the number of lagged values used, is written as</p><p>$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + e_t$$</p><p>$c$ is a constant and $e_t$ is white noise.
This looks a lot like a linear regression model with multiple predictors, but the predictors happen to be lagged values of $y$ (though they are estimated differently).</p><h3 id=integrated>Integrated<a hidden class=anchor aria-hidden=true href=#integrated>#</a></h3><p>Integrated is like the opposite of differencing, and is the part that deals with stationarity.
If you have to difference your dataset 1 time to get it stationary, then $d=1$.
We&rsquo;ll introduce one bit of notation for differencing: $\Delta y_t = y_t - y_{t-1}$ for $d=1$.</p><h3 id=moving-averagehttpswwwotextsorgfpp84><a href=https://www.otexts.org/fpp/8/4>Moving Average</a><a hidden class=anchor aria-hidden=true href=#moving-averagehttpswwwotextsorgfpp84>#</a></h3><p>MA models look somewhat similar to the AR component, but it&rsquo;s dealing with different values.</p><p>$$y_t = c + e_t + \theta_1 e_{t-1} + \theta_2 e_{t-2} + \ldots + \theta_q e_{t-q}$$</p><p>$c$ again is a constant and $e_t$ again is white noise.
But now the coefficients are the <em>residuals</em> from previous predictions.</p><h3 id=combining>Combining<a hidden class=anchor aria-hidden=true href=#combining>#</a></h3><p>Putting that together, an ARIMA(1, 1, 1) process is written as</p><p>$$\Delta y_t = c + \phi_1 \Delta y_{t-1} + \theta_t e_{t-1} + e_t$$</p><p>Using <em>lag notation</em>, where $L y_t = y_{t-1}$, i.e. <code>y.shift()</code> in pandas, we can rewrite that as</p><p>$$(1 - \phi_1 L) (1 - L)y_t = c + (1 + \theta L)e_t$$</p><p>That was for our specific $\mathrm{ARIMA}(1, 1, 1)$ model. For the general $\mathrm{ARIMA}(p, d, q)$, that becomes</p><p>$$(1 - \phi_1 L - \ldots - \phi_p L^p) (1 - L)^d y_t = c + (1 + \theta L + \ldots + \theta_q L^q)e_t$$</p><p>We went through that <em>extremely</em> quickly, so don&rsquo;t feel bad if things aren&rsquo;t clear.
Fortunately, the model is pretty easy to use with statsmodels (using it <em>correctly</em>, in a statistical sense, is another matter).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>mod <span style=color:#f92672>=</span> smt<span style=color:#f92672>.</span>SARIMAX(y, trend<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;c&#39;</span>, order<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> mod<span style=color:#f92672>.</span>fit()
</span></span><span style=display:flex><span>tsplot(res<span style=color:#f92672>.</span>resid[<span style=color:#ae81ff>2</span>:], lags<span style=color:#f92672>=</span><span style=color:#ae81ff>24</span>);
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_81_0.png alt=png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>res<span style=color:#f92672>.</span>summary()
</span></span></code></pre></div><table class=simpletable><caption>Statespace Model Results</caption><tr><th>Dep. Variable:</th><td>fl_date</td><th>No. Observations:</th><td>193</td></tr><tr><th>Model:</th><td>SARIMAX(1, 1, 1)</td><th>Log Likelihood</th><td>-1494.618</td></tr><tr><th>Date:</th><td>Sun, 03 Sep 2017</td><th>AIC</th><td>2997.236</td></tr><tr><th>Time:</th><td>11:21:50</td><th>BIC</th><td>3010.287</td></tr><tr><th>Sample:</th><td>01-01-2000</td><th>HQIC</th><td>3002.521</td></tr><tr><th></th><td>- 01-01-2016</td><th></th><td></td></tr><tr><th>Covariance Type:</th><td>opg</td><th></th><td></td></tr></table><table class=simpletable><tr><td></td><th>coef</th><th>std err</th><th>z</th><th>P>|z|</th><th>[0.025</th><th>0.975]</th></tr><tr><th>intercept</th><td>-5.4306</td><td>66.818</td><td>-0.081</td><td>0.935</td><td>-136.391</td><td>125.529</td></tr><tr><th>ar.L1</th><td>-0.0327</td><td>2.689</td><td>-0.012</td><td>0.990</td><td>-5.303</td><td>5.237</td></tr><tr><th>ma.L1</th><td>0.0775</td><td>2.667</td><td>0.029</td><td>0.977</td><td>-5.149</td><td>5.305</td></tr><tr><th>sigma2</th><td>3.444e+05</td><td>1.69e+04</td><td>20.392</td><td>0.000</td><td>3.11e+05</td><td>3.77e+05</td></tr></table><table class=simpletable><tr><th>Ljung-Box (Q):</th><td>225.58</td><th>Jarque-Bera (JB):</th><td>1211.00</td></tr><tr><th>Prob(Q):</th><td>0.00</td><th>Prob(JB):</th><td>0.00</td></tr><tr><th>Heteroskedasticity (H):</th><td>0.67</td><th>Skew:</th><td>1.20</td></tr><tr><th>Prob(H) (two-sided):</th><td>0.12</td><th>Kurtosis:</th><td>15.07</td></tr></table><p>There&rsquo;s a bunch of output there with various tests, estimated parameters, and information criteria.
Let&rsquo;s just say that things are looking better, but we still haven&rsquo;t accounted for seasonality.</p><p>A seasonal ARIMA model is written as $\mathrm{ARIMA}(p,d,q)×(P,D,Q)_s$.
Lowercase letters are for the non-seasonal component, just like before. Upper-case letters are a similar specification for the seasonal component, where $s$ is the periodicity (4 for quarterly, 12 for monthly).</p><p>It&rsquo;s like we have two processes, one for non-seasonal component and one for seasonal components, and we multiply them together with regular algebra rules.</p><p>The general form of that looks like (quoting the <a href=http://www.statsmodels.org/dev/examples/notebooks/generated/statespace_sarimax_stata.html>statsmodels docs</a> here)</p><p>$$\phi_p(L)\tilde{\phi}_P(L^S)\Delta^d\Delta_s^D y_t = A(t) + \theta_q(L)\tilde{\theta}_Q(L^s)e_t$$</p><p>where</p><ul><li>$\phi_p(L)$ is the non-seasonal autoregressive lag polynomial</li><li>$\tilde{\phi}_P(L^S)$ is the seasonal autoregressive lag polynomial</li><li>$\Delta^d\Delta_s^D$ is the time series, differenced $d$ times, and seasonally differenced $D$ times.</li><li>$A(t)$ is the trend polynomial (including the intercept)</li><li>$\theta_q(L)$ is the non-seasonal moving average lag polynomial</li><li>$\tilde{\theta}_Q(L^s)$ is the seasonal moving average lag polynomial</li></ul><p>I don&rsquo;t find that to be very clear, but maybe an example will help.
We&rsquo;ll fit a seasonal ARIMA$(1,1,2)×(0, 1, 2)_{12}$.</p><p>So the nonseasonal component is</p><ul><li>$p=1$: period autoregressive: use $y_{t-1}$</li><li>$d=1$: one first-differencing of the data (one month)</li><li>$q=2$: use the previous two non-seasonal residual, $e_{t-1}$ and $e_{t-2}$, to forecast</li></ul><p>And the seasonal component is</p><ul><li>$P=0$: Don&rsquo;t use any previous seasonal values</li><li>$D=1$: Difference the series 12 periods back: <code>y.diff(12)</code></li><li>$Q=2$: Use the two previous seasonal residuals</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>mod_seasonal <span style=color:#f92672>=</span> smt<span style=color:#f92672>.</span>SARIMAX(y, trend<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;c&#39;</span>,
</span></span><span style=display:flex><span>                           order<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>), seasonal_order<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>12</span>),
</span></span><span style=display:flex><span>                           simple_differencing<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>res_seasonal <span style=color:#f92672>=</span> mod_seasonal<span style=color:#f92672>.</span>fit()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>res_seasonal<span style=color:#f92672>.</span>summary()
</span></span></code></pre></div><table class=simpletable><caption>Statespace Model Results</caption><tr><th>Dep. Variable:</th><td>fl_date</td><th>No. Observations:</th><td>193</td></tr><tr><th>Model:</th><td>SARIMAX(1, 1, 2)x(0, 1, 2, 12)</td><th>Log Likelihood</th><td>-1357.847</td></tr><tr><th>Date:</th><td>Sun, 03 Sep 2017</td><th>AIC</th><td>2729.694</td></tr><tr><th>Time:</th><td>11:21:53</td><th>BIC</th><td>2752.533</td></tr><tr><th>Sample:</th><td>01-01-2000</td><th>HQIC</th><td>2738.943</td></tr><tr><th></th><td>- 01-01-2016</td><th></th><td></td></tr><tr><th>Covariance Type:</th><td>opg</td><th></th><td></td></tr></table><table class=simpletable><tr><td></td><th>coef</th><th>std err</th><th>z</th><th>P>|z|</th><th>[0.025</th><th>0.975]</th></tr><tr><th>intercept</th><td>-17.5871</td><td>44.920</td><td>-0.392</td><td>0.695</td><td>-105.628</td><td>70.454</td></tr><tr><th>ar.L1</th><td>-0.9988</td><td>0.013</td><td>-74.479</td><td>0.000</td><td>-1.025</td><td>-0.973</td></tr><tr><th>ma.L1</th><td>0.9956</td><td>0.109</td><td>9.130</td><td>0.000</td><td>0.782</td><td>1.209</td></tr><tr><th>ma.L2</th><td>0.0042</td><td>0.110</td><td>0.038</td><td>0.969</td><td>-0.211</td><td>0.219</td></tr><tr><th>ma.S.L12</th><td>-0.7836</td><td>0.059</td><td>-13.286</td><td>0.000</td><td>-0.899</td><td>-0.668</td></tr><tr><th>ma.S.L24</th><td>0.2118</td><td>0.041</td><td>5.154</td><td>0.000</td><td>0.131</td><td>0.292</td></tr><tr><th>sigma2</th><td>1.842e+05</td><td>1.21e+04</td><td>15.240</td><td>0.000</td><td>1.61e+05</td><td>2.08e+05</td></tr></table><table class=simpletable><tr><th>Ljung-Box (Q):</th><td>32.57</td><th>Jarque-Bera (JB):</th><td>1298.39</td></tr><tr><th>Prob(Q):</th><td>0.79</td><th>Prob(JB):</th><td>0.00</td></tr><tr><th>Heteroskedasticity (H):</th><td>0.17</td><th>Skew:</th><td>-1.33</td></tr><tr><th>Prob(H) (two-sided):</th><td>0.00</td><th>Kurtosis:</th><td>15.89</td></tr></table><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tsplot(res_seasonal<span style=color:#f92672>.</span>resid[<span style=color:#ae81ff>12</span>:], lags<span style=color:#f92672>=</span><span style=color:#ae81ff>24</span>);
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_86_0.png alt=png></p><p>Things look much better now.</p><p>One thing I didn&rsquo;t really talk about is order selection. How to choose $p, d, q, P, D$ and $Q$.
R&rsquo;s forecast package does have a handy <code>auto.arima</code> function that does this for you.
Python / statsmodels don&rsquo;t have that at the minute.
The alternative seems to be experience (boo), intuition (boo), and good-old grid-search.
You can fit a bunch of models for a bunch of combinations of the parameters and use the <a href=https://en.wikipedia.org/wiki/Akaike_information_criterion>AIC</a> or <a href=https://en.wikipedia.org/wiki/Bayesian_information_criterion>BIC</a> to choose the best.
<a href=https://www.otexts.org/fpp/8/7>Here</a> is a useful reference, and <a href=http://stackoverflow.com/a/22770973>this</a> StackOverflow answer recommends a few options.</p><h2 id=forecasting>Forecasting<a hidden class=anchor aria-hidden=true href=#forecasting>#</a></h2><p>Now that we fit that model, let&rsquo;s put it to use.
First, we&rsquo;ll make a bunch of one-step ahead forecasts.
At each point (month), we take the history up to that point and make a forecast for the next month.
So the forecast for January 2014 has available all the data up through December 2013.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pred <span style=color:#f92672>=</span> res_seasonal<span style=color:#f92672>.</span>get_prediction(start<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;2001-03-01&#39;</span>)
</span></span><span style=display:flex><span>pred_ci <span style=color:#f92672>=</span> pred<span style=color:#f92672>.</span>conf_int()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ax <span style=color:#f92672>=</span> y<span style=color:#f92672>.</span>plot(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;observed&#39;</span>)
</span></span><span style=display:flex><span>pred<span style=color:#f92672>.</span>predicted_mean<span style=color:#f92672>.</span>plot(ax<span style=color:#f92672>=</span>ax, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Forecast&#39;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>.7</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>fill_between(pred_ci<span style=color:#f92672>.</span>index,
</span></span><span style=display:flex><span>                pred_ci<span style=color:#f92672>.</span>iloc[:, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                pred_ci<span style=color:#f92672>.</span>iloc[:, <span style=color:#ae81ff>1</span>], color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;k&#39;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>.2</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#34;Monthly Flights&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_91_0.png alt=png></p><p>There are a few places where the observed series slips outside the 95% confidence interval.
The series seems especially unstable before 2005.</p><p>Alternatively, we can make <em>dynamic</em> forecasts as of some month (January 2013 in the example below).
That means the forecast from that point forward only use information available as of January 2013.
The predictions are generated in a similar way: a bunch of one-step forecasts.
Only instead of plugging in the <em>actual</em> values beyond January 2013, we plug in the <em>forecast</em> values.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pred_dy <span style=color:#f92672>=</span> res_seasonal<span style=color:#f92672>.</span>get_prediction(start<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;2002-03-01&#39;</span>, dynamic<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;2013-01-01&#39;</span>)
</span></span><span style=display:flex><span>pred_dy_ci <span style=color:#f92672>=</span> pred_dy<span style=color:#f92672>.</span>conf_int()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ax <span style=color:#f92672>=</span> y<span style=color:#f92672>.</span>plot(label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;observed&#39;</span>)
</span></span><span style=display:flex><span>pred_dy<span style=color:#f92672>.</span>predicted_mean<span style=color:#f92672>.</span>plot(ax<span style=color:#f92672>=</span>ax, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Forecast&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>fill_between(pred_dy_ci<span style=color:#f92672>.</span>index,
</span></span><span style=display:flex><span>                pred_dy_ci<span style=color:#f92672>.</span>iloc[:, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                pred_dy_ci<span style=color:#f92672>.</span>iloc[:, <span style=color:#ae81ff>1</span>], color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;k&#39;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>.25</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#34;Monthly Flights&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Highlight the forecast area</span>
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>fill_betweenx(ax<span style=color:#f92672>.</span>get_ylim(), pd<span style=color:#f92672>.</span>Timestamp(<span style=color:#e6db74>&#39;2013-01-01&#39;</span>), y<span style=color:#f92672>.</span>index[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>                 alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>.1</span>, zorder<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>annotate(<span style=color:#e6db74>&#39;Dynamic $</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>longrightarrow$&#39;</span>, (pd<span style=color:#f92672>.</span>Timestamp(<span style=color:#e6db74>&#39;2013-02-01&#39;</span>), <span style=color:#ae81ff>550</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>despine()
</span></span></code></pre></div><p><img loading=lazy src=/images/modern_7_timeseries_94_0.png alt=png></p><h2 id=resources>Resources<a hidden class=anchor aria-hidden=true href=#resources>#</a></h2><p>This is a collection of links for those interested.</p><h3 id=time-series-modeling-in-python>Time series modeling in Python<a hidden class=anchor aria-hidden=true href=#time-series-modeling-in-python>#</a></h3><ul><li><a href=http://www.statsmodels.org/dev/examples/index.html#statespace>Statsmodels Statespace Notebooks</a></li><li><a href=http://www.statsmodels.org/dev/vector_ar.html#var>Statsmodels VAR tutorial</a></li></ul><ul><li><a href=https://github.com/bashtage/arch>ARCH Library by Kevin Sheppard</a></li></ul><h3 id=general-textbooks>General Textbooks<a hidden class=anchor aria-hidden=true href=#general-textbooks>#</a></h3><ul><li><a href=https://www.otexts.org/fpp/>Forecasting: Principles and Practice</a>: A great introduction</li><li><a href=http://wps.aw.com/aw_stock_ie_3/178/45691/11696965.cw/>Stock and Watson</a>: Readable undergraduate resource, has a few chapters on time series</li><li><a href=http://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm>Greene&rsquo;s Econometric Analysis</a>: My favorite PhD level textbook</li><li><a href=http://www.amazon.com/Time-Analysis-James-Douglas-Hamilton/dp/0691042896>Hamilton&rsquo;s Time Series Analysis</a>: A classic</li><li><a href=http://www.amazon.com/New-Introduction-Multiple-Time-Analysis/dp/3540262393>Lutkehpohl&rsquo;s New Introduction to Multiple Time Series Analysis</a>: Extremely dry, but useful if you&rsquo;re implementing this stuff</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Congratulations if you made it this far, this piece just kept growing (and I still had to cut stuff).
The main thing cut was talking about how <code>SARIMAX</code> is implemented on top of using statsmodels&rsquo; statespace framework.
The statespace framework, developed mostly by Chad Fulton over the past couple years, is really nice.
You can pretty easily <a href=http://www.statsmodels.org/dev/examples/notebooks/generated/statespace_local_linear_trend.html>extend it</a> with custom models, but still get all the benefits of the framework&rsquo;s estimation and results facilities.
I&rsquo;d recommend reading the <a href=http://www.statsmodels.org/dev/examples/index.html#statespace>notebooks</a>.
We also didn&rsquo;t get to talk at all about Skipper Seabold&rsquo;s work on VARs, but maybe some other time.</p><p>As always, <a href=https://twitter.com/tomaugspurger>feedback is welcome</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://tomaugspurger.github.io/tags/pandas/>pandas</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://tomaugspurger.github.io>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>