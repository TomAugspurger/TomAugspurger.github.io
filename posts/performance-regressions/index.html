<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Maintaining Performance | Tom's Blog</title><meta name=keywords content="pandas"><meta name=description content="As pandas&rsquo; documentation claims: pandas provides high-performance data structures. But how do we verify that the claim is correct? And how do we ensure that it stays correct over many releases. This post describes
pandas&rsquo; current setup for monitoring performance My personal debugging strategy for understanding and fixing performance regressions when they occur. I hope that the first section topic is useful for library maintainers and the second topic is generally useful for people writing performance-sensitive code."><meta name=author content><link rel=canonical href=https://tomaugspurger.github.io/posts/performance-regressions/><link crossorigin=anonymous href=/assets/css/stylesheet.3690c96d8a707265a16abd3b389bb33e4e3916869c3142cba43a3cfaaed4b5f9.css integrity="sha256-NpDJbYpwcmWhar07OJuzPk45FoacMULLpDo8+q7Utfk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tomaugspurger.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Maintaining Performance"><meta property="og:description" content="As pandas&rsquo; documentation claims: pandas provides high-performance data structures. But how do we verify that the claim is correct? And how do we ensure that it stays correct over many releases. This post describes
pandas&rsquo; current setup for monitoring performance My personal debugging strategy for understanding and fixing performance regressions when they occur. I hope that the first section topic is useful for library maintainers and the second topic is generally useful for people writing performance-sensitive code."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.github.io/posts/performance-regressions/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-04-01T00:00:00+00:00"><meta property="article:modified_time" content="2020-04-01T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Maintaining Performance"><meta name=twitter:description content="As pandas&rsquo; documentation claims: pandas provides high-performance data structures. But how do we verify that the claim is correct? And how do we ensure that it stays correct over many releases. This post describes
pandas&rsquo; current setup for monitoring performance My personal debugging strategy for understanding and fixing performance regressions when they occur. I hope that the first section topic is useful for library maintainers and the second topic is generally useful for people writing performance-sensitive code."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Maintaining Performance","item":"https://tomaugspurger.github.io/posts/performance-regressions/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Maintaining Performance","name":"Maintaining Performance","description":"As pandas\u0026rsquo; documentation claims: pandas provides high-performance data structures. But how do we verify that the claim is correct? And how do we ensure that it stays correct over many releases. This post describes\npandas\u0026rsquo; current setup for monitoring performance My personal debugging strategy for understanding and fixing performance regressions when they occur. I hope that the first section topic is useful for library maintainers and the second topic is generally useful for people writing performance-sensitive code.","keywords":["pandas"],"articleBody":"As pandas’ documentation claims: pandas provides high-performance data structures. But how do we verify that the claim is correct? And how do we ensure that it stays correct over many releases. This post describes\npandas’ current setup for monitoring performance My personal debugging strategy for understanding and fixing performance regressions when they occur. I hope that the first section topic is useful for library maintainers and the second topic is generally useful for people writing performance-sensitive code.\nKnow thyself The first rule of optimization is to measure first. It’s a common trap to think you know the performance of some code just from looking at it. The difficulty is compounded when you’re reviewing a diff in a pull request and you lack some important context. We use benchmarks to measure the performance of code.\nThere’s a strong analogy between using unit tests to verify the correctness of code and using benchmarks to verify its performance. Each gives us some confidence that an implementation behaves as expected and that refactors are not introducing regressions (in correctness or performance). And just as you use can use a test runner like unittest or pytest to organize and run unit tests, you can use a tool to organize and run benchmarks.\nFor that, pandas uses asv.\nairspeed velocity (asv) is a tool for benchmarking Python packages over their lifetime. Runtime, memory consumption and even custom-computed values may be tracked. The results are displayed in an interactive web frontend that requires only a basic static webserver to host.\nasv provides a structured way to write benchmarks. For example, pandas Series.isin benchmark looks roughly like\nclass IsIn: def setup(self): self.s = Series(np.random.randint(1, 10, 100000)) self.values = [1, 2] def time_isin(self): self.s.isin(self.values) There’s some setup, and then the benchmark method starting with time_. Using the asv CLI, benchmarks can be run for a specific commit with asv run , or multiple commits can be compared with asv continuous . Finally, asv will collect performance over time and can visualize the output. You can see pandas’ at https://pandas.pydata.org/speed/pandas/.\nDetecting Regressions asv is designed to be run continuously over a project’s lifetime. In theory, a pull request could be accompanied with an asv report demonstrating that the changes don’t introduce a performance regression. There are a few issues preventing pandas from doing that reliably however, which I’ll go into later.\nHandling Regressions Here’s a high-level overview of my debugging process when a performance regression is discovered (either by ASV detecting one or a user reporting a regression).\nTo make things concrete, we’ll walk through this recent pandas issue, where a slowdown was reported. User reports are often along the lines of\nDataFrame.memory_usage is 100x slower in pandas 1.0 compared to 0.25\nIn this case, DataFrame.memory_usage was slower with object-dtypes and deep=True.\nv1.0.3: memory_usage(deep=True) took 26.4566secs v0.24.0: memory_usage(deep=True) took 6.0479secs v0.23.4: memory_usage(deep=True) took 0.4633secs The first thing to verify is that it’s purely a performance regression, and not a behavior change or bugfix, by ensuring that the outputs match between versions. Sometimes correctness requires sacrificing speed. In this example, we confirmed that the outputs from 0.24 and 1.0.3 matched, so we focused there.\nNow that we have what seems like a legitimate slowdown, I’ll reproduce it locally. I’ll first activate environments for both the old and new versions (I use conda for this, one environment per version of pandas, but venv works as well assuming the error isn’t specific to a version of Python). Then I ensure that I can reproduce the slowdown.\nIn [1]: import pandas as pd In [2]: df = pd.DataFrame({\"A\": list(range(10000))}, dtype=object) In [3]: %timeit df.memory_usage(deep=True) 5.37 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) In [4]: pd.__version__ Out[4]: '0.25.1' versus\nIn [1]: import pandas as pd In [2]: df = pd.DataFrame({\"A\": list(range(10000))}, dtype=object) In [3]: %timeit df.memory_usage(deep=True) 17.5 ms ± 98.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) In [4]: pd.__version__ Out[4]: '1.0.1' So we do have a slowdown, from 5.37ms -\u003e 17.5ms on this example.\nOnce I’ve verified that the outputs match and the slowdown is real, I turn to snakeviz (created by Matt Davis, which measures performance at the function-level. For large enough slowdowns, the issue will jump out immediately with snakeviz.\npandas 0.25\npandas 1.0\nFrom the snakeviz docs, these charts show\nthe fraction of time spent in a function is represented by the extent of a visualization element, either the width of a rectangle or the angular extent of an arc.\nI prefer the “sunburst” / angular extent style, but either works.\nIn this case, I noticed that ~95% of the time was being spent in pandas._libs.lib.memory_usage_of_object, and most of that time was spent in PandasArray.__getitem__ in pandas 1.0.3. This is where a bit of pandas-specific knowledge comes in, but suffice to say, it looks fishy1.\nAs an aside, to create and share these snakeviz profiles, I ran the output of the %snakeviz command through svstatic and uploaded that as a gist (using gist). I then pasted the “raw” URL to https://rawgit.org/ to get the URL embedded here as an iframe.\nLine Profiling With snakeviz, we’ve identified a function or two that’s slowing things down. If I need more details on why that’s function is slow, I’ll use line-profiler. In our example, we’ve identified a couple of functions, IndexOpsMixin.memory_usage and PandasArray.__getitem__ that could be inspected in detail.\nYou point line-profiler at one or more functions with -f and provide a statement to execute. It will measure things about each line in the function, including the number of times it’s hit and how long is spent on that line (per hit and total)\nIn [9]: %load_ext line_profiler In [10]: %lprun -f pd.core.base.IndexOpsMixin.memory_usage df.memory_usage(deep=True) Total time: 0.034319 s File: /Users/taugspurger/miniconda3/envs/pandas=1.0.1/lib/python3.8/site-packages/pandas/core/base.py Function: memory_usage at line 1340 Line # Hits Time Per Hit % Time Line Contents ============================================================== 1340 def memory_usage(self, deep=False): ... 1363 1 56.0 56.0 0.2 if hasattr(self.array, \"memory_usage\"): 1364 return self.array.memory_usage(deep=deep) 1365 1366 1 11.0 11.0 0.0 v = self.array.nbytes 1367 1 18.0 18.0 0.1 if deep and is_object_dtype(self) and not PYPY: 1368 1 34233.0 34233.0 99.7 v += lib.memory_usage_of_objects(self.array) 1369 1 1.0 1.0 0.0 return v THe % time column clearly points to lib.memory_usage_of_objects. This is a Cython function, so we can’t use line-profiler on it. But we know from the snakeviz output above that we eventually get to PandasArray.__getitem__\nIn [11]: %lprun -f pd.arrays.PandasArray.__getitem__ df.memory_usage(deep=True) Timer unit: 1e-06 s Total time: 0.041508 s File: /Users/taugspurger/miniconda3/envs/pandas=1.0.1/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py Function: __getitem__ at line 232 Line # Hits Time Per Hit % Time Line Contents ============================================================== 232 def __getitem__(self, item): 233 10000 4246.0 0.4 10.2 if isinstance(item, type(self)): 234 item = item._ndarray 235 236 10000 25475.0 2.5 61.4 item = check_array_indexer(self, item) 237 238 10000 4394.0 0.4 10.6 result = self._ndarray[item] 239 10000 4386.0 0.4 10.6 if not lib.is_scalar(item): 240 result = type(self)(result) 241 10000 3007.0 0.3 7.2 return result In this particular example, the most notable thing is that fact that we’re calling this function 10,000 times, which amounts to once per item on our 10,000 row DataFrame. Again, the details of this specific example and the fix aren’t too important, but the solution was to just stop doing that2.\nThe fix was provided by @neilkg soon after the issue was identified, and crucially included a new asv benchmark for memory_usage with object dtypes. Hopefully we won’t regress on this again in the future.\nWorkflow issues This setup is certainly better than nothing. But there are a few notable problems, some general and some specific to pandas:\nWriting benchmarks is hard work (just like tests). There’s the general issue of writing and maintaining code. And on top of that, writing a good ASV benchmark requires some knowledge specific to ASV. And again, just like tests, your benchmarks can be trusted only as far as their coverage. For a large codebase like pandas you’ll need a decently large benchmark suite.\nBut that large benchmark suite comes with it’s own costs. Currently pandas’ full suite takes about 2 hours to run. This rules out running the benchmarks on most public CI providers. And even if we could finish it in time, we couldn’t really trust the results. These benchmarks, at least as written, really do need dedicated hardware to be stable over time. Pandas has a machine in my basement, but maintaining that has been a time-consuming, challenging process.\nThis is my current setup, which stuffs the benchmark server (the black Intel NUC) and a router next to my wife’s art storage. We reached this solution after my 2 year old unplugged the old setup (on my office floor) one too many times. Apologies for the poor cabling.\nWe deploy the benchmarks (for pandas and a few other NumFOCUS projects) using Ansible. The scripts get the benchmarks in place, Airflow to run them nightly, and supervisord to kick everything off. The outputs are rsynced over to the pandas webserver and served at https://pandas.pydata.org/speed/. You can see pandas’ at https://pandas.pydata.org/speed/pandas/. If this seems like a house of cards waiting to tumble, that’s because it is.\nPandas has applied for a NumFOCUS small development grant to improve our benchmark process. Ideally maintainers would be able to ask a bot @asv-bot run -b memory_usage which would kick off a process that pulled down the pull request and ran the requested benchmarks on a dedicated machine (that isn’t easily accessible by my children).\nRecap To summarize:\nWe need benchmarks to monitor performance, especially over time We use tools like asv to organize and benchmark continuously When regressions occur, we use snakeviz and line-profiler to diagnose the problem PandasArray is a very simple wrapper that implements pandas' ExtensionArray interface for 1d NumPy ndarrays, so it’s essentially just an ndarray. But, crucially, it’s a Python class so it’s getitem is relatively slow compared to numpy.ndarray’s getitem. ↩︎\nIt still does an elementwise getitem, but NumPy’s __getitem__ is much faster than PandasArray’s. ↩︎\n","wordCount":"1654","inLanguage":"en","datePublished":"2020-04-01T00:00:00Z","dateModified":"2020-04-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.github.io/posts/performance-regressions/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.github.io accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.github.io/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.github.io/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Maintaining Performance</h1><div class=post-meta><span title='2020-04-01 00:00:00 +0000 UTC'>April 1, 2020</span></div></header><div class=post-content><p>As pandas&rsquo; <a href=https://pandas.pydata.org/docs/>documentation</a> claims: pandas
provides <em>high-performance</em> data structures. But how do we verify that the claim
is correct? And how do we ensure that it <em>stays</em> correct over many releases.
This post describes</p><ol><li>pandas&rsquo; current setup for monitoring performance</li><li>My personal debugging strategy for understanding and fixing performance
regressions when they occur.</li></ol><p>I hope that the first section topic is useful for library maintainers and the
second topic is generally useful for people writing performance-sensitive code.</p><h2 id=know-thyself>Know thyself<a hidden class=anchor aria-hidden=true href=#know-thyself>#</a></h2><p>The first rule of optimization is to measure first. It&rsquo;s a common trap to think
you know the performance of some code just from looking at it. The difficulty is
compounded when you&rsquo;re reviewing a diff in a pull request and you lack some
important context. We use benchmarks to measure the performance of code.</p><p>There&rsquo;s a strong analogy between using unit tests to verify the correctness of
code and using benchmarks to verify its performance. Each gives us some
confidence that an implementation behaves as expected and that refactors are not
introducing regressions (in correctness or performance). And just as you use can
use a test runner like <code>unittest</code> or <code>pytest</code> to organize and run unit tests,
you can use a tool to organize and run benchmarks.</p><p>For that, pandas uses <a href=https://asv.readthedocs.io/en/stable/>asv</a>.</p><blockquote><p>airspeed velocity (<code>asv</code>) is a tool for benchmarking Python packages over
their lifetime. Runtime, memory consumption and even custom-computed values
may be tracked. The results are displayed in an interactive web frontend that
requires only a basic static webserver to host.</p></blockquote><p><code>asv</code> provides a structured way to write benchmarks. For example, pandas <code>Series.isin</code>
<a href=https://github.com/pandas-dev/pandas/blob/d1b1236f0d8402d1df6ad7cd916d07ba45706269/asv_bench/benchmarks/series_methods.py>benchmark</a> looks roughly like</p><pre tabindex=0><code>class IsIn:

    def setup(self):
        self.s = Series(np.random.randint(1, 10, 100000))
        self.values = [1, 2]

    def time_isin(self):
        self.s.isin(self.values)
</code></pre><p>There&rsquo;s some setup, and then the benchmark method starting with <code>time_</code>. Using
the <code>asv</code> CLI, benchmarks can be run for a specific commit with
<code>asv run &lt;commit HASH></code>, or multiple commits can be compared with
<code>asv continuous &lt;GIT RANGE></code>. Finally, <code>asv</code> will collect performance over time
and can visualize the output. You can see pandas&rsquo; at
<a href=https://pandas.pydata.org/speed/pandas/>https://pandas.pydata.org/speed/pandas/</a>.</p><p><img loading=lazy src=/images/asv-overview.png alt="pandas&amp;rsquo; asv overview"></p><h2 id=detecting-regressions>Detecting Regressions<a hidden class=anchor aria-hidden=true href=#detecting-regressions>#</a></h2><p><code>asv</code> is designed to be run continuously over a project&rsquo;s lifetime. In theory, a
pull request could be accompanied with an <code>asv</code> report demonstrating that the
changes don&rsquo;t introduce a performance regression. There are a few issues
preventing pandas from doing that reliably however, which I&rsquo;ll go into later.</p><h2 id=handling-regressions>Handling Regressions<a hidden class=anchor aria-hidden=true href=#handling-regressions>#</a></h2><p>Here&rsquo;s a high-level overview of my debugging process when a performance
regression is discovered (either by ASV detecting one or a user reporting a
regression).</p><p>To make things concrete, we&rsquo;ll walk through <a href=https://github.com/pandas-dev/pandas/issues/33012>this recent pandas
issue</a>, where a slowdown was
reported. User reports are often along the lines of</p><blockquote><p><code>DataFrame.memory_usage</code> is 100x slower in pandas 1.0 compared to 0.25</p></blockquote><p>In this case, <code>DataFrame.memory_usage</code> was slower with <code>object</code>-dtypes and
<code>deep=True</code>.</p><pre tabindex=0><code>v1.0.3: memory_usage(deep=True) took 26.4566secs

v0.24.0: memory_usage(deep=True) took 6.0479secs

v0.23.4: memory_usage(deep=True) took 0.4633secs
</code></pre><p>The first thing to verify is that it&rsquo;s purely a performance regression, and not
a behavior change or bugfix, by <a href=https://github.com/pandas-dev/pandas/issues/33012#issuecomment-603828279>ensuring that the outputs
match</a>
between versions. Sometimes correctness requires sacrificing speed. In this
example, we confirmed that the outputs from 0.24 and 1.0.3 matched, so we
focused there.</p><p>Now that we have what seems like a legitimate slowdown, I&rsquo;ll reproduce it
locally. I&rsquo;ll first activate environments for both the old and new versions (I
use <a href=https://conda.io/en/latest/><code>conda</code></a> for this, one environment per version
of pandas, but <code>venv</code> works as well assuming the error isn&rsquo;t specific to a
version of Python). Then I ensure that I can reproduce the slowdown.</p><p><img loading=lazy src=/images/performance-comparison.png alt="Comparison of two benchmarks"></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>1</span>]: <span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>2</span>]: df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#34;A&#34;</span>: list(range(<span style=color:#ae81ff>10000</span>))}, dtype<span style=color:#f92672>=</span>object)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>3</span>]: <span style=color:#f92672>%</span>timeit df<span style=color:#f92672>.</span>memory_usage(deep<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span><span style=color:#ae81ff>5.37</span> ms <span style=color:#960050;background-color:#1e0010>±</span> <span style=color:#ae81ff>201</span> µs per loop (mean <span style=color:#960050;background-color:#1e0010>±</span> std<span style=color:#f92672>.</span> dev<span style=color:#f92672>.</span> of <span style=color:#ae81ff>7</span> runs, <span style=color:#ae81ff>100</span> loops each)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>4</span>]: pd<span style=color:#f92672>.</span>__version__
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>4</span>]: <span style=color:#e6db74>&#39;0.25.1&#39;</span>
</span></span></code></pre></div><p>versus</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>1</span>]: <span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>2</span>]: df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#34;A&#34;</span>: list(range(<span style=color:#ae81ff>10000</span>))}, dtype<span style=color:#f92672>=</span>object)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>3</span>]: <span style=color:#f92672>%</span>timeit df<span style=color:#f92672>.</span>memory_usage(deep<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span><span style=color:#ae81ff>17.5</span> ms <span style=color:#960050;background-color:#1e0010>±</span> <span style=color:#ae81ff>98.7</span> µs per loop (mean <span style=color:#960050;background-color:#1e0010>±</span> std<span style=color:#f92672>.</span> dev<span style=color:#f92672>.</span> of <span style=color:#ae81ff>7</span> runs, <span style=color:#ae81ff>100</span> loops each)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>4</span>]: pd<span style=color:#f92672>.</span>__version__
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>4</span>]: <span style=color:#e6db74>&#39;1.0.1&#39;</span>
</span></span></code></pre></div><p>So we do have a slowdown, from 5.37ms -> 17.5ms on this example.</p><p>Once I&rsquo;ve verified that the outputs match and the slowdown is real, I turn to
<a href=https://jiffyclub.github.io/snakeviz/>snakeviz</a> (created by <a href=https://twitter.com/jiffyclub>Matt
Davis</a>, which measures performance at the
function-level. For large enough slowdowns, the issue will jump out immediately
with snakeviz.</p><p><a href=https://gistcdn.rawgit.org/TomAugspurger/bad09c3e4a03338590545033ad2da925/353fbf67bb1ff7c6f039854af1c2d51cb503c865/pandas-0.25_static.html><strong>pandas 0.25</strong></a></p><iframe title="pandas-0.25 snakeviz" width=900px height=600px src=https://gistcdn.rawgit.org/TomAugspurger/bad09c3e4a03338590545033ad2da925/353fbf67bb1ff7c6f039854af1c2d51cb503c865/pandas-0.25_static.html></iframe><p><a href=https://gistcdn.rawgit.org/TomAugspurger/98bc79523187f1fde3093b5af63ae68c/5b1d3815fb2319ff365c3881daeac70bf9077e77/pandas-1.0_static.html><strong>pandas 1.0</strong></a></p><iframe title="pandas 1.0.3 snakeviz" width=900px height=600px src=https://gistcdn.rawgit.org/TomAugspurger/98bc79523187f1fde3093b5af63ae68c/5b1d3815fb2319ff365c3881daeac70bf9077e77/pandas-1.0_static.html></iframe><p>From the <a href=https://jiffyclub.github.io/snakeviz/#interpreting-results>snakeviz
docs</a>, these charts
show</p><blockquote><p>the fraction of time spent in a function is represented by the extent of a
visualization element, either the width of a rectangle or the angular extent
of an arc.</p></blockquote><p>I prefer the &ldquo;sunburst&rdquo; / angular extent style, but either works.</p><p>In this case, I noticed that ~95% of the time was being spent in
<code>pandas._libs.lib.memory_usage_of_object</code>, and most of that time was spent in
<code>PandasArray.__getitem__</code> in pandas 1.0.3. This is where a bit of
pandas-specific knowledge comes in, but suffice to say, it looks fishy<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>As an aside, to create and share these snakeviz profiles, I ran the output of
the <code>%snakeviz</code> command through
<a href=https://gist.github.com/jiffyclub/6b5e0f0f05ab487ff607><code>svstatic</code></a> and
uploaded that as a gist (using <a href=https://github.com/defunkt/gist><code>gist</code></a>). I
then pasted the &ldquo;raw&rdquo; URL to <a href=https://rawgit.org/>https://rawgit.org/</a> to get the URL embedded here as
an iframe.</p><h2 id=line-profiling>Line Profiling<a hidden class=anchor aria-hidden=true href=#line-profiling>#</a></h2><p>With snakeviz, we&rsquo;ve identified a function or two that&rsquo;s slowing things down. If
I need more details on <em>why</em> that&rsquo;s function is slow, I&rsquo;ll use
<a href=https://github.com/rkern/line_profiler>line-profiler</a>. In our example, we&rsquo;ve
identified a couple of functions, <code>IndexOpsMixin.memory_usage</code> and
<code>PandasArray.__getitem__</code> that could be inspected in detail.</p><p>You point <code>line-profiler</code> at one or more functions with <code>-f</code> and provide a
statement to execute. It will measure things about each line in the function,
including the number of times it&rsquo;s hit and how long is spent on that line (per
hit and total)</p><pre tabindex=0><code>In  [9]: %load_ext line_profiler
In [10]: %lprun -f pd.core.base.IndexOpsMixin.memory_usage df.memory_usage(deep=True)
Total time: 0.034319 s
File: /Users/taugspurger/miniconda3/envs/pandas=1.0.1/lib/python3.8/site-packages/pandas/core/base.py
Function: memory_usage at line 1340

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1340                                               def memory_usage(self, deep=False):
  ...
  1363         1         56.0     56.0      0.2          if hasattr(self.array, &#34;memory_usage&#34;):
  1364                                                       return self.array.memory_usage(deep=deep)
  1365
  1366         1         11.0     11.0      0.0          v = self.array.nbytes
  1367         1         18.0     18.0      0.1          if deep and is_object_dtype(self) and not PYPY:
  1368         1      34233.0  34233.0     99.7              v += lib.memory_usage_of_objects(self.array)
  1369         1          1.0      1.0      0.0          return v
</code></pre><p>THe <code>% time</code> column clearly points to <code>lib.memory_usage_of_objects</code>. This is a
Cython function, so we can&rsquo;t use <code>line-profiler</code> on it. But we know from the
snakeviz output above that we eventually get to <code>PandasArray.__getitem__</code></p><pre tabindex=0><code>In [11]: %lprun -f pd.arrays.PandasArray.__getitem__ df.memory_usage(deep=True)
Timer unit: 1e-06 s

Total time: 0.041508 s
File: /Users/taugspurger/miniconda3/envs/pandas=1.0.1/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py
Function: __getitem__ at line 232

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   232                                               def __getitem__(self, item):
   233     10000       4246.0      0.4     10.2          if isinstance(item, type(self)):
   234                                                       item = item._ndarray
   235
   236     10000      25475.0      2.5     61.4          item = check_array_indexer(self, item)
   237
   238     10000       4394.0      0.4     10.6          result = self._ndarray[item]
   239     10000       4386.0      0.4     10.6          if not lib.is_scalar(item):
   240                                                       result = type(self)(result)
   241     10000       3007.0      0.3      7.2          return result
</code></pre><p>In this particular example, the most notable thing is that fact that we&rsquo;re
calling this function 10,000 times, which amounts to once per item on our 10,000
row <code>DataFrame</code>. Again, the details of this specific example and the fix aren&rsquo;t
too important, but the solution was to just stop doing that<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p><a href=https://github.com/pandas-dev/pandas/pull/33102>The fix</a> was provided by
<a href=https://github.com/neilkg>@neilkg</a> soon after the issue was identified, and
crucially included a new asv benchmark for <code>memory_usage</code> with object dtypes.
Hopefully we won&rsquo;t regress on this again in the future.</p><h2 id=workflow-issues>Workflow issues<a hidden class=anchor aria-hidden=true href=#workflow-issues>#</a></h2><p>This setup is certainly better than nothing. But there are a few notable
problems, some general and some specific to pandas:</p><p>Writing benchmarks is hard work (just like tests). There&rsquo;s the general issue of
writing and maintaining code. And on top of that, writing a good ASV benchmark
requires some knowledge specific to ASV. And again, just like tests, your
benchmarks can be trusted only as far as their coverage. For a large codebase
like pandas you&rsquo;ll need a decently large benchmark suite.</p><p>But that large benchmark suite comes with it&rsquo;s own costs. Currently pandas&rsquo; full
suite takes about 2 hours to run. This rules out running the benchmarks on most
public CI providers. And even if we could finish it in time, we couldn&rsquo;t really
trust the results. These benchmarks, at least as written, really do need
dedicated hardware to be stable over time. Pandas has a machine in my basement,
but maintaining that has been a time-consuming, challenging process.</p><p><img loading=lazy src=/images/benchmark-server.png alt="Pandas&amp;rsquo; benchmark server"></p><p>This is my current setup, which stuffs the benchmark server (the black Intel
NUC) and a router next to my wife&rsquo;s art storage. We reached this solution after
my 2 year old unplugged the old setup (on my office floor) one too many times.
Apologies for the poor cabling.</p><p>We <a href=https://github.com/asv-runner/asv-runner>deploy the benchmarks</a> (for pandas
and a few other NumFOCUS projects) using Ansible. The scripts get the benchmarks
in place, Airflow to run them nightly, and supervisord to kick everything off.
The outputs are <code>rsync</code>ed over to the pandas webserver and served at
<a href=https://pandas.pydata.org/speed/>https://pandas.pydata.org/speed/</a>. You can
see pandas&rsquo; at
<a href=https://pandas.pydata.org/speed/pandas>https://pandas.pydata.org/speed/pandas/</a>.
If this seems like a house of cards waiting to tumble, that&rsquo;s because it is.</p><p><img loading=lazy src=/images/performance-airflow.png alt="pandas&amp;rsquo; airflow server"></p><p>Pandas has applied for a NumFOCUS small development grant to improve our
benchmark process. Ideally maintainers would be able to ask a bot <code>@asv-bot run -b memory_usage</code> which would kick off a process that pulled down the pull
request and ran the requested benchmarks on a dedicated machine (that isn&rsquo;t
easily accessible by my children).</p><h2 id=recap>Recap<a hidden class=anchor aria-hidden=true href=#recap>#</a></h2><p>To summarize:</p><ol><li>We need benchmarks to monitor performance, especially over time</li><li>We use tools like <code>asv</code> to organize and benchmark continuously</li><li>When regressions occur, we use <code>snakeviz</code> and <code>line-profiler</code> to diagnose the
problem</li></ol><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>PandasArray is a very simple wrapper that implements pandas'
ExtensionArray interface for 1d NumPy ndarrays, so it&rsquo;s essentially just an
ndarray. But, crucially, it&rsquo;s a Python class so it&rsquo;s getitem is relatively
slow compared to numpy.ndarray&rsquo;s getitem.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>It still does an elementwise getitem, but NumPy&rsquo;s <code>__getitem__</code> is much
faster than <code>PandasArray</code>&rsquo;s.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://tomaugspurger.github.io/tags/pandas/>pandas</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://tomaugspurger.github.io>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>