<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>STAC Updates I'm Excited About | Tom's Blog</title>
<meta name=keywords content><meta name=description content="I wanted to share an update on a couple of developments in the STAC ecosystem that I&rsquo;m excited about. It&rsquo;s a great sign that even after 2 years after its initial release, the STAC ecosystem is still growing and improving how we can catalog, serve, and access geospatial data.
STAC and Geoparquet A STAC API is a great way to query for data. But, like any API serving JSON, its throughput is limited."><meta name=author content><link rel=canonical href=https://tomaugspurger.net/posts/stac-updates/><link crossorigin=anonymous href=/assets/css/stylesheet.3690c96d8a707265a16abd3b389bb33e4e3916869c3142cba43a3cfaaed4b5f9.css integrity="sha256-NpDJbYpwcmWhar07OJuzPk45FoacMULLpDo8+q7Utfk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://tomaugspurger.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.net/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.net/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="STAC Updates I'm Excited About"><meta property="og:description" content="I wanted to share an update on a couple of developments in the STAC ecosystem that I&rsquo;m excited about. It&rsquo;s a great sign that even after 2 years after its initial release, the STAC ecosystem is still growing and improving how we can catalog, serve, and access geospatial data.
STAC and Geoparquet A STAC API is a great way to query for data. But, like any API serving JSON, its throughput is limited."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.net/posts/stac-updates/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-15T12:00:00-05:00"><meta property="article:modified_time" content="2023-10-15T12:00:00-05:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="STAC Updates I'm Excited About"><meta name=twitter:description content="I wanted to share an update on a couple of developments in the STAC ecosystem that I&rsquo;m excited about. It&rsquo;s a great sign that even after 2 years after its initial release, the STAC ecosystem is still growing and improving how we can catalog, serve, and access geospatial data.
STAC and Geoparquet A STAC API is a great way to query for data. But, like any API serving JSON, its throughput is limited."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.net/posts/"},{"@type":"ListItem","position":3,"name":"STAC Updates I'm Excited About","item":"https://tomaugspurger.net/posts/stac-updates/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"STAC Updates I'm Excited About","name":"STAC Updates I\u0027m Excited About","description":"I wanted to share an update on a couple of developments in the STAC ecosystem that I\u0026rsquo;m excited about. It\u0026rsquo;s a great sign that even after 2 years after its initial release, the STAC ecosystem is still growing and improving how we can catalog, serve, and access geospatial data.\nSTAC and Geoparquet A STAC API is a great way to query for data. But, like any API serving JSON, its throughput is limited.","keywords":[],"articleBody":"I wanted to share an update on a couple of developments in the STAC ecosystem that I’m excited about. It’s a great sign that even after 2 years after its initial release, the STAC ecosystem is still growing and improving how we can catalog, serve, and access geospatial data.\nSTAC and Geoparquet A STAC API is a great way to query for data. But, like any API serving JSON, its throughput is limited. So in May 2022, the Planetary Computer team decided to export snapshots of our STAC database as geoparquet. Each STAC collection is exported as a Parquet dataset, where each record in the dataset is a STAC item. We pitched this as a way to do bulk queries over the data, where returning many and many pages of JSON would be slow (and expensive for our servers and database).\nLooking at the commit history, the initial prototype was done over a couple of days. I wish I had my notes from our discussions, but this feels like the kind of thing that came out of an informal discussion like “This access pattern kind of sucks”, followed by “What if we …. ?”, and then “Let’s try it!1”. And so we tried it, and it’s been great!\nI think STAC as geoparquet can become a standard way to transfer STAC data in bulk. Chris Holmes has an open PR defining a specification for what the columns and types should be, which will help more tools than just that stac-geoparquet library interpret the data.\nAnd Kyle Barron has an open PR making the stac-geoparquet library “arrow-native” by using Apache Arrow arrays and tables directly (via pyarrow), rather than pandas / geopandas. When I initially sketched out stac-geoparquet, it might have been just a bit early to do that. But given that we’re dealing with complicated, nested types (which isn’t NumPy’s strong suite) and we aren’t doing any analysis (which is pandas / NumPy’s strong suite), this will be a great way to move the data around.\nNow I’m just hoping for a PostgreSQL ADBC adapter so that our PostGIS database can output the STAC items as Arrow memory. Then we can be all Arrow from the time the data leaves the database to the time we’re writing the parquet files.\nSTAC and Kerchunk Kerchunk is, I think, going to see some widespread adoption over the next year or two. It’s a project (both a Python library and a specification) for putting a cloud-optimized veneer on top of non-cloud optimized data formats (like NetCDF / HDF5 and GRIB2).\nBriefly, those file formats tend not to work great in the cloud because\nIn the cloud, we want to read files over the network (data are stored in Blob Storage, which is on a different machine than your compute). These file formats are pretty complicated, and can typically only be read by one library implemented in C / C++, which isn’t always able to read data over the network. Reading the metadata (to build a structure like an xarray Dataset) tends to require reading many small pieces of data from many parts of the file. This is slow over the network, where each small read could translate to an HTTP request. On an SSD, seeking around the file to gather metadata is fine. Over the network, it’s slow. Together, those mean that you aren’t able to easily load subsets of the data (even if the data are internally chunked!). You can’t load the metadata to do your filtering operations, and even if you could you might need to download the whole file just to throw away a bunch of data.\nThat’s where Kerchunk comes in. The idea is that the data provider can scan the files once ahead of time, extracting the Kerchunk indices, which include\nThe metadata (dimension names, coordinate values, attributes, etc.), letting you build a high-level object like an xarray.Dataset without needing any (additional) HTTP requests. The byte offsets for each chunk of each data variable, letting you access arbitrary subsets of the data without needing to download and discard unnecessary data. You store that metadata somewhere (in a JSON file, say) and users access the original NetCDF / GRIB2 data via that Kerchunk index file. You can even do metadata-only operations, like combining data variables from many files, or concatenating along a dimension to make a time series, without ever downloading the data.\nWe’ve had some experimental support for accessing a couple datasets hosted on the Planetary Computer via Kerchunk indices for a while now. We generated some indices and through them up in Blob Storage, including them as an asset in the STAC item. I’ve never really been happy with how how that works in practice, because of the extra hop from STAC to Kerchunk to the actual data.\nI think that Kerchunk is just weird enough and hard enough to use that it can take time for users to feel comfortable with it. It’s hard to explain that if you want the data from this NetCDF file, you need to download this other JSON file, and then open that up with this other fsspec filesystem (no, not the Azure Blob Storage filesystem where the NetCDF and JSON files are, that’ll come later), and pass that result to the Zarr reader in xarray (no, the data isn’t stored in Zarr, we’re just using the Zarr API to access the data via the references…).\nThose two additional levels of indirection (through a sidecar JSON file and then the Zarr reader via fsspec’s reference file system) are a real hurdle. So some of my teammate’s are working on storing the Kerchunk indices in the STAC items.\nMy goal is to enable an access pattern like this:\n\u003e\u003e\u003e import xarray as xr \u003e\u003e\u003e import pystac_client \u003e\u003e\u003e catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\") \u003e\u003e\u003e items = catalog.search(collections=[\"noaa-nwm\"], datetime=\"2023-10-15\", query=...) \u003e\u003e\u003e ds = xr.open_dataset(items, engine=\"stac\") Where the step from STAC to xarray / pandas / whatever is as easy with NetCDF or GRIB2 data as it is with COGs are Zarr data (thanks to projects like stackstac and odc-stac.) This is using ideas from Julia Signell’s xpystac library for that final layer, which would know how to translate the STAC items (with embedded Kerchunk references) into an xarray Dataset.\nI just made an update to xstac, a library for creating STAC items for data that can be reresented as an xarray Datasets, to add support for embedding Kerchunk indices in a STAC item representing a dataset. The goal is to be “STAC-native” (by using things like the datacube extension), while still providing enough information for Kerchunk to do its thing. I’ll do a proper STAC extension later, but I want to get some real-world usage of it first.\nI think this is similar in spirit to how Arraylake can store Kerchunk indices in their database, which hooks into their Zarr-compatible API.\nThe main concern here is that we’d blow up the size of the STAC items. That would bloat our database, slow down STAC queries and responses. But overall, I think it’s worth it for the ergonomics when it comes to loading the data. We’ll see.\nGetting Involved Reach out, either on GitHub or by email, if you’re interested in getting involved in any of these projects.\nI do distinctly remember that our “hosted QGIS” was exactly that. Yuvi had made a post on the Pangeo Discourse and Dan had asked about how Desktop GIS users could use Planetary Computer data (we had just helped fund the STAC plugin for QGIS). I added that JupyterHub profile based on Yuvi and Scott Hendersen’s work and haven’t touched it since. ↩︎\n","wordCount":"1272","inLanguage":"en","datePublished":"2023-10-15T12:00:00-05:00","dateModified":"2023-10-15T12:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.net/posts/stac-updates/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.net accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.net/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.net/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.net/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.net/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>STAC Updates I'm Excited About</h1><div class=post-meta>&lt;span title='2023-10-15 12:00:00 -0500 -0500'>October 15, 2023&lt;/span></div></header><div class=post-content><p>I wanted to share an update on a couple of developments in the <a href=https://stacspec.org/>STAC</a> ecosystem
that I&rsquo;m excited about. It&rsquo;s a great sign that even after 2 years after its
initial release, the STAC ecosystem is still growing and improving how we can
catalog, serve, and access geospatial data.</p><h2 id=stac-and-geoparquet>STAC and Geoparquet<a hidden class=anchor aria-hidden=true href=#stac-and-geoparquet>#</a></h2><p>A STAC API is a great way to query for data. But, like any API serving JSON, its
throughput is limited. So in May 2022, the Planetary Computer team decided to
export snapshots of our STAC database as <a href=https://geoparquet.org/>geoparquet</a>. Each STAC collection is
exported as a Parquet dataset, where each record in the dataset is a STAC item.
We pitched this as a way to do <a href=https://planetarycomputer.microsoft.com/docs/quickstarts/stac-geoparquet/>bulk queries</a> over the data, where
returning many and many pages of JSON would be slow (and expensive for our
servers and database).</p><p>Looking at the <a href="https://github.com/stac-utils/stac-geoparquet/commits/main?after=a5a5bf2d958672c36f0c4c4cc827970833c18380+69&amp;branch=main&amp;qualified_name=refs%2Fheads%2Fmain">commit history</a>, the initial prototype was done over a
couple of days. I wish I had my notes from our discussions, but this feels like
the kind of thing that came out of an informal discussion like &ldquo;This access
pattern kind of sucks&rdquo;, followed by &ldquo;What if we &mldr;. ?&rdquo;, and then &ldquo;Let&rsquo;s
try it!<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>&rdquo;. And so we tried it, and it&rsquo;s been great!</p><p>I think STAC as geoparquet can become a standard way to transfer STAC data in
bulk. Chris Holmes has an <a href=https://github.com/stac-utils/stac-geoparquet/pull/28>open PR</a> defining a specification for
what the columns and types should be, which will help more tools than just that
<code>stac-geoparquet</code> library interpret the data.</p><p>And Kyle Barron has an <a href=https://github.com/stac-utils/stac-geoparquet/pull/27>open PR</a> making the <code>stac-geoparquet</code>
library &ldquo;arrow-native&rdquo; by using <a href=https://arrow.apache.org/>Apache Arrow</a> arrays and tables directly
(via pyarrow), rather than pandas / geopandas. When I initially sketched out
<code>stac-geoparquet</code>, it might have been just a bit early to do that. But given
that we&rsquo;re dealing with complicated, nested types (which isn&rsquo;t NumPy&rsquo;s strong
suite) and we aren&rsquo;t doing any analysis (which <em>is</em> pandas / NumPy&rsquo;s strong
suite), this will be a great way to move the data around.</p><p>Now I&rsquo;m just hoping for a PostgreSQL <a href=https://arrow.apache.org/docs/format/ADBC.html>ADBC</a> adapter so that our PostGIS
database can output the STAC items as Arrow memory. Then we can be all Arrow
from the time the data leaves the database to the time we&rsquo;re writing the parquet
files.</p><h2 id=stac-and-kerchunk>STAC and Kerchunk<a hidden class=anchor aria-hidden=true href=#stac-and-kerchunk>#</a></h2><p><a href=https://fsspec.github.io/kerchunk/>Kerchunk</a> is, I think, going to see some widespread adoption over the next year
or two. It&rsquo;s a project (both a Python library and a specification) for putting a
cloud-optimized veneer on top of non-cloud optimized data formats (like NetCDF /
<a href=https://www.hdfgroup.org/>HDF5</a> and GRIB2).</p><p>Briefly, those file formats tend not to work great in the cloud because</p><ol><li>In the cloud, we want to read files over the network (data are stored in Blob
Storage, which is on a different machine than your compute). These file
formats are pretty complicated, and can typically only be read by one library
implemented in C / C++, which isn&rsquo;t always able to read data over the
network.</li><li>Reading the <em>metadata</em> (to build a structure like an xarray Dataset) tends to
require reading many small pieces of data from many parts of the file. This
is slow over the network, where each small read could translate to an HTTP
request. On an SSD, seeking around the file to gather metadata is fine. Over
the network, it&rsquo;s slow.</li></ol><p>Together, those mean that you aren&rsquo;t able to easily load subsets of the data
(even if the data are internally chunked!). You can&rsquo;t load the metadata to do
your filtering operations, and even if you could you might need to download the
whole file just to throw away a bunch of data.</p><p>That&rsquo;s where Kerchunk comes in. The idea is that the data provider can scan the
files once ahead of time, extracting the <em>Kerchunk indices</em>, which include</p><ol><li>The metadata (dimension names, coordinate values, attributes, etc.), letting
you build a high-level object like an <code>xarray.Dataset</code> without needing any
(additional) HTTP requests.</li><li>The byte offsets for each chunk of each data variable, letting you access
arbitrary subsets of the data without needing to download and discard
unnecessary data.</li></ol><p>You store that metadata somewhere (in a JSON file, say) and users access the
original NetCDF / GRIB2 data via that Kerchunk index file. You can even do
metadata-only operations, like combining data variables from many files, or
concatenating along a dimension to make a time series, without ever downloading
the data.</p><p>We&rsquo;ve had some <a href=https://planetarycomputer.microsoft.com/dataset/nasa-nex-gddp-cmip6#Example-Notebook>experimental support</a> for accessing a couple
datasets hosted on the Planetary Computer via Kerchunk indices for a while now.
We generated some indices and through them up in Blob Storage, including them as
an asset in the STAC item. I&rsquo;ve never really been happy with how how that works
in practice, because of the extra hop from STAC to Kerchunk to the actual data.</p><p>I think that Kerchunk is just weird enough and hard enough to use that it can
take time for users to feel comfortable with it. It&rsquo;s hard to explain that if
you want the data from <em>this</em> NetCDF file, you need to download this <em>other</em>
JSON file, and then open that up with this other fsspec filesystem (no, not the
Azure Blob Storage filesystem where the NetCDF and JSON files are, that&rsquo;ll come
later), and pass that result to the Zarr reader in xarray (no, the data isn&rsquo;t
stored in Zarr, we&rsquo;re just using the Zarr API to access the data via the
references&mldr;).</p><p>Those two additional levels of indirection (through a sidecar JSON file and then
the Zarr reader via fsspec&rsquo;s reference file system) are a real hurdle. So some
of my teammate&rsquo;s are working on storing the Kerchunk indices in the STAC
items.</p><p>My goal is to enable an access pattern like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>import</span> xarray <span style=color:#66d9ef>as</span> xr
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>import</span> pystac_client
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> catalog <span style=color:#f92672>=</span> pystac_client<span style=color:#f92672>.</span>Client<span style=color:#f92672>.</span>open(<span style=color:#e6db74>&#34;https://planetarycomputer.microsoft.com/api/stac/v1&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> items <span style=color:#f92672>=</span> catalog<span style=color:#f92672>.</span>search(collections<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;noaa-nwm&#34;</span>], datetime<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;2023-10-15&#34;</span>, query<span style=color:#f92672>=...</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> ds <span style=color:#f92672>=</span> xr<span style=color:#f92672>.</span>open_dataset(items, engine<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;stac&#34;</span>)
</span></span></code></pre></div><p>Where the step from STAC to xarray / pandas / whatever is as easy with NetCDF or
GRIB2 data as it is with COGs are Zarr data (thanks to projects like <a href=https://stackstac.readthedocs.io/en/latest/>stackstac</a>
and <a href=https://odc-stac.readthedocs.io/>odc-stac</a>.) This is using ideas from Julia Signell&rsquo;s <a href=https://github.com/stac-utils/xpystac>xpystac</a> library for
that final layer, which would know how to translate the STAC items (with
embedded Kerchunk references) into an xarray Dataset.</p><p>I just made an <a href=https://github.com/stac-utils/xstac/pull/38>update</a> to <a href=https://github.com/stac-utils/xstac>xstac</a>,
a library for creating STAC items for data that can be reresented as an xarray
Datasets, to add support for embedding Kerchunk indices in a STAC item
representing a dataset. The goal is to be &ldquo;STAC-native&rdquo; (by using things like
the datacube extension), while still providing enough information for Kerchunk
to do its thing. I&rsquo;ll do a proper STAC extension later, but I want to get some
real-world usage of it first.</p><p>I think this is similar in spirit to how
<a href=https://docs.earthmover.io/>Arraylake</a> can store Kerchunk indices in their
database, which hooks into their Zarr-compatible API.</p><p>The main concern here is that we&rsquo;d blow up the size of the STAC items. That
would bloat our database, slow down STAC queries and responses. But overall, I
think it&rsquo;s worth it for the ergonomics when it comes to loading the data. We&rsquo;ll
see.</p><h2 id=getting-involved>Getting Involved<a hidden class=anchor aria-hidden=true href=#getting-involved>#</a></h2><p>Reach out, either on GitHub or by email, if you&rsquo;re interested in getting
involved in any of these projects.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>I do distinctly remember that our <a href=https://planetarycomputer.microsoft.com/docs/overview/qgis-plugin/>&ldquo;hosted
QGIS&rdquo;</a>
was exactly that. <a href=https://github.com/yuvipanda>Yuvi</a> had made a
<a href=https://discourse.pangeo.io/t/run-linux-desktop-apps-in-mybinder-org-your-jupyterhub/1978>post</a>
on the Pangeo Discourse and
<a href=http://awesomesongbook.com/00s_songs/00s_songs.html>Dan</a> had asked about
how Desktop GIS users could use Planetary Computer data (we had just helped
fund the STAC plugin for QGIS). I added that JupyterHub profile based on
Yuvi and Scott Hendersen&rsquo;s work and haven&rsquo;t touched it since.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://tomaugspurger.net>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>