<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Distributed Auto-ML with TPOT with Dask | Tom's Blog</title><meta name=keywords content><meta name=description content="This work is supported by Anaconda Inc.
This post describes a recent improvement made to TPOT. TPOT is an automated machine learning library for Python. It does some feature engineering and hyper-parameter optimization for you. TPOT uses genetic algorithms to evaluate which models are performing well and how to choose new models to try out in the next generation.
Parallelizing TPOT In TPOT-730, we made some modifications to TPOT to support distributed training."><meta name=author content><link rel=canonical href=https://tomaugspurger.github.io/posts/distributed-tpot/><link crossorigin=anonymous href=/assets/css/stylesheet.67b4a5a78be69ca812d40e5543fbd83efd0b0cc5025c2505fd7758fd5d32ec2e.css integrity="sha256-Z7Slp4vmnKgS1A5VQ/vYPv0LDMUCXCUF/XdY/V0y7C4=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tomaugspurger.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tomaugspurger.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tomaugspurger.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://tomaugspurger.github.io/apple-touch-icon.png><link rel=mask-icon href=https://tomaugspurger.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Distributed Auto-ML with TPOT with Dask"><meta property="og:description" content="This work is supported by Anaconda Inc.
This post describes a recent improvement made to TPOT. TPOT is an automated machine learning library for Python. It does some feature engineering and hyper-parameter optimization for you. TPOT uses genetic algorithms to evaluate which models are performing well and how to choose new models to try out in the next generation.
Parallelizing TPOT In TPOT-730, we made some modifications to TPOT to support distributed training."><meta property="og:type" content="article"><meta property="og:url" content="https://tomaugspurger.github.io/posts/distributed-tpot/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-08-30T00:00:00+00:00"><meta property="article:modified_time" content="2018-08-30T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Distributed Auto-ML with TPOT with Dask"><meta name=twitter:description content="This work is supported by Anaconda Inc.
This post describes a recent improvement made to TPOT. TPOT is an automated machine learning library for Python. It does some feature engineering and hyper-parameter optimization for you. TPOT uses genetic algorithms to evaluate which models are performing well and how to choose new models to try out in the next generation.
Parallelizing TPOT In TPOT-730, we made some modifications to TPOT to support distributed training."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://tomaugspurger.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Distributed Auto-ML with TPOT with Dask","item":"https://tomaugspurger.github.io/posts/distributed-tpot/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Distributed Auto-ML with TPOT with Dask","name":"Distributed Auto-ML with TPOT with Dask","description":"This work is supported by Anaconda Inc.\nThis post describes a recent improvement made to TPOT. TPOT is an automated machine learning library for Python. It does some feature engineering and hyper-parameter optimization for you. TPOT uses genetic algorithms to evaluate which models are performing well and how to choose new models to try out in the next generation.\nParallelizing TPOT In TPOT-730, we made some modifications to TPOT to support distributed training.","keywords":[],"articleBody":"This work is supported by Anaconda Inc.\nThis post describes a recent improvement made to TPOT. TPOT is an automated machine learning library for Python. It does some feature engineering and hyper-parameter optimization for you. TPOT uses genetic algorithms to evaluate which models are performing well and how to choose new models to try out in the next generation.\nParallelizing TPOT In TPOT-730, we made some modifications to TPOT to support distributed training. As a TPOT user, the only changes you need to make to your code are\nConnect a client to your Dask Cluster Specify the use_dask=True argument to your TPOT estimator From there, all the training will use your cluster of machines. This screencast shows an example on an 80-core Dask cluster.\nCommentary Fitting a TPOT estimator consists of several stages. The bulk of the time is spent evaluating individual scikit-learn pipelines. Dask-ML already had code for splitting apart a scikit-learn Pipeline.fit call into individual tasks. This is used in Dask-ML’s hyper-parameter optimization to avoid repeating work. We were able to drop-in Dask-ML’s fit and scoring method for the one already used in TPOT. That small change allows fitting the many individual models in a generation to be done on a cluster.\nThere’s still some room for improvement. Internal to TPOT, some time is spent determining the next set of models to try out (this is the “mutation and crossover phase”). That’s not (yet) been parallelized with Dask, so you’ll notice some periods of inactivity on the cluster.\nNext Steps This will be available in the next release of TPOT. You can try out a small example now on the dask-examples binder.\nStepping back a bit, I think this is a good example of how libraries can use Dask internally to parallelize workloads for their users. Deep down in TPOT there was a single method for fitting many scikit-learn models on some data and collecting the results. Dask-ML has code for building a task graph that does the same thing. We were able to swap out the eager TPOT code for the lazy dask version, and get things distributed on a cluster. Projects like xarray have been able to do a similar thing with dask Arrays in place of NumPy arrays. If Dask-ML hadn’t already had that code, dask.delayed could have been used instead.\nIf you have a library that you think could take advantage of Dask, please reach out!\n","wordCount":"402","inLanguage":"en","datePublished":"2018-08-30T00:00:00Z","dateModified":"2018-08-30T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tomaugspurger.github.io/posts/distributed-tpot/"},"publisher":{"@type":"Organization","name":"Tom's Blog","logo":{"@type":"ImageObject","url":"https://tomaugspurger.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tomaugspurger.github.io accesskey=h title="Tom's Blog (Alt + H)">Tom's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tomaugspurger.github.io/about/ title=About><span>About</span></a></li><li><a href=https://tomaugspurger.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://tomaugspurger.github.io/index.xml title=RSS><span>RSS</span></a></li><li><a href=https://tomaugspurger.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Distributed Auto-ML with TPOT with Dask</h1><div class=post-meta><span title='2018-08-30 00:00:00 +0000 UTC'>August 30, 2018</span></div></header><div class=post-content><p><em>This work is supported by <a href=https://www.anaconda.com/>Anaconda Inc</a>.</em></p><p>This post describes a recent improvement made to <a href=https://epistasislab.github.io/tpot/>TPOT</a>. TPOT is an
<a href=https://en.wikipedia.org/wiki/Automated_machine_learning>automated machine learning</a> library for Python. It does some feature
engineering and hyper-parameter optimization for you. TPOT uses <a href=https://en.wikipedia.org/wiki/Genetic_programming>genetic
algorithms</a> to evaluate which models are performing well and how to choose
new models to try out in the next generation.</p><h2 id=parallelizing-tpot>Parallelizing TPOT<a hidden class=anchor aria-hidden=true href=#parallelizing-tpot>#</a></h2><p>In <a href=https://github.com/EpistasisLab/tpot/pull/730>TPOT-730</a>, we made some modifications to TPOT to support
distributed training. As a TPOT user, the only changes you need to make to your
code are</p><ol><li>Connect a client to your Dask Cluster</li><li>Specify the <code>use_dask=True</code> argument to your TPOT estimator</li></ol><p>From there, all the training will use your cluster of machines. This screencast
shows an example on an 80-core Dask cluster.</p><iframe width=560 height=315 src="https://www.youtube-nocookie.com/embed/uyx9nBuOYQQ?rel=0" frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id=commentary>Commentary<a hidden class=anchor aria-hidden=true href=#commentary>#</a></h2><p>Fitting a TPOT estimator consists of several stages. The bulk of the time is
spent evaluating individual scikit-learn pipelines. Dask-ML already had code for
splitting apart a scikit-learn <code>Pipeline.fit</code> call into individual tasks. This
is used in Dask-ML&rsquo;s hyper-parameter optimization to <a href=https://dask.github.io/dask-ml/hyper-parameter-search.html#avoid-repeated-work>avoid repeating
work</a>. We were able to drop-in Dask-ML&rsquo;s fit and scoring method
for the one already used in TPOT. That small change allows fitting the many
individual models in a generation to be done on a cluster.</p><p>There&rsquo;s still some room for improvement. Internal to TPOT, some time is spent
determining the next set of models to try out (this is the &ldquo;mutation and
crossover phase&rdquo;). That&rsquo;s not (yet) been parallelized with Dask, so you&rsquo;ll
notice some periods of inactivity on the cluster.</p><h2 id=next-steps>Next Steps<a hidden class=anchor aria-hidden=true href=#next-steps>#</a></h2><p>This will be available in the next release of TPOT. You can try out a small
example now on the <a href="https://mybinder.org/v2/gh/dask/dask-examples/master?filepath=machine-learning%2Ftpot.ipynb">dask-examples binder</a>.</p><p>Stepping back a bit, I think this is a good example of how libraries can use
Dask internally to parallelize workloads for their users. Deep down in TPOT
there was a single method for fitting many scikit-learn models on some data and
collecting the results. Dask-ML has code for <em>building a task graph</em> that does
the same thing. We were able to swap out the eager TPOT code for the lazy dask
version, and get things distributed on a cluster. Projects like <a href=http://xarray.pydata.org/en/stable/>xarray</a>
have been able to do a similar thing with <a href=http://xarray.pydata.org/en/stable/dask.html>dask Arrays in place of NumPy
arrays</a>. If Dask-ML hadn&rsquo;t already had that code,
<a href=http://dask.pydata.org/en/latest/delayed.html><code>dask.delayed</code></a> could have been used instead.</p><p>If you have a library that you think could take advantage of Dask, please <a href=https://github.com/dask/dask>reach
out</a>!</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://tomaugspurger.github.io>Tom's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a rel=me href=https://mastodon.social/@TomAugspurger></a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>