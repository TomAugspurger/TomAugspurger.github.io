<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tom's Blog</title><link>https://tomaugspurger.github.io/</link><description>Recent content on Tom's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 18 Dec 2022 16:51:46 -0600</lastBuildDate><atom:link href="https://tomaugspurger.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Rebooting</title><link>https://tomaugspurger.github.io/posts/rebooting/</link><pubDate>Sun, 18 Dec 2022 16:51:46 -0600</pubDate><guid>https://tomaugspurger.github.io/posts/rebooting/</guid><description>Like some others, I&amp;rsquo;m getting back into blogging.
I&amp;rsquo;ll be &amp;ldquo;straying from my lane&amp;rdquo; and won&amp;rsquo;t just be writing about Python data libraries (though there will still be some of that). If you too would like to blog more, I&amp;rsquo;d encourge you to read Simon Willison&amp;rsquo;s What to blog About and Matt Rocklin&amp;rsquo;s Write Short Blogposts.
Because I&amp;rsquo;m me, I couldn&amp;rsquo;t just make a new post. I also had to switch static site generators, just becauase.</description></item><item><title>What's Next?</title><link>https://tomaugspurger.github.io/posts/whats-next/</link><pubDate>Wed, 11 Nov 2020 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/whats-next/</guid><description>Some personal news: Last Friday was my last day at Anaconda. Next week, I&amp;rsquo;m joining Microsoft&amp;rsquo;s AI for Earth team. This is a very bittersweet transition. While I loved working at Anaconda and all the great people there, I&amp;rsquo;m extremely excited about what I&amp;rsquo;ll be working on at Microsoft.
Reflections I was inspired to write this section by Jim Crist&amp;rsquo;s post on a similar topic: https://jcristharif.com/farewell-to-anaconda.html. I&amp;rsquo;ll highlight some of the projects I worked on while at Anaconda.</description></item><item><title>Maintaining Performance</title><link>https://tomaugspurger.github.io/posts/performance-regressions/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/performance-regressions/</guid><description>As pandas&amp;rsquo; documentation claims: pandas provides high-performance data structures. But how do we verify that the claim is correct? And how do we ensure that it stays correct over many releases. This post describes
pandas&amp;rsquo; current setup for monitoring performance My personal debugging strategy for understanding and fixing performance regressions when they occur. I hope that the first section topic is useful for library maintainers and the second topic is generally useful for people writing performance-sensitive code.</description></item><item><title>Dask Workshop</title><link>https://tomaugspurger.github.io/posts/dask-workshop/</link><pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/dask-workshop/</guid><description>Dask Summit Recap
Last week was the first Dask Developer Workshop. This brought together many of the core Dask developers and its heavy users to discuss the project. I want to share some of the experience with those who weren&amp;rsquo;t able to attend.
This was a great event. Aside from any technical discussions, it was ncie to meet all the people. From new acquaintences to people you&amp;rsquo;re on weekly calls with, it was great to interact with everyone.</description></item><item><title>A Confluence of Extension</title><link>https://tomaugspurger.github.io/posts/confluence-extension/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/confluence-extension/</guid><description>This post describes a few protocols taking shape in the scientific Python community. On their own, each is powerful. Together, I think they enable for an explosion of creativity in the community.
Each of the protocols / interfaces we&amp;rsquo;ll consider deal with extending.
NEP-13: NumPy __array_ufunc__ NEP-18: NumPy __array_function__ Pandas Extension types Custom Dask Collections First, a bit of brief background on each.
NEP-13 and NEP-18, each deal with using the NumPy API on non-NumPy ndarray objects.</description></item><item><title>Tabular Data in Scikit-Learn and Dask-ML</title><link>https://tomaugspurger.github.io/posts/sklearn-dask-tabular/</link><pubDate>Mon, 17 Sep 2018 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/sklearn-dask-tabular/</guid><description>Scikit-Learn 0.20.0 will contain some nice new features for working with tabular data. This blogpost will introduce those improvements with a small demo. We&amp;rsquo;ll then see how Dask-ML was able to piggyback on the work done by scikit-learn to offer a version that works well with Dask Arrays and DataFrames.
import dask import dask.array as da import dask.dataframe as dd import numpy as np import pandas as pd import seaborn as sns import fastparquet from distributed import Client from distributed.</description></item><item><title>Distributed Auto-ML with TPOT with Dask</title><link>https://tomaugspurger.github.io/posts/distributed-tpot/</link><pubDate>Thu, 30 Aug 2018 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/distributed-tpot/</guid><description>This work is supported by Anaconda Inc.
This post describes a recent improvement made to TPOT. TPOT is an automated machine learning library for Python. It does some feature engineering and hyper-parameter optimization for you. TPOT uses genetic algorithms to evaluate which models are performing well and how to choose new models to try out in the next generation.
Parallelizing TPOT In TPOT-730, we made some modifications to TPOT to support distributed training.</description></item><item><title>Moral Philosophy for pandas or: What is `.values`?</title><link>https://tomaugspurger.github.io/posts/pandas-moral-philosophy/</link><pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/pandas-moral-philosophy/</guid><description>The other day, I put up a Twitter poll asking a simple question: What&amp;rsquo;s the type of series.values?
Pop Quiz! What are the possible results for the following:
&amp;gt;&amp;gt;&amp;gt; type(pandas.Series.values)
&amp;mdash; Tom Augspurger (@TomAugspurger) August 6, 2018 I was a bit limited for space, so I&amp;rsquo;ll expand on the options here. Choose as many as you want.
NumPy ndarray pandas Categorical (or all of the above) An Index or any of it&amp;rsquo;s subclasses (DatetimeIndex, CategoricalIndex, RangeIndex, etc.</description></item><item><title>Modern Pandas (Part 8): Scaling</title><link>https://tomaugspurger.github.io/posts/modern-8-scaling/</link><pubDate>Mon, 23 Apr 2018 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/modern-8-scaling/</guid><description>This is part 1 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling As I sit down to write this, the third-most popular pandas question on StackOverflow covers how to use pandas for large datasets. This is in tension with the fact that a pandas DataFrame is an in memory container. You can&amp;rsquo;t have a DataFrame larger than your machine&amp;rsquo;s RAM.</description></item><item><title>dask-ml 0.4.1 Released</title><link>https://tomaugspurger.github.io/posts/dask-ml-041/</link><pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/dask-ml-041/</guid><description>This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.
dask-ml 0.4.1 was released today with a few enhancements. See the changelog for all the changes from 0.4.0.
Conda packages are available on conda-forge
$ conda install -c conda-forge dask-ml and wheels and the source are available on PyPI
$ pip install dask-ml I wanted to highlight one change, that touches on a topic I mentioned in my first post on scalable Machine Learning.</description></item><item><title>Extension Arrays for Pandas</title><link>https://tomaugspurger.github.io/posts/pandas-extension-arrays/</link><pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/pandas-extension-arrays/</guid><description>This is a status update on some enhancements for pandas. The goal of the work is to store things that are sufficiently array-like in a pandas DataFrame, even if they aren&amp;rsquo;t a regular NumPy array. Pandas already does this in a few places for some blessed types (like Categorical); we&amp;rsquo;d like to open that up to anybody.
A couple months ago, a client came to Anaconda with a problem: they have a bunch of IP Address data that they&amp;rsquo;d like to work with in pandas.</description></item><item><title>Easy distributed training with Joblib and dask</title><link>https://tomaugspurger.github.io/posts/distributed-joblib/</link><pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/distributed-joblib/</guid><description>This work is supported by Anaconda Inc and the Data Driven Discovery Initiative from the Moore Foundation.
This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I&amp;rsquo;m thankful to them for hosting me and Anaconda for sending me there. This article will talk about some improvements we made to improve training scikit-learn models using a cluster.</description></item><item><title>Rewriting scikit-learn for big data, in under 9 hours.</title><link>https://tomaugspurger.github.io/posts/dask-ml-iid/</link><pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/dask-ml-iid/</guid><description>This past week, I had a chance to visit some of the scikit-learn developers at Inria in Paris. It was a fun and productive week, and I&amp;rsquo;m thankful to them for hosting me and Anaconda for sending me there.
Towards the end of our week, Gael threw out the observation that for many applications, you don&amp;rsquo;t need to train on the entire dataset, a sample is often sufficient. But it&amp;rsquo;d be nice if the trained estimator would be able to transform and predict for dask arrays, getting all the nice distributed parallelism and memory management dask brings.</description></item><item><title>dask-ml</title><link>https://tomaugspurger.github.io/posts/dask-ml-announce/</link><pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/dask-ml-announce/</guid><description>Today we released the first version of dask-ml, a library for parallel and distributed machine learning. Read the documentation or install it with
pip install dask-ml Packages are currently building for conda-forge, and will be up later today.
conda install -c conda-forge dask-ml The Goals dask is, to quote the docs, &amp;ldquo;a flexible parallel computing library for analytic computing.&amp;rdquo; dask.array and dask.dataframe have done a great job scaling NumPy arrays and pandas dataframes; dask-ml hopes to do the same in the machine learning domain.</description></item><item><title>Scalable Machine Learning (Part 3): Parallel</title><link>https://tomaugspurger.github.io/posts/scalable-ml-03/</link><pubDate>Sat, 16 Sep 2017 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/scalable-ml-03/</guid><description>This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
This is part three of my series on scalable machine learning.
Small Fit, Big Predict Scikit-Learn Partial Fit Parallel Machine Learning You can download a notebook of this post [here][notebook].
In part one, I talked about the type of constraints that push us to parallelize or distribute a machine learning workload. Today, we&amp;rsquo;ll be talking about the second constraint, &amp;ldquo;I&amp;rsquo;m constrained by time, and would like to fit more models at once, by using all the cores of my laptop, or all the machines in my cluster&amp;rdquo;.</description></item><item><title>Scalable Machine Learning (Part 2): Partial Fit</title><link>https://tomaugspurger.github.io/posts/scalable-ml-02/</link><pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/scalable-ml-02/</guid><description>This work is supported by Anaconda, Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
This is part two of my series on scalable machine learning.
Small Fit, Big Predict Scikit-Learn Partial Fit You can download a notebook of this post here.
Scikit-learn supports out-of-core learning (fitting a model on a dataset that doesn&amp;rsquo;t fit in RAM), through it&amp;rsquo;s partial_fit API. See here.
The basic idea is that, for certain estimators, learning can be done in batches.</description></item><item><title>Scalable Machine Learning (Part 1)</title><link>https://tomaugspurger.github.io/posts/scalable-ml-01/</link><pubDate>Mon, 11 Sep 2017 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/scalable-ml-01/</guid><description>This work is supported by Anaconda Inc. and the Data Driven Discovery Initiative from the Moore Foundation.
Anaconda is interested in scaling the scientific python ecosystem. My current focus is on out-of-core, parallel, and distributed machine learning. This series of posts will introduce those concepts, explore what we have available today, and track the community&amp;rsquo;s efforts to push the boundaries.
You can download a Jupyter notebook demonstrating the analysis here.</description></item><item><title>Dask Performace Trip</title><link>https://tomaugspurger.github.io/posts/dask-performance-story/</link><pubDate>Tue, 06 Sep 2016 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/dask-performance-story/</guid><description>I&amp;rsquo;m faced with a fairly specific problem: Compute the pairwise distances between two matrices $X$ and $Y$ as quickly as possible. We&amp;rsquo;ll assume that $Y$ is fairly small, but $X$ may not fit in memory. This post tracks my progress.</description></item><item><title>Introducing Stitch</title><link>https://tomaugspurger.github.io/posts/intro-stitch/</link><pubDate>Tue, 30 Aug 2016 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/intro-stitch/</guid><description>Today I released stitch into the wild. If you haven&amp;rsquo;t yet, check out the examples page to see an example of what stitch does, and the Github repo for how to install. I&amp;rsquo;m using this post to explain why I wrote stitch, and some issues it tries to solve.
Why knitr / knitpy / stitch / RMarkdown? Each of these tools or formats have the same high-level goal: produce reproducible, dynamic (to changes in the data) reports.</description></item><item><title>Modern Pandas (Part 7): Timeseries</title><link>https://tomaugspurger.github.io/posts/modern-7-timeseries/</link><pubDate>Fri, 13 May 2016 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/modern-7-timeseries/</guid><description>This is part 7 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Timeseries Pandas started out in the financial world, so naturally it has strong timeseries support.
The first half of this post will look at pandas&amp;rsquo; capabilities for manipulating time series data. The second half will discuss modelling time series data with statsmodels.
%matplotlib inline import os import numpy as np import pandas as pd import pandas_datareader.</description></item><item><title>Modern Pandas (Part 6): Visualization</title><link>https://tomaugspurger.github.io/posts/modern-6-visualization/</link><pubDate>Thu, 28 Apr 2016 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/modern-6-visualization/</guid><description>This is part 6 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Visualization and Exploratory Analysis A few weeks ago, the R community went through some hand-wringing about plotting packages. For outsiders (like me) the details aren&amp;rsquo;t that important, but some brief background might be useful so we can transfer the takeaways to Python. The competing systems are &amp;ldquo;base R&amp;rdquo;, which is the plotting system built into the language, and ggplot2, Hadley Wickham&amp;rsquo;s implementation of the grammar of graphics.</description></item><item><title>Modern Pandas (Part 5): Tidy Data</title><link>https://tomaugspurger.github.io/posts/modern-5-tidy/</link><pubDate>Fri, 22 Apr 2016 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/modern-5-tidy/</guid><description>This is part 5 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Reshaping &amp;amp; Tidy Data Structuring datasets to facilitate analysis (Wickham 2014)
So, you&amp;rsquo;ve sat down to analyze a new dataset. What do you do first?
In episode 11 of Not So Standard Deviations, Hilary and Roger discussed their typical approaches. I&amp;rsquo;m with Hilary on this one, you should make sure your data is tidy.</description></item><item><title>Modern Panadas (Part 3): Indexes</title><link>https://tomaugspurger.github.io/posts/modern-3-indexes/</link><pubDate>Mon, 11 Apr 2016 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/modern-3-indexes/</guid><description>This is part 3 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Indexes can be a difficult concept to grasp at first. I suspect this is partly becuase they&amp;rsquo;re somewhat peculiar to pandas. These aren&amp;rsquo;t like the indexes put on relational database tables for performance optimizations. Rather, they&amp;rsquo;re more like the row_labels of an R DataFrame, but much more capable.</description></item><item><title>Modern Pandas (Part 4): Performance</title><link>https://tomaugspurger.github.io/posts/modern-4-performance/</link><pubDate>Fri, 08 Apr 2016 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/modern-4-performance/</guid><description>This is part 4 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Wes McKinney, the creator of pandas, is kind of obsessed with performance. From micro-optimizations for element access, to embedding a fast hash table inside pandas, we all benefit from his and others&amp;rsquo; hard work. This post will focus mainly on making efficient use of pandas and NumPy.</description></item><item><title>Modern Pandas (Part 2): Method Chaining</title><link>https://tomaugspurger.github.io/posts/method-chaining/</link><pubDate>Mon, 04 Apr 2016 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/method-chaining/</guid><description>This is part 2 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Method Chaining Method chaining, where you call methods on an object one after another, is in vogue at the moment. It&amp;rsquo;s always been a style of programming that&amp;rsquo;s been possible with pandas, and over the past several releases, we&amp;rsquo;ve added methods that enable even more chaining.</description></item><item><title>Modern Pandas (Part 1)</title><link>https://tomaugspurger.github.io/posts/modern-1-intro/</link><pubDate>Mon, 21 Mar 2016 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/modern-1-intro/</guid><description>This is part 1 in my series on writing modern idiomatic pandas.
Modern Pandas Method Chaining Indexes Fast Pandas Tidy Data Visualization Time Series Scaling Effective Pandas Introduction This series is about how to make effective use of pandas, a data analysis library for the Python programming language. It&amp;rsquo;s targeted at an intermediate level: people who have some experience with pandas, but are looking to improve.
Prior Art There are many great resources for learning pandas; this is not one of them.</description></item><item><title>Practical Pandas Part 3 - Exploratory Data Analysis</title><link>https://tomaugspurger.github.io/posts/pp03/</link><pubDate>Tue, 16 Sep 2014 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/pp03/</guid><description>Welcome back. As a reminder:
In part 1 we got dataset with my cycling data from last year merged and stored in an HDF5 store In part 2 we did some cleaning and augmented the cycling data with data from http://forecast.io. You can find the full source code and data at this project&amp;rsquo;s GitHub repo.
Today we&amp;rsquo;ll use pandas, seaborn, and matplotlib to do some exploratory data analysis. For fun, we&amp;rsquo;ll make some maps at the end using folium.</description></item><item><title>Practical Pandas Part 2 - More Tidying, More Data, and Merging</title><link>https://tomaugspurger.github.io/posts/pp02/</link><pubDate>Thu, 04 Sep 2014 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/pp02/</guid><description>This is Part 2 in the Practical Pandas Series, where I work through a data analysis problem from start to finish.
It&amp;rsquo;s a misconception that we can cleanly separate the data analysis pipeline into a linear sequence of steps from
data acqusition data tidying exploratory analysis model building production As you work through a problem you&amp;rsquo;ll realize, &amp;ldquo;I need this other bit of data&amp;rdquo;, or &amp;ldquo;this would be easier if I stored the data this way&amp;rdquo;, or more commonly &amp;ldquo;strange, that&amp;rsquo;s not supposed to happen&amp;rdquo;.</description></item><item><title>Practical Pandas Part 1 - Reading the Data</title><link>https://tomaugspurger.github.io/posts/pp01/</link><pubDate>Tue, 26 Aug 2014 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/pp01/</guid><description>This is the first post in a series where I&amp;rsquo;ll show how I use pandas on real-world datasets.
For this post, we&amp;rsquo;ll look at data I collected with Cyclemeter on my daily bike ride to and from school last year. I had to manually start and stop the tracking at the beginning and end of each ride. There may have been times where I forgot to do that, so we&amp;rsquo;ll see if we can find those.</description></item><item><title/><link>https://tomaugspurger.github.io/feeds/all.atom.xml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/feeds/all.atom.xml/</guid><description/></item><item><title/><link>https://tomaugspurger.github.io/posts/compat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/compat/</guid><description>Title: Compatibility Code Date: 2019-12-12 Slug: compatibility status: draft
Compatibility Code Most libraries with dependencies will want to support multiple versions of that dependency. But supporting old version is a pain: it requires compatibility code, code that is around solely to get the same output from versions of a library. This post gives some advice on writing compatibility code.
Don&amp;rsquo;t write your own version parser Centralize all version parsing Use consistent version comparisons Use Python&amp;rsquo;s argument unpacking Clean up unused compatibility code 1.</description></item><item><title/><link>https://tomaugspurger.github.io/posts/pandas-binder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/pandas-binder/</guid><description>Title: pandas + binder Date: 2019-07-21 tags:
pandas This post describes the start of a journey to get pandas&amp;rsquo; documentation running on Binder. The end result is this nice button: For a while now I&amp;rsquo;ve been jealous of Dask&amp;rsquo;s examples repository. That&amp;rsquo;s a repository containing a collection of Jupyter notebooks demonstrating Dask in action. It stitches together some tools to present a set of documentation that is both viewable as a static site at examples.</description></item><item><title>About</title><link>https://tomaugspurger.github.io/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/about/</guid><description>Hi, I&amp;rsquo;m Tom. I&amp;rsquo;m a programmer living in Des Moines, IA. I work for Microsoft.
Talks Pandas: .head() to .tail() | video | materials Mind the Gap! Bridging the scikit-learn - pandas dtype divide | video | materials Pandas: .head() to .tail() | video | materials Podcasts Microsoft Planetary Computer on Talk Python Pandas Extension Arrays on Podcast.__init__. Writing Effective Pandas: A series on writing effective, idiomatic pandas. A few posts on Medium with various co-authors.</description></item><item><title>dplyr and pandas</title><link>https://tomaugspurger.github.io/posts/dplry-pandas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/dplry-pandas/</guid><description>This notebook compares pandas and dplyr. The comparison is just on syntax (verbage), not performance. Whether you&amp;rsquo;re an R user looking to switch to pandas (or the other way around), I hope this guide will help ease the transition.
We&amp;rsquo;ll work through the introductory dplyr vignette to analyze some flight data.
I&amp;rsquo;m working on a better layout to show the two packages side by side. But for now I&amp;rsquo;m just putting the dplyr code in a comment above each python call.</description></item><item><title>Organizing Papers</title><link>https://tomaugspurger.github.io/posts/organizing-papers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/organizing-papers/</guid><description>As a graduate student, you read a lot of journal articles&amp;hellip; a lot. With the material in the articles being as difficult as it is, I didn&amp;rsquo;t want to worry about organizing everything as well. That&amp;rsquo;s why I wrote this script to help (I may have also been procrastinating from studying for my qualifiers). This was one of my earliest little projects, so I&amp;rsquo;m not claiming that this is the best way to do anything.</description></item><item><title>Tidy Data in Action</title><link>https://tomaugspurger.github.io/posts/tidy-data-in-action/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/tidy-data-in-action/</guid><description>Hadley Whickham wrote a famous paper (for a certain definition of famous) about the importance of tidy data when doing data analysis. I want to talk a bit about that, using an example from a StackOverflow post, with a solution using pandas. The principles of tidy data aren&amp;rsquo;t language specific.
A tidy dataset must satisfy three criteria (page 4 in Whickham&amp;rsquo;s paper):
Each variable forms a column. Each observation forms a row.</description></item><item><title>Using Python to tackle the CPS</title><link>https://tomaugspurger.github.io/posts/tackling-the-cps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/tackling-the-cps/</guid><description>The Current Population Survey is an important source of data for economists. It&amp;rsquo;s modern form took shape in the 70&amp;rsquo;s and unfortunately the data format and distribution shows its age. Some centers like IPUMS have attempted to put a nicer face on accessing the data, but they haven&amp;rsquo;t done everything yet. In this series I&amp;rsquo;ll describe methods I used to fetch, parse, and analyze CPS data for my second year paper.</description></item><item><title>Using Python to tackle the CPS (Part 2)</title><link>https://tomaugspurger.github.io/posts/tackling-the-cps-part-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/tackling-the-cps-part-2/</guid><description>Last time, we used Python to fetch some data from the Current Population Survey. Today, we&amp;rsquo;ll work on parsing the files we just downloaded.
We downloaded two types of files last time:
CPS monthly tables: a fixed-width format text file with the actual data Data Dictionaries: a text file describing the layout of the monthly tables Our goal is to parse the monthly tables. Here&amp;rsquo;s the first two lines from the unzipped January 1994 file:</description></item><item><title>Using Python to tackle the CPS (Part 3)</title><link>https://tomaugspurger.github.io/posts/tackling-the-cps-part-3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/tackling-the-cps-part-3/</guid><description>In part 2 of this series, we set the stage to parse the data files themselves.
As a reminder, we have a dictionary that looks like
id length start end 0 HRHHID 15 1 15 1 HRMONTH 2 16 17 2 HRYEAR4 4 18 21 3 HURESPLI 2 22 23 4 HUFINAL 3 24 26 ... ... ... ... giving the columns of the raw CPS data files. This post (or two) will describe the reading of the actual data files, and the somewhat tricky process of matching individuals across the different files.</description></item><item><title>Using Python to tackle the CPS (Part 4)</title><link>https://tomaugspurger.github.io/posts/tackling-the-cps-part-4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tomaugspurger.github.io/posts/tackling-the-cps-part-4/</guid><description>Last time, we got to where we&amp;rsquo;d like to have started: One file per month, with each month laid out the same.
As a reminder, the CPS interviews households 8 times over the course of 16 months. They&amp;rsquo;re interviewed for 4 months, take 8 months off, and are interviewed four more times. So if your first interview was in month $m$, you&amp;rsquo;re also interviewed in months $$m + 1, m + 2, m + 3, m + 12, m + 13, m + 14, m + 15$$.</description></item></channel></rss>